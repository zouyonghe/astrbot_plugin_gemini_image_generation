"""
APIå®¢æˆ·ç«¯æ¨¡å—
æä¾›Google Geminiå’ŒOpenAIå…¼å®¹APIçš„å®¢æˆ·ç«¯å®ç°
"""

from __future__ import annotations

import asyncio
import base64
import json
import os
import re
import tempfile
import urllib.parse
import urllib.request
from pathlib import Path
from typing import Any

import aiohttp
from astrbot.api import logger

from .api import get_api_provider
from .api_types import APIError, ApiRequestConfig

try:
    from .tl_utils import (
        IMAGE_CACHE_DIR,
        SUPPORTED_IMAGE_MIME_TYPES,
        coerce_supported_image,
        coerce_supported_image_bytes,
        encode_file_to_base64,
        get_plugin_data_dir,
        normalize_image_input,
        resolve_image_source_to_path,
        save_base64_image,
        save_image_data,
        save_image_stream,
    )
except ImportError:
    from pathlib import Path

    async def save_base64_image(
        base64_data: str, image_format: str = "png"
    ) -> str | None:
        """å ä½ç¬¦å‡½æ•°"""
        return None

    async def save_image_data(
        image_data: bytes, image_format: str = "png"
    ) -> str | None:
        """å ä½ç¬¦å‡½æ•°"""
        return None

    async def save_image_stream(
        stream_reader, image_format: str = "png", target_path=None
    ):
        return None

    def encode_file_to_base64(file_path, chunk_size: int = 65536) -> str:
        return ""

    def get_plugin_data_dir() -> Path:
        return Path(".")

    IMAGE_CACHE_DIR = get_plugin_data_dir() / "images" / "download_cache"
    SUPPORTED_IMAGE_MIME_TYPES = {
        "image/png",
        "image/jpeg",
        "image/webp",
        "image/heic",
        "image/heif",
    }

    def coerce_supported_image_bytes(mime_type, raw_bytes):
        return None, None

    def coerce_supported_image(mime_type, base64_data):
        return None, None

    async def normalize_image_input(
        image_input: Any, *, image_cache_dir=None, image_input_mode="force_base64"
    ):
        return None, None


class GeminiAPIClient:
    """éµå¾ªå®˜æ–¹ API è§„èŒƒçš„ Gemini API å®¢æˆ·ç«¯

    ç‰¹æ€§ï¼š
    - æ”¯æŒ Google å®˜æ–¹ API å’Œ OpenAI API
    - æ”¯æŒè‡ªå®šä¹‰ API Base URLï¼ˆåä»£ï¼‰
    - æ”¯æŒä»»æ„æ¨¡å‹åç§°
    - éµå¾ªå®˜æ–¹ Gemini API è§„èŒƒ
    """

    # Google å®˜æ–¹ API é»˜è®¤åœ°å€
    GOOGLE_API_BASE = "https://generativelanguage.googleapis.com/v1beta"

    # OpenAI API é»˜è®¤åœ°å€
    OPENAI_API_BASE = "https://api.openai.com/v1"

    def __init__(self, api_keys: list[str]):
        """
        åˆå§‹åŒ– API å®¢æˆ·ç«¯

        Args:
            api_keys: API å¯†é’¥åˆ—è¡¨
        """
        self.api_keys = api_keys or []
        self.current_key_index = 0
        self._lock = asyncio.Lock()
        self.proxy = (
            os.environ.get("HTTPS_PROXY")
            or os.environ.get("https_proxy")
            or os.environ.get("HTTP_PROXY")
            or os.environ.get("http_proxy")
        )
        if self.proxy:
            logger.debug(f"æ£€æµ‹åˆ°ä»£ç†é…ç½®ï¼Œä½¿ç”¨ä»£ç†: {self.proxy}")
        logger.debug(f"API å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œæ”¯æŒ {len(self.api_keys)} ä¸ª API å¯†é’¥")
        self.verbose_logging: bool = False
        self._session: aiohttp.ClientSession | None = None
        self._session_lock = asyncio.Lock()

    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å–æˆ–åˆ›å»ºå¯å¤ç”¨çš„ aiohttp ä¼šè¯"""
        if self._session and not self._session.closed:
            return self._session
        async with self._session_lock:
            if self._session and not self._session.closed:
                return self._session
            self._session = aiohttp.ClientSession()
            return self._session

    async def close(self):
        """å…³é—­å†…éƒ¨å¤ç”¨çš„ aiohttp ä¼šè¯"""
        if self._session and not self._session.closed:
            await self._session.close()
        self._session = None

    @staticmethod
    def _coerce_supported_image_bytes(
        mime_type: str | None, raw_bytes: bytes
    ) -> tuple[str | None, str | None]:
        return coerce_supported_image_bytes(mime_type, raw_bytes)

    @staticmethod
    def _coerce_supported_image(
        mime_type: str | None, base64_data: str
    ) -> tuple[str | None, str | None]:
        return coerce_supported_image(mime_type, base64_data)

    @staticmethod
    def _validate_and_normalize_b64(
        raw_data: str, *, context: str = "", allow_relaxed_return: bool = False
    ) -> str:
        """
        æ ¡éªŒå¹¶å½’ä¸€åŒ– base64ï¼š
        - å»æ‰å‰ç¼€/æ¢è¡Œ
        - å°è¯•æ ‡å‡†è§£ç å¤±è´¥åå›é€€ urlsafe è§£ç ï¼ˆè¡¥é½ paddingï¼‰
        - å†å¤±è´¥å°è¯•å®½æ¾è¿‡æ»¤/è‡ªåŠ¨è¡¥é½ padding åè§£ç é‡ç¼–ç 
        è¿”å›å¯ç›´æ¥ä½¿ç”¨çš„çº¯ base64 å­—ç¬¦ä¸²ï¼Œå¤±è´¥æŠ›å‡ºå¼‚å¸¸ã€‚
        """
        cleaned = (raw_data or "").strip().replace("\n", "")
        if ";base64," in cleaned:
            _, _, cleaned = cleaned.partition(";base64,")

        def try_decode(data: str) -> str:
            base64.b64decode(data, validate=True)
            return data

        try:
            return try_decode(cleaned)
        except Exception:
            # å›é€€ urlsafe base64
            alt = cleaned.replace("-", "+").replace("_", "/")
            pad_len = (-len(alt)) % 4
            if pad_len:
                alt += "=" * pad_len
            try:
                return try_decode(alt)
            except Exception as e:
                # æœ€åå°è¯•å®½æ¾è¿‡æ»¤éæ³•å­—ç¬¦/è¡¥é½ padding åè§£ç é‡ç¼–ç 
                relaxed = re.sub(r"[^A-Za-z0-9+/=_-]", "", cleaned)
                pad_len2 = (-len(relaxed)) % 4
                if pad_len2:
                    relaxed += "=" * pad_len2
                try:
                    raw = base64.b64decode(relaxed, validate=False)
                    if raw:
                        return base64.b64encode(raw).decode("utf-8")
                except Exception:
                    pass
                if allow_relaxed_return and relaxed:
                    return relaxed
                if allow_relaxed_return and cleaned:
                    # ä»æ— æ³•è§£ç æ—¶ï¼Œå…è®¸ç›´æ¥å›é€€åŸå§‹å­—ç¬¦ä¸²äº¤ç”±ä¸‹æ¸¸å¤„ç†
                    return cleaned
                raise APIError(
                    f"å‚è€ƒå›¾ base64 æ ¡éªŒå¤±è´¥{f'ï¼ˆ{context}ï¼‰' if context else ''}ï¼Œè¯·æ£€æŸ¥å›¾ç‰‡åé‡è¯•ã€‚",
                    None,
                    "invalid_reference_image",
                ) from e

    async def get_next_api_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ª API å¯†é’¥"""
        async with self._lock:
            if not self.api_keys:
                raise ValueError("API å¯†é’¥åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            key = self.api_keys[self.current_key_index % len(self.api_keys)]
            return key

    async def rotate_api_key(self):
        """è½®æ¢åˆ°ä¸‹ä¸€ä¸ª API å¯†é’¥"""
        async with self._lock:
            if len(self.api_keys) > 1:
                self.current_key_index = (self.current_key_index + 1) % len(
                    self.api_keys
                )
                logger.debug(
                    f"å·²è½®æ¢åˆ°ä¸‹ä¸€ä¸ª API å¯†é’¥ï¼Œå½“å‰ç´¢å¼•: {self.current_key_index}"
                )

    async def _prepare_google_payload(self, config: ApiRequestConfig) -> dict[str, Any]:
        """å‘åå…¼å®¹ï¼šå§”æ‰˜ç»™ GoogleProvider æ„å»º payloadã€‚"""
        provider = get_api_provider("google")
        req = await provider.build_request(client=self, config=config)
        return req.payload

    async def _prepare_openai_payload(self, config: ApiRequestConfig) -> dict[str, Any]:
        """å‘åå…¼å®¹ï¼šå§”æ‰˜ç»™ OpenAICompatProvider æ„å»º payloadã€‚"""
        provider = get_api_provider(config.api_type)
        req = await provider.build_request(client=self, config=config)
        return req.payload

    async def _normalize_image_input(
        self,
        image_input: Any,
        image_input_mode: str = "force_base64",
        image_cache_dir=None,
    ) -> tuple[str | None, str | None]:
        """ç»Ÿä¸€è°ƒç”¨ tl_utils çš„å‚è€ƒå›¾è§„èŒƒåŒ–é€»è¾‘"""
        return await normalize_image_input(
            image_input,
            image_cache_dir=image_cache_dir or IMAGE_CACHE_DIR,
            image_input_mode=image_input_mode,
        )

    async def _process_reference_image(
        self,
        image_input: Any,
        idx: int,
        image_input_mode: str = "force_base64",
    ) -> tuple[str | None, str | None, bool]:
        """
        ç»Ÿä¸€å¤„ç†å‚è€ƒå›¾åƒï¼Œè¿”å› (mime_type, data, is_url)ã€‚

        å¤„ç†æµç¨‹ï¼š
        1. å°è¯•è§£æä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„
        2. å°è¯•è§„èŒƒåŒ–è½¬æ¢ä¸º base64
        3. å°è¯•é€šè¿‡ QQ ä¸‹è½½å™¨è·å–
        4. è¿”å›å¤„ç†ç»“æœ

        Returns:
            (mime_type, data, is_url):
            - mime_type: MIME ç±»å‹
            - data: base64 æ•°æ®æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
            - is_url: åŸå§‹è¾“å…¥æ˜¯å¦ä¸º URL
        """
        image_str = str(image_input).strip()
        is_url = image_str.startswith(("http://", "https://"))

        data = None
        mime_type = None

        # 1. å°è¯•è§£æä¸ºæœ¬åœ°æ–‡ä»¶
        try:
            local_path = await resolve_image_source_to_path(
                image_input,
                image_input_mode=image_input_mode,
                api_client=self,
                download_qq_image_fn=None,
            )
            if local_path and Path(local_path).exists():
                suffix = Path(local_path).suffix.lower().lstrip(".") or "png"
                mime_type = f"image/{suffix}"
                data = encode_file_to_base64(local_path)
                logger.debug(
                    f"[_process_reference_image] ä»æœ¬åœ°æ–‡ä»¶è·å–æˆåŠŸ: idx={idx}"
                )
        except Exception as e:
            logger.debug(
                f"[_process_reference_image] æœ¬åœ°æ–‡ä»¶è§£æå¤±è´¥: idx={idx} err={e}"
            )

        # 2. å°è¯•è§„èŒƒåŒ–è½¬æ¢
        if not data:
            try:
                with tempfile.TemporaryDirectory(prefix="gemini_ref_tmp_") as tmp_dir:
                    temp_cache = Path(tmp_dir)
                    mime_type, data = await self._normalize_image_input(
                        image_input,
                        image_input_mode=image_input_mode,
                        image_cache_dir=temp_cache,
                    )
                if data:
                    logger.debug(
                        f"[_process_reference_image] è§„èŒƒåŒ–è½¬æ¢æˆåŠŸ: idx={idx} mime={mime_type}"
                    )
                else:
                    logger.debug(
                        f"[_process_reference_image] è§„èŒƒåŒ–è½¬æ¢è¿”å›ç©º: idx={idx}"
                    )
            except Exception as e:
                logger.debug(
                    f"[_process_reference_image] è§„èŒƒåŒ–è½¬æ¢å¤±è´¥: idx={idx} err={e}"
                )

        # 3. QQ ä¸‹è½½å™¨é€»è¾‘å·²æ•´åˆåˆ° normalize_image_input å’Œ resolve_image_source_to_path ä¸­

        return mime_type, data, is_url

    def _validate_b64_with_fallback(
        self, data: str, context: str = ""
    ) -> tuple[str, bool]:
        """
        æ ¡éªŒ base64 æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›é€ä¼ çš„åŸå§‹æ•°æ®ã€‚

        Returns:
            (result, validated): result æ˜¯å¤„ç†åçš„æ•°æ®ï¼Œvalidated è¡¨ç¤ºæ˜¯å¦é€šè¿‡æ ¡éªŒ
        """
        try:
            validated = self._validate_and_normalize_b64(
                data, context=context, allow_relaxed_return=True
            )
            return validated, True
        except APIError:
            # æ ¡éªŒå¤±è´¥ï¼Œé€ä¼ åŸå§‹æ•°æ®ï¼ˆå»æ‰ data URI å‰ç¼€ï¼‰
            raw = str(data).strip()
            if ";base64," in raw:
                _, _, raw = raw.partition(";base64,")
            return raw, False

    @staticmethod
    def _ensure_mime_type(mime_type: str | None, default: str = "image/png") -> str:
        """ç¡®ä¿ MIME ç±»å‹æœ‰æ•ˆ"""
        if mime_type and mime_type.startswith("image/"):
            return mime_type
        return default

    async def _get_api_url(
        self, config: ApiRequestConfig
    ) -> tuple[str, dict[str, str], dict[str, Any]]:
        """
        æ ¹æ®é…ç½®è·å– API URLã€è¯·æ±‚å¤´å’Œè´Ÿè½½

        æ™ºèƒ½å¤„ç†APIè·¯å¾„å‰ç¼€ï¼Œæ— éœ€æ‰‹åŠ¨è¾“å…¥/v1æˆ–/v1beta
        """
        provider = get_api_provider(config.api_type)
        req = await provider.build_request(client=self, config=config)
        return req.url, req.headers, req.payload

    async def generate_image(
        self,
        config: ApiRequestConfig,
        max_retries: int = 3,
        total_timeout: int = 120,
        per_retry_timeout: int = None,
        max_total_time: int = None,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """
        ç”Ÿæˆå›¾åƒ

        Args:
            config: è¯·æ±‚é…ç½®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            total_timeout: æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            (image_urls, image_paths, text_content, thought_signature)ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨å’ŒNone
        """
        if not self.api_keys:
            raise ValueError("æœªé…ç½® API å¯†é’¥")

        if not config.api_key:
            config.api_key = await self.get_next_api_key()

        enable_smart_retry = bool(getattr(config, "enable_smart_retry", True))

        # è·å–è¯·æ±‚ä¿¡æ¯
        url, headers, payload = await self._get_api_url(config)

        logger.debug(f"ä½¿ç”¨ {config.model} (é€šè¿‡ {config.api_type}) ç”Ÿæˆå›¾åƒ")
        logger.debug(f"API ç«¯ç‚¹: {url[:80]}...")
        logger.debug(
            "è¯·æ±‚å‚æ•°æ¦‚è§ˆ: refs=%s prompt_len=%s aspect=%s res=%s",
            len(config.reference_images or []),
            len(config.prompt or ""),
            config.aspect_ratio,
            config.resolution,
        )

        if config.resolution or config.aspect_ratio:
            logger.debug(
                f"åˆ†è¾¨ç‡: {config.resolution or 'é»˜è®¤'}, é•¿å®½æ¯”: {config.aspect_ratio or 'é»˜è®¤'}"
            )

        if config.api_base:
            logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰ API Base: {config.api_base}")

        # åŒæ­¥è¯¦ç»†æ—¥å¿—å¼€å…³ï¼Œä¾¿äºåœ¨å†…éƒ¨ç½‘ç»œè¯·æ±‚ä¸­æ§åˆ¶è¾“å‡ºç²’åº¦
        self.verbose_logging = bool(getattr(config, "verbose_logging", False))

        return await self._make_request(
            url=url,
            payload=payload,
            headers=headers,
            api_type=config.api_type,
            model=config.model,
            max_retries=max_retries,
            total_timeout=total_timeout,
            api_base=config.api_base,
            enable_smart_retry=enable_smart_retry,
            per_retry_timeout=per_retry_timeout,
            max_total_time=max_total_time,
        )

    async def _make_request(
        self,
        url: str,
        payload: dict[str, Any],
        headers: dict[str, str],
        api_type: str,
        model: str,
        max_retries: int,
        total_timeout: int = 120,
        api_base: str = None,
        enable_smart_retry: bool = True,
        per_retry_timeout: int | None = None,
        max_total_time: int | None = None,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """æ‰§è¡Œ API è¯·æ±‚å¹¶å¤„ç†å“åº”ï¼ˆæ”¯æŒé‡è¯•ã€æ€»è€—æ—¶ä¸Šé™ä¸å¤š Key è½®æ¢ï¼‰"""

        # é˜²å¾¡æ€§å¤åˆ¶ headersï¼Œé¿å…å¹¶å‘è¯·æ±‚é—´å›  Key è½®æ¢å¯¼è‡´çš„ç›¸äº’å½±å“
        headers = dict(headers)

        def coerce_int(value: Any, default: int) -> int:
            try:
                return int(value)
            except Exception:
                return default

        effective_max_retries = max(coerce_int(max_retries, 1), 1)
        enable_smart_retry = bool(enable_smart_retry)
        if not enable_smart_retry:
            effective_max_retries = 1

        base_timeout = per_retry_timeout if per_retry_timeout is not None else total_timeout
        base_timeout_int = max(coerce_int(base_timeout, total_timeout), 1)

        max_total_time_int = None
        if max_total_time is not None:
            mt = coerce_int(max_total_time, 0)
            if mt > 0:
                max_total_time_int = mt

        session = await self._get_session()
        loop = asyncio.get_running_loop()
        started_at = loop.time()
        last_error: APIError | None = None

        async def rotate_key_if_possible(err: APIError) -> str | None:
            """åœ¨å¯ç”¨å¤š Key ä¸”é”™è¯¯å¯èƒ½ä¸ Key ç›¸å…³æ—¶è½®æ¢ï¼Œå¹¶æ›´æ–° headersã€‚"""
            if not enable_smart_retry or len(self.api_keys) <= 1:
                return None

            status = err.status_code
            if status not in {401, 402, 403, 429} and err.error_type != "quota":
                return None

            try:
                await self.rotate_api_key()
                new_key = await self.get_next_api_key()
            except Exception as e:
                logger.debug(f"è½®æ¢ API Key å¤±è´¥ï¼Œå°†ç»§ç»­ä½¿ç”¨å½“å‰ Key: {e}")
                return None

            updated = False
            if "Authorization" in headers:
                auth = str(headers.get("Authorization") or "")
                if auth.lower().startswith("bearer "):
                    headers["Authorization"] = f"Bearer {new_key}"
                    updated = True
            if "x-goog-api-key" in headers:
                headers["x-goog-api-key"] = new_key
                updated = True
            for k in ("X-Api-Key", "X-API-Key", "x-api-key"):
                if k in headers:
                    headers[k] = new_key
                    updated = True

            if not updated:
                logger.debug("å·²è½®æ¢ API Keyï¼Œä½†æœªèƒ½è¯†åˆ«éœ€è¦æ›´æ–°çš„è¯·æ±‚å¤´å­—æ®µ")
            else:
                logger.debug(f"å·²è½®æ¢åˆ°æ–°çš„ API Key: {new_key[:12]}...")

            return new_key

        def is_retryable(err: APIError) -> bool:
            """æ ¹æ®é”™è¯¯ç±»å‹/çŠ¶æ€ç åˆ¤æ–­æ˜¯å¦å€¼å¾—é‡è¯•ã€‚"""
            if err.error_type == "no_image_retry":
                return True
            if err.error_type in {"timeout", "network"}:
                return True

            status = err.status_code
            if status is None:
                return True
            if 500 <= status < 600:
                return True
            if status in {408, 500, 502, 503, 504}:
                return True
            if status == 429:
                return True
            if status in {401, 402, 403}:
                return len(self.api_keys) > 1
            return False

        for attempt in range(effective_max_retries):
            if max_total_time_int is not None:
                elapsed = loop.time() - started_at
                remaining = max_total_time_int - elapsed
                if remaining <= 0:
                    timeout_msg = (
                        "å›¾åƒç”Ÿæˆæ—¶é—´è¿‡é•¿ï¼Œè¶…å‡ºäº†æ¡†æ¶é™åˆ¶ã€‚è¯·å°è¯•ç®€åŒ–å›¾åƒæè¿°æˆ–åœ¨æ¡†æ¶é…ç½®ä¸­å¢åŠ  tool_call_timeout åˆ° 90-120 ç§’ã€‚"
                    )
                    raise APIError(timeout_msg, None, "timeout") from None

                attempt_timeout_int = max(min(base_timeout_int, int(remaining)), 1)
            else:
                attempt_timeout_int = base_timeout_int

            timeout_cfg = aiohttp.ClientTimeout(
                total=attempt_timeout_int, sock_read=attempt_timeout_int
            )

            try:
                logger.debug(
                    "å‘é€è¯·æ±‚ï¼ˆå°è¯• %s/%s, timeout=%ssï¼‰",
                    attempt + 1,
                    effective_max_retries,
                    attempt_timeout_int,
                )
                return await self._perform_request(
                    session,
                    url,
                    payload,
                    headers,
                    api_type,
                    model,
                    timeout=timeout_cfg,
                    api_base=api_base,
                )

            except asyncio.CancelledError:
                # åªæœ‰æ¡†æ¶å–æ¶ˆæ‰ä¸é‡è¯•ï¼ˆè¿™æ˜¯æœ€é¡¶å±‚çš„è¶…æ—¶ï¼‰
                logger.debug("è¯·æ±‚è¢«æ¡†æ¶å–æ¶ˆï¼ˆå·¥å…·è°ƒç”¨æ€»è¶…æ—¶ï¼‰ï¼Œä¸å†é‡è¯•")
                timeout_msg = "å›¾åƒç”Ÿæˆæ—¶é—´è¿‡é•¿ï¼Œè¶…å‡ºäº†æ¡†æ¶é™åˆ¶ã€‚è¯·å°è¯•ç®€åŒ–å›¾åƒæè¿°æˆ–åœ¨æ¡†æ¶é…ç½®ä¸­å¢åŠ  tool_call_timeout åˆ° 90-120 ç§’ã€‚"
                raise APIError(timeout_msg, None, "cancelled") from None
            except Exception as e:
                if isinstance(e, APIError):
                    err = e
                    if not err.error_type:
                        err.error_type = self._classify_error(e, err.message)
                else:
                    err_msg = str(e)
                    err_type = self._classify_error(e, err_msg)
                    status_code = getattr(e, "status", None)
                    err = APIError(err_msg, status_code, err_type)

                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºä¸å¯é‡è¯•é”™è¯¯
                if not is_retryable(err):
                    logger.error(f"ä¸å¯é‡è¯•é”™è¯¯: {err.message}")
                    raise err from None

                # å¯é‡è¯•é”™è¯¯ï¼Œä½†å·²ç”¨å°½é‡è¯•æ¬¡æ•°
                if attempt >= effective_max_retries - 1:
                    logger.error(
                        f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({effective_max_retries})ï¼Œç”Ÿæˆå¤±è´¥: {err.message}"
                    )
                    raise err from None

                last_error = err
                logger.warning(
                    "å¯é‡è¯•é”™è¯¯ (å°è¯• %s/%s): %s",
                    attempt + 1,
                    effective_max_retries,
                    err.message,
                )

                await rotate_key_if_possible(err)

                delay = min(2 ** (attempt + 2), 10)
                logger.debug(f"ç­‰å¾… {delay} ç§’åé‡è¯•...")
                await asyncio.sleep(delay)

        if last_error:
            raise last_error from None

        return [], [], None, None

    def _classify_error(self, exception: Exception, error_msg: str) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        if isinstance(exception, asyncio.TimeoutError):
            return "timeout"
        elif "timeout" in error_msg.lower():
            return "timeout"
        elif "connection" in error_msg.lower():
            return "network"
        elif isinstance(exception, aiohttp.ClientError):
            return "network"
        else:
            return "unknown"

    def _is_retryable_error(self, error_type: str, exception: Exception) -> bool:
        """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•"""
        # ç‰¹æ®Šå¤„ç†ï¼šæœªç”Ÿæˆå›¾åƒçš„é‡è¯•
        if error_type == "no_image_retry":
            return True

        # å¯é‡è¯•çš„é”™è¯¯ï¼šè¶…æ—¶ã€ç½‘ç»œé”™è¯¯ã€æœåŠ¡å™¨é”™è¯¯
        if error_type in ["timeout", "network"]:
            return True

        # HTTP çŠ¶æ€ç åˆ¤æ–­
        if hasattr(exception, "status"):
            status = exception.status
            # å¯é‡è¯•ï¼š408, 500, 502, 503, 504
            # ä¸å¯é‡è¯•ï¼š401, 402, 403, 422, 429ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰
            if status in [408, 500, 502, 503, 504]:
                return True
            elif status in [401, 402, 403, 422, 429]:
                return False

        return True  # é»˜è®¤é‡è¯•æœªçŸ¥é”™è¯¯

    async def _perform_request(
        self,
        session: aiohttp.ClientSession,
        url: str,
        payload: dict[str, Any],
        headers: dict[str, str],
        api_type: str,
        model: str,
        *,
        timeout: aiohttp.ClientTimeout | None = None,
        api_base: str = None,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """æ‰§è¡Œå®é™…çš„HTTPè¯·æ±‚"""
        logger.debug(
            "å‘é€è¯·æ±‚: url=%s api_type=%s model=%s payload_keys=%s",
            url[:100],
            api_type,
            model,
            list(payload.keys()),
        )

        async with session.post(
            url,
            json=payload,
            headers=headers,
            proxy=self.proxy,
            timeout=timeout,
        ) as response:
            logger.debug(f"å“åº”çŠ¶æ€: {response.status}")
            response_text = await response.text()
            content_type = response.headers.get("Content-Type", "") or ""

            # è§£æ JSON å“åº”ï¼Œæ·»åŠ é”™è¯¯å¤„ç†
            try:
                response_data = json.loads(response_text) if response_text else {}
            except json.JSONDecodeError as e:
                # SSE å“åº”ï¼ˆtext/event-streamï¼‰éœ€è¦é¢å¤–è§£æ
                if (
                    "text/event-stream" in content_type.lower()
                    or response_text.strip().startswith("data:")
                ):
                    try:
                        response_data = self._parse_sse_payload(response_text)
                        logger.debug("æ£€æµ‹åˆ° SSE å“åº”ï¼Œå·²å®Œæˆ JSON è½¬æ¢")
                    except Exception as sse_error:
                        logger.error(f"SSE è§£æå¤±è´¥: {sse_error}")
                        logger.error(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response_text[:500]}")
                        raise APIError(
                            f"API è¿”å›äº†æ— æ•ˆçš„ JSON/SSE å“åº”: {sse_error}",
                            response.status,
                        ) from None
                else:
                    logger.error(f"JSON è§£æå¤±è´¥: {e}")
                    logger.error(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response_text[:500]}")
                    raise APIError(
                        f"API è¿”å›äº†æ— æ•ˆçš„ JSON å“åº”: {e}", response.status
                    ) from None

            if response.status == 200:
                logger.debug("API è°ƒç”¨æˆåŠŸ")
                if api_type == "google":
                    return await self._parse_gresponse(response_data, session)
                else:  # openai å…¼å®¹æ ¼å¼
                    return await self._parse_openai_response(
                        response_data, session, api_base
                    )
            elif response.status in [429, 402, 403]:
                error_msg = response_data.get("error", {}).get(
                    "message", f"HTTP {response.status}"
                )
                logger.warning(f"API é…é¢/æƒé™é—®é¢˜: {error_msg}")
                raise APIError(error_msg, response.status, "quota")
            else:
                error_msg = response_data.get("error", {}).get(
                    "message", f"HTTP {response.status}"
                )
                logger.warning(f"API é”™è¯¯: {error_msg}")
                raise APIError(error_msg, response.status)

    def _parse_sse_payload(self, raw_text: str) -> dict[str, Any]:
        """è§£æ text/event-stream å“åº”ï¼Œæå–æœ€åä¸€ä¸ªåŒ…å«æœ‰æ•ˆ payload çš„ data åŒ…"""

        events: list[dict[str, Any]] = []
        data_lines: list[str] = []

        def flush_event():
            """å°†ç´¯è®¡çš„ data è¡Œæ‹¼æ¥å¹¶è§£æä¸ºä¸€ä¸ªäº‹ä»¶"""
            if not data_lines:
                return
            data_text = "\n".join(data_lines).strip()
            data_lines.clear()
            if not data_text or data_text == "[DONE]":
                return
            try:
                parsed = json.loads(data_text)
                if isinstance(parsed, dict):
                    events.append(parsed)
            except json.JSONDecodeError as e:
                logger.warning(
                    "SSE äº‹ä»¶è§£æå¤±è´¥: %s | ç‰‡æ®µ: %s",
                    e,
                    data_text[:160],
                )

        for raw_line in raw_text.splitlines():
            stripped = raw_line.strip()
            if not stripped:
                flush_event()
                continue
            if stripped.startswith(":"):
                # SSE æ³¨é‡Šè¡Œï¼Œç›´æ¥è·³è¿‡
                continue
            if stripped.startswith("data:"):
                data_lines.append(stripped.removeprefix("data:").lstrip())
                continue

            # å°‘æ•°å®ç°ä¼šçœç•¥å‰ç¼€ï¼Œè¿™é‡Œå°è¯•å…¼å®¹
            if stripped and stripped != "[DONE]":
                data_lines.append(stripped)

        flush_event()

        if not events:
            raise ValueError(
                f"SSE å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ data äº‹ä»¶ (æ”¶åˆ° {len(raw_text)} å­—ç¬¦, ç‰‡æ®µ: {raw_text[:160]!r})"
            )

        # ä¼˜å…ˆè¿”å›å« candidates/choices/data å­—æ®µçš„äº‹ä»¶ï¼Œé¿å… STOP åŒ…è¦†ç›–æœ‰æ•ˆè´Ÿè½½
        for event in reversed(events):
            if not isinstance(event, dict):
                continue
            if event.get("candidates") or event.get("choices") or event.get("data"):
                logger.debug(
                    "SSE å“åº”å…±è§£æ %s ä¸ªäº‹ä»¶ï¼Œè¿”å›å«æœ‰æ•ˆè´Ÿè½½çš„æœ«å°¾äº‹ä»¶",
                    len(events),
                )
                return event

        logger.debug("SSE å“åº”åªåŒ…å«é€šç”¨äº‹ä»¶ï¼Œè¿”å›æœ€åä¸€ä¸ª data åŒ…")
        return events[-1]

    async def _parse_gresponse(
        self, response_data: dict, session: aiohttp.ClientSession
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """è§£æ Google å®˜æ–¹ API å“åº”"""
        provider = get_api_provider("google")
        return await provider.parse_response(
            client=self, response_data=response_data, session=session
        )

    async def _parse_openai_response(
        self, response_data: dict, session: aiohttp.ClientSession, api_base: str = None
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """è§£æ OpenAI API å“åº”"""

        image_urls: list[str] = []
        image_paths: list[str] = []
        text_content = None
        thought_signature = None
        fail_reasons: list[str] = []
        fallback_texts = self._collect_fallback_texts(response_data)

        message: dict[str, Any] | None = None
        if "choices" in response_data and response_data["choices"]:
            choice = response_data["choices"][0]
            message = choice.get("message", {})
        else:
            message = self._coerce_basic_openai_message(response_data)

        if message:
            if "choices" not in response_data:
                logger.debug(
                    "[openai] ä½¿ç”¨éæ ‡å‡†å­—æ®µæ„é€  messageï¼Œkeys=%s",
                    list(response_data.keys())[:5],
                )
            content = message.get("content", "")

            text_chunks: list[str] = []
            image_candidates: list[str] = []
            extracted_urls: list[str] = []

            logger.debug(
                "[openai] è§£æå“åº” choicesï¼Œcontent_type=%s images_field=%s",
                type(content),
                bool(message.get("images")),
            )

            if isinstance(content, list):
                for part in content:
                    if not isinstance(part, dict):
                        continue

                    part_type = part.get("type")
                    if part_type == "text" and "text" in part:
                        text_val = str(part.get("text", ""))
                        text_chunks.append(text_val)
                        extracted_urls.extend(self._find_image_urls_in_text(text_val))
                    elif part_type == "image_url":
                        image_obj = part.get("image_url") or {}
                        if isinstance(image_obj, dict):
                            url_val = image_obj.get("url")
                            if url_val:
                                image_candidates.append(url_val)
            elif isinstance(content, str):
                text_chunks.append(content)
                extracted_urls.extend(self._find_image_urls_in_text(content))

            # æ ‡å‡† images å­—æ®µï¼ˆå…¼å®¹ Gemini/OpenAI æ··åˆæ ¼å¼ï¼‰
            if message.get("images"):
                for image_item in message["images"]:
                    if not isinstance(image_item, dict):
                        continue

                    # å…¸å‹æ ¼å¼ï¼š{"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
                    image_obj = image_item.get("image_url")
                    if isinstance(image_obj, dict):
                        url_val = image_obj.get("url")
                        if isinstance(url_val, str) and url_val:
                            image_candidates.append(url_val)
                    elif isinstance(image_obj, str) and image_obj:
                        image_candidates.append(image_obj)
                    # é€€åŒ–æ ¼å¼ï¼š{"url": "..."}
                    elif isinstance(image_item.get("url"), str):
                        image_candidates.append(image_item["url"])

            # åˆå¹¶åœ¨æ–‡æœ¬é‡Œè§£æåˆ°çš„å›¾åƒ URL
            if extracted_urls:
                image_candidates.extend(extracted_urls)

            # ç»„è£…æ–‡æœ¬å†…å®¹
            if text_chunks:
                text_content = " ".join([t for t in text_chunks if t]).strip() or None

            # æŒ‰é¡ºåºå¤„ç†å›¾åƒå€™é€‰
            for candidate_url in image_candidates:
                logger.debug("[openai] å¤„ç†å€™é€‰URL: %s", str(candidate_url)[:120])
                if isinstance(candidate_url, str) and candidate_url.startswith(
                    "data:image/"
                ):
                    image_url, image_path = await self._parse_data_uri(candidate_url)
                elif isinstance(candidate_url, str):
                    # grok2api é€‚é…ï¼šå¤„ç†ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ /images/xxxï¼‰
                    if candidate_url.startswith("/") and not candidate_url.startswith(
                        "//"
                    ):
                        if api_base:
                            # ä» api_base æå– scheme å’Œ netloc
                            parsed_base = urllib.parse.urlparse(api_base)
                            base_url = f"{parsed_base.scheme}://{parsed_base.netloc}"
                            full_url = urllib.parse.urljoin(base_url, candidate_url)
                            # grok2api é€‚é…ï¼šç«‹å³ä¸‹è½½ä¸´æ—¶ç¼“å­˜çš„å›¾ç‰‡ï¼ˆé¿å…è¢«æ¸…ç†ï¼‰
                            logger.debug(
                                f"[grok2api é€‚é…] ç›¸å¯¹è·¯å¾„è½¬æ¢å¹¶ä¸‹è½½: {candidate_url} -> {full_url}"
                            )
                            image_url, image_path = await self._download_image(
                                full_url, session, use_cache=False
                            )
                            # åªä¿ç•™æœ¬åœ°è·¯å¾„ï¼ˆ_download_image è¿”å›çš„ä¸¤ä¸ªå€¼ç›¸åŒï¼Œé¿å…é‡å¤ï¼‰
                            if image_path:
                                image_paths.append(image_path)
                            continue
                        else:
                            logger.warning(
                                f"å‘ç°ç›¸å¯¹è·¯å¾„ URL ä½†æœªæä¾› api_baseï¼Œè·³è¿‡: {candidate_url}"
                            )
                            continue
                    # å¯¹äºå¯è®¿é—®çš„ http(s) é“¾æ¥ï¼Œç›´æ¥è¿”å› URLï¼Œé¿å…é‡å¤ä¸‹è½½å ç”¨å¸¦å®½
                    if candidate_url.startswith("http://") or candidate_url.startswith(
                        "https://"
                    ):
                        # grok2api é€‚é…ï¼šæ£€æµ‹ä¸´æ—¶ç¼“å­˜ URL å¹¶å¼ºåˆ¶ä¸‹è½½ï¼ˆé¿å…è¢«æ¸…ç†ï¼‰
                        # ä¸´æ—¶ç¼“å­˜ URL ç‰¹å¾ï¼šåŒ…å« /images/users- æˆ– /temp/image/
                        is_temp_cache = (
                            "/images/users-" in candidate_url
                            or "/temp/image/" in candidate_url
                        )
                        if is_temp_cache:
                            logger.debug(
                                f"[grok2api é€‚é…] æ£€æµ‹åˆ°ä¸´æ—¶ç¼“å­˜ URLï¼Œå¼ºåˆ¶ä¸‹è½½: {candidate_url}"
                            )
                            image_url, image_path = await self._download_image(
                                candidate_url, session, use_cache=False
                            )
                            # åªä¿ç•™æœ¬åœ°è·¯å¾„ï¼ˆ_download_image è¿”å›çš„ä¸¤ä¸ªå€¼ç›¸åŒï¼Œé¿å…é‡å¤ï¼‰
                            if image_path:
                                image_paths.append(image_path)
                            continue
                        # å…¶ä»–æ°¸ä¹… URL ç›´æ¥ä½¿ç”¨
                        image_urls.append(candidate_url)
                        logger.debug(
                            f"ğŸ–¼ï¸ OpenAI è¿”å›å¯ç›´æ¥è®¿é—®çš„å›¾åƒé“¾æ¥: {candidate_url}"
                        )
                        continue
                    image_url, image_path = await self._download_image(
                        candidate_url, session, use_cache=False
                    )
                else:
                    logger.warning(f"è·³è¿‡éå­—ç¬¦ä¸²ç±»å‹çš„å›¾åƒURL: {type(candidate_url)}")
                    continue

                if image_url or image_path:
                    if image_url:
                        image_urls.append(image_url)
                    if image_path:
                        image_paths.append(image_path)

            # content ä¸­æŸ¥æ‰¾å†…è” data URIï¼ˆæ–‡æœ¬é‡Œï¼‰
            extracted_urls: list[str] = []
            extracted_paths: list[str] = []

            if isinstance(content, str):
                extracted_urls, extracted_paths = await self._extract_from_content(
                    content
                )
            elif text_content:
                extracted_urls, extracted_paths = await self._extract_from_content(
                    text_content
                )

            if extracted_urls or extracted_paths:
                image_urls.extend(extracted_urls)
                image_paths.extend(extracted_paths)

            # é¢å¤–åœ¨æ±‡æ€»æ–‡æœ¬ä¸­æœç´¢ http(s) å›¾ç‰‡é“¾æ¥ï¼Œå…¼å®¹åªè¿”å›æ–‡æœ¬çš„æƒ…å†µ
            if text_content:
                http_urls = self._find_image_urls_in_text(text_content)
                for url in http_urls:
                    # grok2api é€‚é…ï¼šè·³è¿‡ä¸´æ—¶ç¼“å­˜ URLï¼ˆå·²åœ¨ä¸Šé¢ä¸‹è½½å¹¶æ·»åŠ åˆ° image_pathsï¼‰
                    is_temp_cache = "/images/users-" in url or "/temp/image/" in url
                    if is_temp_cache:
                        logger.debug(
                            f"[grok2api é€‚é…] è·³è¿‡æ–‡æœ¬ä¸­çš„ä¸´æ—¶ç¼“å­˜ URLï¼ˆå·²ä¸‹è½½ï¼‰: {url}"
                        )
                        continue
                    if url not in image_urls:
                        image_urls.append(url)

                # æ¾æ•£æå– data:image ç‰‡æ®µï¼Œé¿å…å›  Markdown/æ¢è¡Œå¯¼è‡´é—æ¼
                loose_matches = re.finditer(
                    r"data:image/([a-zA-Z0-9.+-]+);base64,([-A-Za-z0-9+/=_\\s]+)",
                    text_content,
                    flags=re.IGNORECASE,
                )
                for m in loose_matches:
                    fmt = m.group(1)
                    b64_raw = m.group(2)
                    b64_clean = re.sub(r"\\s+", "", b64_raw)
                    image_path = await save_base64_image(b64_clean, fmt.lower())
                    if image_path:
                        image_urls.append(image_path)
                        image_paths.append(image_path)
                        logger.debug(
                            "[openai] æ¾æ•£æå– data URI æˆåŠŸ: fmt=%s len=%s",
                            fmt,
                            len(b64_clean),
                        )

        else:
            logger.debug("[openai] å“åº”ç¼ºå°‘å¯ç”¨çš„ message å­—æ®µï¼Œå°è¯• data/b64 è§£æ")

        if not (image_urls or image_paths) and fallback_texts:
            fallback_added = await self._append_images_from_texts(
                fallback_texts, image_urls, image_paths
            )
            if fallback_added and not text_content:
                text_content = (
                    " ".join(t.strip() for t in fallback_texts if t and t.strip())
                    or text_content
                )

        # OpenAI æ ¼å¼
        if not image_urls and not image_paths and response_data.get("data"):
            for image_item in response_data["data"]:
                if "url" in image_item:
                    image_url, image_path = await self._download_image(
                        image_item["url"], session, use_cache=False
                    )
                    if image_url:
                        image_urls.append(image_url)
                    if image_path:
                        image_paths.append(image_path)
                elif "b64_json" in image_item:
                    image_path = await save_base64_image(image_item["b64_json"], "png")
                    if image_path:
                        # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                        image_urls.append(image_path)
                        image_paths.append(image_path)

        if image_urls or image_paths:
            logger.debug(
                f"ğŸ–¼ï¸ OpenAI æ”¶é›†åˆ° {len(image_paths) or len(image_urls)} å¼ å›¾ç‰‡"
            )
            return image_urls, image_paths, text_content, thought_signature

        # å¦‚æœåªæœ‰æ–‡æœ¬å†…å®¹ï¼Œä¹Ÿè¿”å›
        if text_content:
            # å¦‚æœé…ç½®äº†éœ€è¦æ–‡æœ¬å“åº”ï¼Œä¸”ç¡®å®æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¿™é‡Œåº”è¯¥æŠ¥é”™è§¦å‘é‡è¯•è€Œä¸æ˜¯ç›´æ¥è¿”å›æ–‡æœ¬
            # é™¤éè¿™æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬è¯·æ±‚ï¼ˆä½†åœ¨ç”Ÿå›¾æ’ä»¶é‡Œé€šå¸¸ä¸æ˜¯ï¼‰
            detail = (
                f" | å‚è€ƒå›¾å¤„ç†æç¤º: {'; '.join(fail_reasons[:3])}"
                if fail_reasons
                else ""
            )
            logger.debug(
                "[openai] ä»…è¿”å›æ–‡æœ¬ï¼Œé•¿åº¦=%s é¢„è§ˆ=%s",
                len(text_content),
                text_content[:200],
            )
            logger.warning(f"OpenAIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæœªç”Ÿæˆå›¾åƒï¼Œå°†è§¦å‘é‡è¯•{detail}")
            logger.debug(f"OpenAIå“åº”å†…å®¹: {str(response_data)[:1000]}")
            raise APIError(
                f"å›¾åƒç”Ÿæˆå¤±è´¥ï¼šAPIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæ­£åœ¨é‡è¯•... | å“åº”é¢„è§ˆ: {str(response_data)[:300]}",
                500,
                "no_image_retry",
            )

        logger.warning(
            f"OpenAI å“åº”æ ¼å¼ä¸æ”¯æŒæˆ–æœªæ‰¾åˆ°å›¾åƒæ•°æ®ï¼Œå“åº”: {str(response_data)[:500]}"
        )

    def _normalize_message_value(self, raw_value: Any) -> dict[str, Any] | None:
        """å½’ä¸€åŒ–ä»»æ„å¸¸è§å­—æ®µä¸ºæ ‡å‡† message ç»“æ„"""
        if raw_value is None:
            return None

        if isinstance(raw_value, dict):
            if raw_value.get("role") and "content" in raw_value:
                return raw_value

            if "message" in raw_value:
                nested = self._normalize_message_value(raw_value.get("message"))
                if nested:
                    return nested

            for key in ("content", "text", "output", "result", "response"):
                if key in raw_value:
                    nested = self._normalize_message_value(raw_value.get(key))
                    if nested:
                        return nested

            return None

        if isinstance(raw_value, list):
            if raw_value:
                return {"role": "assistant", "content": raw_value}
            return None

        if isinstance(raw_value, str):
            cleaned = raw_value.strip()
            if cleaned:
                return {"role": "assistant", "content": cleaned}
            return None

        return None

    def _coerce_basic_openai_message(
        self, response_data: dict[str, Any]
    ) -> dict[str, Any] | None:
        """ä»å¸¸è§å…¼å®¹æ ¼å¼æå– messageï¼Œå…¼å®¹ body/content/text ç­‰å­—æ®µ"""

        primary_keys = [
            "message",
            "content",
            "text",
            "output",
            "result",
            "response",
        ]
        nested_keys = [
            "body",
            "modelOutput",
            "model_output",
            "response_body",
        ]

        for key in primary_keys:
            normalized = self._normalize_message_value(response_data.get(key))
            if normalized:
                return normalized

        for key in nested_keys:
            value = response_data.get(key)
            if isinstance(value, (dict, list, str)):
                normalized = self._normalize_message_value(value)
                if normalized:
                    return normalized

        return None

    def _collect_fallback_texts(self, payload: dict[str, Any]) -> list[str]:
        """æ”¶é›†å¸¸è§å­—æ®µä¸­çš„æ–‡æœ¬å“åº”ï¼Œç”¨äºå…œåº•æå– Markdown é“¾æ¥"""
        if not isinstance(payload, dict):
            return []

        candidate_keys = (
            "content",
            "text",
            "output",
            "result",
            "response",
            "message",
        )
        container_keys = (
            "body",
            "response_body",
            "modelOutput",
            "model_output",
            "modelOutputs",
            "model_outputs",
        )

        texts: list[str] = []

        def push(value: Any):
            if value is None:
                return
            if isinstance(value, str):
                cleaned = value.strip()
                if cleaned:
                    texts.append(cleaned)
                return
            if isinstance(value, list):
                for item in value:
                    push(item)
                return
            if isinstance(value, dict):
                for key in candidate_keys:
                    if key in value:
                        push(value.get(key))

        for key in candidate_keys:
            push(payload.get(key))

        for key in container_keys:
            push(payload.get(key))

        # å»é‡ä½†ä¿æŒé¡ºåº
        seen: set[str] = set()
        ordered: list[str] = []
        for text in texts:
            if text not in seen:
                seen.add(text)
                ordered.append(text)
        return ordered

    async def _append_images_from_texts(
        self,
        texts: list[str],
        image_urls: list[str],
        image_paths: list[str],
    ) -> bool:
        """ä»é¢å¤–çš„æ–‡æœ¬å­—æ®µä¸­æå– http(s)/data URI å›¾åƒ"""

        appended = False
        for text in texts:
            if not text:
                continue

            http_urls = self._find_image_urls_in_text(text)
            for url in http_urls:
                if url not in image_urls:
                    image_urls.append(url)
                    appended = True

            extra_urls, extra_paths = await self._extract_from_content(text)
            for url in extra_urls:
                if url not in image_urls:
                    image_urls.append(url)
                    appended = True
            for path in extra_paths:
                if path not in image_paths:
                    image_paths.append(path)
                    appended = True

        return appended

    async def _parse_data_uri(self, data_uri: str) -> tuple[str | None, str | None]:
        """è§£æ data URI æ ¼å¼çš„å›¾åƒ"""
        try:
            if ";base64," not in data_uri:
                logger.error("æ— æ•ˆçš„ data URI æ ¼å¼")
                return None, None

            header, base64_data = data_uri.split(";base64,", 1)
            mime_type = header.replace("data:", "")
            format_type = mime_type.split("/")[1] if "/" in mime_type else "png"

            image_path = await save_base64_image(base64_data, format_type)
            if image_path:
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                image_url = image_path
                return image_url, image_path
        except Exception as e:
            logger.error(f"è§£æ data URI å¤±è´¥: {e}")

        return None, None

    async def _extract_from_content(self, content: str) -> tuple[list[str], list[str]]:
        """ä»æ–‡æœ¬å†…å®¹ä¸­æå–æ‰€æœ‰ data URI å›¾åƒï¼Œä¿æŒé¡ºåº"""
        # OpenAI å…¼å®¹æ¥å£æœ‰æ—¶ä¼šæŠŠå›¾ç‰‡ä»¥ Markdown data URI å½¢å¼å¡è¿›çº¯æ–‡æœ¬
        # ä¸ºäº†æ›´é²æ£’ï¼Œå…è®¸å¤§å°å†™æ··æ’ã€åŒ…å« -/_ï¼Œå¹¶è·¨å¤šè¡ŒåŒ¹é…
        pattern = re.compile(
            r"data\s*:\s*image/([a-zA-Z0-9.+-]+)\s*;\s*base64\s*,\s*([-A-Za-z0-9+/=_\s]+)",
            flags=re.IGNORECASE,
        )
        matches = pattern.findall(content)

        image_urls: list[str] = []
        image_paths: list[str] = []

        for image_format, base64_string in matches:
            # å…ˆç®€å•æ¸…æ´—éæ³•å­—ç¬¦ï¼Œé¿å…å› æ„å¤–æ’å…¥çš„ç¬¦å·å¯¼è‡´è§£ç å¤±è´¥
            cleaned_b64 = re.sub(r"[^A-Za-z0-9+/=_-]", "", base64_string)
            image_path = await save_base64_image(
                cleaned_b64 or base64_string, image_format.lower()
            )
            if image_path:
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                image_url = image_path
                image_urls.append(image_url)
                image_paths.append(image_path)

        return image_urls, image_paths

    def _find_image_urls_in_text(self, text: str) -> list[str]:
        """ä»æ–‡æœ¬/Markdownä¸­æå–å¯ç”¨çš„ http(s) å›¾ç‰‡é“¾æ¥"""
        if not text:
            return []

        # Markdown å›¾ç‰‡è¯­æ³•ä¸è£¸éœ²çš„å›¾ç‰‡é“¾æ¥
        markdown_pattern = r"!\[[^\]]*\]\((https?://[^)]+)\)"
        # Markdown å›¾ç‰‡è¯­æ³•ä¸­çš„ data URIï¼ˆå¦‚ ![image](data:image/png;base64,...)ï¼‰
        markdown_data_uri_pattern = r"!\[[^\]]*\]\((data:image/[^)]+)\)"
        # grok2api é€‚é…ï¼šæ”¯æŒç›¸å¯¹è·¯å¾„ (å¦‚ ![image](/images/xxx))
        markdown_relative_pattern = r"!\[[^\]]*\]\((/[^)]+|[^/:)]+/[^)]+)\)"
        raw_pattern = (
            r"(https?://[^\s)]+\.(?:png|jpe?g|gif|webp|bmp|tiff|avif))(?:\b|$)"
        )
        spaced_pattern = r"(https?\s*:\s*/\s*/[^\s)]+)"

        urls: list[str] = []
        seen: set[str] = set()

        def _push(candidate: str):
            cleaned = candidate.strip().replace("&amp;", "&").rstrip(").,;")
            # grok2api é€‚é…ï¼šç§»é™¤ URL ä¸¤ç«¯çš„å¼•å·ï¼ˆå•å¼•å·æˆ–åŒå¼•å·ï¼‰
            cleaned = cleaned.strip("'\"")
            if cleaned and cleaned not in seen:
                seen.add(cleaned)
                urls.append(cleaned)

        for pattern in (markdown_pattern, markdown_data_uri_pattern, raw_pattern):
            for match in re.findall(pattern, text, flags=re.IGNORECASE):
                _push(match)

        # grok2api é€‚é…ï¼šæå–ç›¸å¯¹è·¯å¾„
        for match in re.findall(markdown_relative_pattern, text, flags=re.IGNORECASE):
            if not match.startswith(("http://", "https://", "data:")):
                _push(match)

        # é€‚é…å¸¦ç©ºæ ¼çš„ http:// ç‰‡æ®µï¼ˆå¦‚ "http: //1. 2. 3. 4/image.png"ï¼‰
        for match in re.findall(spaced_pattern, text, flags=re.IGNORECASE):
            compact = re.sub(r"\s+", "", match)
            if compact.lower().startswith(("http://", "https://")):
                _push(compact)

        return urls

    async def _download_image(
        self,
        image_url: str,
        session: aiohttp.ClientSession,
        use_cache: bool = False,
    ) -> tuple[str | None, str | None]:
        """ä¸‹è½½å¹¶ä¿å­˜å›¾åƒï¼Œå¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤å…³é—­ä»¥é¿å…è¿”å›æ—§å›¾ï¼‰"""
        cleaned_url = (
            image_url.replace("&amp;", "&") if isinstance(image_url, str) else image_url
        )
        parsed = urllib.parse.urlparse(cleaned_url)
        is_http = parsed.scheme in {"http", "https"}
        cache_key = None

        # é’ˆå¯¹ CQ ç å›¾æœåŠ¡å™¨å¢åŠ ä¸“ç”¨è¯·æ±‚å¤´
        headers: dict[str, str] = {}
        if is_http:
            headers.update(
                {
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36"
                    ),
                    "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
                    "Accept-Language": "zh-CN,zh;q=0.9",
                    "Connection": "keep-alive",
                }
            )
            if "gchat.qpic.cn" in (parsed.netloc or ""):
                headers["Referer"] = "https://qun.qq.com"
            elif parsed.scheme and parsed.netloc:
                headers["Referer"] = f"{parsed.scheme}://{parsed.netloc}"

        # ç¼“å­˜å‘½ä¸­ç›´æ¥è¿”å›ï¼Œå‡å°‘é‡å¤ä¸‹è½½ä¸å†…å­˜å ç”¨
        if cache_key:
            try:
                IMAGE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
                cached = next(IMAGE_CACHE_DIR.glob(f"{cache_key}.*"), None)
                if cached and cached.exists() and cached.stat().st_size > 0:
                    logger.debug(f"å›¾åƒä¸‹è½½å‘½ä¸­ç¼“å­˜: {cleaned_url}")
                    return str(cached), str(cached)
            except Exception as e:
                logger.debug(f"æ£€æŸ¥å›¾åƒç¼“å­˜å¤±è´¥: {e}")

        max_retries = 1
        retry_interval = 1.0

        for attempt in range(1, max_retries + 1):
            try:
                logger.debug(
                    f"æ­£åœ¨ä¸‹è½½å›¾åƒ: {cleaned_url[:100]}... å°è¯• {attempt}/{max_retries}"
                )

                async with session.get(
                    cleaned_url,
                    timeout=aiohttp.ClientTimeout(total=30),
                    proxy=self.proxy,
                    headers=headers or None,
                ) as response:
                    if response.status != 200:
                        try:
                            err_text = await response.text()
                        except Exception:
                            err_text = ""

                        response_reason = response.reason or ""
                        response_content_type = response.headers.get("Content-Type", "")
                        query_params = urllib.parse.parse_qs(parsed.query)
                        param_issues: list[str] = []

                        # ä»…åœ¨å‡ºç° 400 é”™è¯¯æ—¶è¿›è¡Œå‚æ•°åˆæ³•æ€§æ£€æŸ¥
                        if response.status == 400:
                            appid = (query_params.get("appid") or [None])[0]
                            if appid and not re.fullmatch(r"[A-Za-z0-9]+", appid):
                                param_issues.append("appid æ ¼å¼å¼‚å¸¸ï¼ˆä»…å…è®¸å­—æ¯æ•°å­—ï¼‰")

                            fileid = (query_params.get("fileid") or [None])[0]
                            if fileid and not re.fullmatch(r"[A-Za-z0-9._-]+", fileid):
                                param_issues.append(
                                    "fileid æ ¼å¼å¼‚å¸¸ï¼ˆä»…å…è®¸å­—æ¯æ•°å­—ã€.ã€_ã€-ï¼‰"
                                )

                            rkey = (query_params.get("rkey") or [None])[0]
                            if rkey and re.search(r"[^A-Za-z0-9._-]", rkey):
                                param_issues.append("rkey åŒ…å«ç‰¹æ®Šå­—ç¬¦")

                            spec = (query_params.get("spec") or [None])[0]
                            if spec and not str(spec).isdigit():
                                param_issues.append("spec å‚æ•°åº”ä¸ºæ•°å­—")

                        # æ ¹æ®å“åº”å†…å®¹ä¸æ ¡éªŒç»“æœç»™å‡ºå»ºè®®
                        suggestions: list[str] = []
                        if " " in cleaned_url or "%20" in cleaned_url:
                            suggestions.append("URLæ ¼å¼é”™è¯¯ â†’ æ£€æŸ¥URLç¼–ç ")
                        if param_issues:
                            suggestions.append("å‚æ•°é”™è¯¯ â†’ æ£€æŸ¥å‚æ•°æ ¼å¼")
                        err_lower = err_text.lower() if err_text else ""
                        if any(keyword in err_lower for keyword in ["auth", "key"]):
                            suggestions.append("è®¤è¯é”™è¯¯ â†’ æ£€æŸ¥APIå¯†é’¥")
                        if any(
                            keyword in err_lower
                            for keyword in ["limit", "é¢‘ç‡", "é™åˆ¶"]
                        ):
                            suggestions.append("æœåŠ¡å™¨é™åˆ¶ â†’ å»ºè®®ç¨åé‡è¯•")
                        if not suggestions:
                            suggestions.append("æœåŠ¡å™¨é™åˆ¶ â†’ å»ºè®®ç¨åé‡è¯•")

                        logger.error(
                            "ä¸‹è½½å›¾åƒå¤±è´¥: HTTP %s %s url=%s å“åº”æ‘˜è¦=%s å»ºè®®=%s",
                            response.status,
                            response_reason,
                            cleaned_url,
                            err_text[:200],
                            "ï¼›".join(dict.fromkeys(suggestions)),
                        )

                        if self.verbose_logging:
                            logger.debug(
                                "HTTP 400 å‚æ•°æ£€æŸ¥ç»“æœ: %s",
                                "; ".join(param_issues)
                                if param_issues
                                else "æœªå‘ç°æ˜æ˜¾å¼‚å¸¸",
                            )
                            logger.debug("å®Œæ•´è¯·æ±‚å¤´: %s", headers or {})
                            logger.debug(
                                "User-Agent: %s", (headers or {}).get("User-Agent", "")
                            )
                            logger.debug(
                                "Content-Type: %s, Accept: %s",
                                (headers or {}).get("Content-Type", "æœªè®¾ç½®"),
                                (headers or {}).get("Accept", "æœªè®¾ç½®"),
                            )
                            logger.debug(
                                "æœåŠ¡å™¨å“åº”è¯¦æƒ…: status=%s, reason=%s, phrase=%s, content-type=%s",
                                response.status,
                                response_reason,
                                getattr(response, "reason", ""),
                                response_content_type,
                            )
                            logger.debug(
                                "æœåŠ¡å™¨å“åº”ä½“é¢„è§ˆ: %s",
                                err_text[:1000] if err_text else "<empty>",
                            )

                        if response.status == 400 and attempt < max_retries:
                            await asyncio.sleep(retry_interval * attempt)
                            continue
                        return None, None

                    content_type = response.headers.get("Content-Type", "")

                    if "/" in content_type:
                        image_format = content_type.split("/")[1].split(";")[0] or "png"
                    else:
                        image_format = "png"

                    target_path = None
                    if cache_key:
                        target_path = IMAGE_CACHE_DIR / f"{cache_key}.{image_format}"

                    image_path = await save_image_stream(
                        response.content, image_format, target_path=target_path
                    )
                    if image_path:
                        # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                        image_url_local = image_path
                        return image_url_local, image_path
            except aiohttp.ClientError as e:
                logger.error(f"ä¸‹è½½å›¾åƒå‘ç”Ÿç½‘ç»œå¼‚å¸¸: {e}")
            except Exception as e:
                logger.error(f"ä¸‹è½½å›¾åƒå¤±è´¥: {e}")

            if attempt < max_retries:
                await asyncio.sleep(retry_interval * attempt)

        return None, None


# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºAPIClientåˆ«å
APIClient = GeminiAPIClient

# å…¨å±€ API å®¢æˆ·ç«¯å®ä¾‹
_api_client: GeminiAPIClient | None = None


def get_api_client(api_keys: list[str]) -> GeminiAPIClient:
    """è·å–æˆ–åˆ›å»º API å®¢æˆ·ç«¯å®ä¾‹"""
    global _api_client
    if _api_client is None:
        _api_client = GeminiAPIClient(api_keys)
    return _api_client


def clear_api_client():
    """æ¸…é™¤å…¨å±€ API å®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _api_client
    _api_client = None

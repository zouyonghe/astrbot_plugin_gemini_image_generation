"""
APIå®¢æˆ·ç«¯æ¨¡å—
æä¾›Google Geminiå’ŒOpenAIå…¼å®¹APIçš„å®¢æˆ·ç«¯å®ç°
"""

from __future__ import annotations

import asyncio
import base64
import json
import os
import re
import tempfile
import time
import urllib.parse
import urllib.request
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import aiohttp

from astrbot.api import logger

try:
    from .tl_utils import (
        IMAGE_CACHE_DIR,
        SUPPORTED_IMAGE_MIME_TYPES,
        coerce_supported_image,
        coerce_supported_image_bytes,
        download_qq_image,
        encode_file_to_base64,
        get_plugin_data_dir,
        normalize_image_input,
        resolve_image_source_to_path,
        save_base64_image,
        save_image_data,
        save_image_stream,
    )
except ImportError:
    from pathlib import Path

    async def save_base64_image(
        base64_data: str, image_format: str = "png"
    ) -> str | None:
        """å ä½ç¬¦å‡½æ•°"""
        return None

    async def save_image_data(
        image_data: bytes, image_format: str = "png"
    ) -> str | None:
        """å ä½ç¬¦å‡½æ•°"""
        return None

    async def save_image_stream(
        stream_reader, image_format: str = "png", target_path=None
    ):
        return None

    def encode_file_to_base64(file_path, chunk_size: int = 65536) -> str:
        return ""

    def get_plugin_data_dir() -> Path:
        return Path(".")

    IMAGE_CACHE_DIR = get_plugin_data_dir() / "images" / "download_cache"
    SUPPORTED_IMAGE_MIME_TYPES = {
        "image/png",
        "image/jpeg",
        "image/webp",
        "image/heic",
        "image/heif",
    }

    def coerce_supported_image_bytes(mime_type, raw_bytes):
        return None, None

    def coerce_supported_image(mime_type, base64_data):
        return None, None

    async def normalize_image_input(
        image_input: Any, *, image_cache_dir=None, image_input_mode="force_base64"
    ):
        return None, None


@dataclass
class ApiRequestConfig:
    """API è¯·æ±‚é…ç½®ï¼ˆåŸºäº Gemini å®˜æ–¹æ–‡æ¡£ï¼‰"""

    model: str
    prompt: str
    api_type: str = "openai"
    api_base: str | None = None
    api_key: str | None = None
    resolution: str | None = None
    aspect_ratio: str | None = None
    enable_grounding: bool = False
    response_modalities: str = "TEXT_IMAGE"  # é»˜è®¤åŒæ—¶è¿”å›æ–‡æœ¬å’Œå›¾åƒ
    max_tokens: int = 1000
    reference_images: list[str] | None = None
    response_text: str | None = None  # å­˜å‚¨æ–‡æœ¬å“åº”
    enable_smart_retry: bool = True  # æ™ºèƒ½é‡è¯•å¼€å…³
    enable_text_response: bool = False  # æ–‡æœ¬å“åº”å¼€å…³
    force_resolution: bool = False  # å¼ºåˆ¶ä¼ é€’åˆ†è¾¨ç‡å‚æ•°
    verbose_logging: bool = False  # è¯¦ç»†æ—¥å¿—å¼€å…³
    image_input_mode: str = "force_base64"  # å‚è€ƒå›¾ç»Ÿä¸€è½¬ base64

    # å®˜æ–¹æ–‡æ¡£æ¨èå‚æ•°
    temperature: float = 0.7  # æ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼Œ0.0-1.0
    seed: int | None = None  # å›ºå®šç§å­ä»¥ç¡®ä¿ä¸€è‡´æ€§
    safety_settings: dict | None = None  # å®‰å…¨è®¾ç½®


class APIError(Exception):
    """API é”™è¯¯åŸºç±»"""

    def __init__(self, message: str, status_code: int = None, error_type: str = None):
        super().__init__(message)
        self.message = message
        self.status_code = status_code
        self.error_type = error_type


class GeminiAPIClient:
    """éµå¾ªå®˜æ–¹ API è§„èŒƒçš„ Gemini API å®¢æˆ·ç«¯

    ç‰¹æ€§ï¼š
    - æ”¯æŒ Google å®˜æ–¹ API å’Œ OpenAI API
    - æ”¯æŒè‡ªå®šä¹‰ API Base URLï¼ˆåä»£ï¼‰
    - æ”¯æŒä»»æ„æ¨¡å‹åç§°
    - éµå¾ªå®˜æ–¹ Gemini API è§„èŒƒ
    """

    # Google å®˜æ–¹ API é»˜è®¤åœ°å€
    GOOGLE_API_BASE = "https://generativelanguage.googleapis.com/v1beta"

    # OpenAI API é»˜è®¤åœ°å€
    OPENAI_API_BASE = "https://api.openai.com/v1"

    def __init__(self, api_keys: list[str]):
        """
        åˆå§‹åŒ– API å®¢æˆ·ç«¯

        Args:
            api_keys: API å¯†é’¥åˆ—è¡¨
        """
        self.api_keys = api_keys or []
        self.current_key_index = 0
        self._lock = asyncio.Lock()
        self.proxy = (
            os.environ.get("HTTPS_PROXY")
            or os.environ.get("https_proxy")
            or os.environ.get("HTTP_PROXY")
            or os.environ.get("http_proxy")
        )
        if self.proxy:
            logger.debug(f"æ£€æµ‹åˆ°ä»£ç†é…ç½®ï¼Œä½¿ç”¨ä»£ç†: {self.proxy}")
        logger.debug(f"API å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œæ”¯æŒ {len(self.api_keys)} ä¸ª API å¯†é’¥")
        self.verbose_logging: bool = False

    @staticmethod
    def _coerce_supported_image_bytes(
        mime_type: str | None, raw_bytes: bytes
    ) -> tuple[str | None, str | None]:
        return coerce_supported_image_bytes(mime_type, raw_bytes)

    @staticmethod
    def _coerce_supported_image(
        mime_type: str | None, base64_data: str
    ) -> tuple[str | None, str | None]:
        return coerce_supported_image(mime_type, base64_data)

    @staticmethod
    def _validate_and_normalize_b64(
        raw_data: str, *, context: str = "", allow_relaxed_return: bool = False
    ) -> str:
        """
        æ ¡éªŒå¹¶å½’ä¸€åŒ– base64ï¼š
        - å»æ‰å‰ç¼€/æ¢è¡Œ
        - å°è¯•æ ‡å‡†è§£ç å¤±è´¥åå›é€€ urlsafe è§£ç ï¼ˆè¡¥é½ paddingï¼‰
        - å†å¤±è´¥å°è¯•å®½æ¾è¿‡æ»¤/è‡ªåŠ¨è¡¥é½ padding åè§£ç é‡ç¼–ç 
        è¿”å›å¯ç›´æ¥ä½¿ç”¨çš„çº¯ base64 å­—ç¬¦ä¸²ï¼Œå¤±è´¥æŠ›å‡ºå¼‚å¸¸ã€‚
        """
        cleaned = (raw_data or "").strip().replace("\n", "")
        if ";base64," in cleaned:
            _, _, cleaned = cleaned.partition(";base64,")

        def try_decode(data: str) -> str:
            base64.b64decode(data, validate=True)
            return data

        try:
            return try_decode(cleaned)
        except Exception:
            # å›é€€ urlsafe base64
            alt = cleaned.replace("-", "+").replace("_", "/")
            pad_len = (-len(alt)) % 4
            if pad_len:
                alt += "=" * pad_len
            try:
                return try_decode(alt)
            except Exception as e:
                # æœ€åå°è¯•å®½æ¾è¿‡æ»¤éæ³•å­—ç¬¦/è¡¥é½ padding åè§£ç é‡ç¼–ç 
                relaxed = re.sub(r"[^A-Za-z0-9+/=_-]", "", cleaned)
                pad_len2 = (-len(relaxed)) % 4
                if pad_len2:
                    relaxed += "=" * pad_len2
                try:
                    raw = base64.b64decode(relaxed, validate=False)
                    if raw:
                        return base64.b64encode(raw).decode("utf-8")
                except Exception:
                    pass
                if allow_relaxed_return and relaxed:
                    return relaxed
                if allow_relaxed_return and cleaned:
                    # ä»æ— æ³•è§£ç æ—¶ï¼Œå…è®¸ç›´æ¥å›é€€åŸå§‹å­—ç¬¦ä¸²äº¤ç”±ä¸‹æ¸¸å¤„ç†
                    return cleaned
                raise APIError(
                    f"å‚è€ƒå›¾ base64 æ ¡éªŒå¤±è´¥{f'ï¼ˆ{context}ï¼‰' if context else ''}ï¼Œè¯·æ£€æŸ¥å›¾ç‰‡åé‡è¯•ã€‚",
                    None,
                    "invalid_reference_image",
                ) from e

    async def get_next_api_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ª API å¯†é’¥"""
        async with self._lock:
            if not self.api_keys:
                raise ValueError("API å¯†é’¥åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            key = self.api_keys[self.current_key_index % len(self.api_keys)]
            return key

    async def rotate_api_key(self):
        """è½®æ¢åˆ°ä¸‹ä¸€ä¸ª API å¯†é’¥"""
        async with self._lock:
            if len(self.api_keys) > 1:
                self.current_key_index = (self.current_key_index + 1) % len(
                    self.api_keys
                )
                logger.debug(
                    f"å·²è½®æ¢åˆ°ä¸‹ä¸€ä¸ª API å¯†é’¥ï¼Œå½“å‰ç´¢å¼•: {self.current_key_index}"
                )

    async def _prepare_google_payload(self, config: ApiRequestConfig) -> dict[str, Any]:
        """å‡†å¤‡ Google å®˜æ–¹ API è¯·æ±‚è´Ÿè½½ï¼ˆéµå¾ªå®˜æ–¹è§„èŒƒï¼‰"""
        logger.debug(
            "[FLOW_DEBUG][google] æ„å»º payload: model=%s refs=%s force_b64=%s aspect=%s res=%s",
            config.model,
            len(config.reference_images or []),
            config.image_input_mode,
            config.aspect_ratio,
            config.resolution,
        )
        parts = [{"text": config.prompt}]

        added_refs = 0
        fail_reasons: list[str] = []
        if config.reference_images:
            for image_input in config.reference_images[:14]:
                logger.debug(
                    "[REF_DEBUG][google] å¤„ç†å‚è€ƒå›¾ idx=%s type=%s preview=%s",
                    added_refs,
                    type(image_input),
                    str(image_input)[:120],
                )
                # ä¼˜å…ˆå°è¯•è§£æä¸ºæœ¬åœ°æ–‡ä»¶ï¼ˆåŒ…å«ç¼“å­˜æ–‡ä»¶ï¼‰ï¼Œå†è½¬ base64ï¼›ä¸ºé¿å…å¤ç”¨æ—§ç¼“å­˜ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•
                data = None
                mime_type = None
                local_path: str | None = None
                try:
                    local_path = await resolve_image_source_to_path(
                        image_input,
                        image_input_mode=config.image_input_mode,
                        api_client=self,
                        download_qq_image_fn=download_qq_image,
                    )
                    if local_path and Path(local_path).exists():
                        suffix = Path(local_path).suffix.lower().lstrip(".") or "png"
                        mime_type = f"image/{suffix}"
                        data = encode_file_to_base64(local_path)
                except Exception:
                    local_path = None
                    data = None
                    mime_type = None

                if not data:
                    # ç»Ÿä¸€è½¬æ¢ä¸ºå—æ”¯æŒçš„ base64ï¼Œé¿å…ç›´é“¾ä¸å¯è¾¾/æ ¼å¼ä¸ç¡®å®š
                    temp_cache = Path(
                        tempfile.mkdtemp(prefix="gemini_ref_tmp_", dir="/tmp")
                    )
                    mime_type, data = await GeminiAPIClient._normalize_image_input(
                        image_input,
                        image_input_mode=config.image_input_mode,
                        image_cache_dir=temp_cache,
                    )
                if not data and isinstance(image_input, str):
                    # å†å°è¯•é€šè¿‡ QQ ä¸‹è½½å™¨ç›´æ¥è·å– data URL
                    try:
                        qq_data = await download_qq_image(str(image_input))
                        if qq_data:
                            if ";base64," in qq_data:
                                mime_type = qq_data.split(";", 1)[0].replace(
                                    "data:", ""
                                )
                                _, _, raw_b64 = qq_data.partition(";base64,")
                                data = raw_b64
                            else:
                                data = qq_data
                    except Exception:
                        pass
                if not data:
                    # æœ€ç»ˆå…œåº•ï¼šç›´æ¥ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²äº¤ç»™ APIï¼Œé¿å…åœ¨æ’ä»¶ä¾§æ‹¦æˆª
                    data = str(image_input).strip()
                    mime_type = mime_type or "image/png"

                # ä¸¥æ ¼æ ¡éªŒ base64ï¼Œé¿å…ä¼ å…¥æ— æ•ˆæ•°æ®å¯¼è‡´ inline_data è§£ç é”™è¯¯
                try:
                    data = GeminiAPIClient._validate_and_normalize_b64(
                        data, context="google-inline", allow_relaxed_return=True
                    )
                except APIError as e:
                    # å¦‚æœéªŒè¯å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹/å»å‰ç¼€çš„ base64 é€ä¼ ï¼Œé¿å…åœ¨æ’ä»¶ä¾§æ‹¦æˆª
                    raw = str(data).strip()
                    if ";base64," in raw:
                        _, _, raw = raw.partition(";base64,")
                    data = raw
                    logger.debug(
                        "è·³è¿‡ base64 æ ¡éªŒï¼Œç›´æ¥é€ä¼ å‚è€ƒå›¾: %s... | %s",
                        str(image_input)[:80],
                        e.message,
                    )
                    fail_reasons.append(
                        f"idx={added_refs} base64æ ¡éªŒå¤±è´¥å·²é€ä¼  | {e.message}"
                    )
                logger.debug(
                    "[REF_DEBUG][google] æˆåŠŸå¤„ç†å‚è€ƒå›¾ idx=%s mime=%s size=%s",
                    added_refs,
                    mime_type,
                    len(str(data)) if data else 0,
                )

                parts.append({"inlineData": {"mimeType": mime_type, "data": data}})
                added_refs += 1

        if config.reference_images and added_refs == 0:
            raise APIError(
                "å‚è€ƒå›¾å…¨éƒ¨æ— æ•ˆæˆ–ä¸‹è½½å¤±è´¥ï¼Œè¯·é‡æ–°å‘é€å›¾ç‰‡åé‡è¯•ã€‚"
                + (f" è¯¦æƒ…: {'; '.join(fail_reasons[:3])}" if fail_reasons else ""),
                None,
                "invalid_reference_image",
            )

        contents = [{"role": "user", "parts": parts}]

        generation_config = {"responseModalities": ["TEXT", "IMAGE"]}

        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œå›¾åƒç”Ÿæˆå¿…é¡»åŒæ—¶åŒ…å« TEXT å’Œ IMAGE modalities
        # è¿™æ ·å¯ä»¥ç¡®ä¿è¿”å›å›¾åƒè€Œä¸æ˜¯çº¯æ–‡æœ¬
        modalities_map = {
            "TEXT": ["TEXT"],
            "IMAGE": ["IMAGE"],
            "TEXT_IMAGE": ["TEXT", "IMAGE"],
        }

        # è·å–é…ç½®çš„æ¨¡æ€
        modalities = modalities_map.get(config.response_modalities, ["TEXT", "IMAGE"])

        # ç¡®ä¿åŒ…å«å›¾åƒæ¨¡æ€
        if "IMAGE" not in modalities:
            logger.warning("é…ç½®ä¸­ç¼ºå°‘ IMAGE modalityï¼Œè‡ªåŠ¨æ·»åŠ ä»¥æ”¯æŒå›¾åƒç”Ÿæˆ")
            modalities.append("IMAGE")

        # ç¡®ä¿åŒ…å«æ–‡æœ¬æ¨¡æ€
        if "TEXT" not in modalities:
            logger.debug("æ·»åŠ  TEXT modality ä»¥æä¾›æ›´å¥½çš„å…¼å®¹æ€§")
            modalities.append("TEXT")

        generation_config["responseModalities"] = modalities
        logger.debug(f"å“åº”æ¨¡æ€: {modalities}")

        image_config = {}

        # æ ¹æ®å®˜æ–¹æ–‡æ¡£è®¾ç½®å›¾åƒå°ºå¯¸
        if config.resolution:
            resolution = config.resolution.upper()

            if resolution in ["1K", "1024x1024"]:
                image_config["image_size"] = "1K"
                logger.debug("è®¾ç½®å›¾åƒå°ºå¯¸: 1K")
            elif resolution in ["2K", "2048x2048"]:
                image_config["image_size"] = "2K"
                logger.debug("è®¾ç½®å›¾åƒå°ºå¯¸: 2K")
            elif resolution in ["4K", "4096x4096"]:
                image_config["image_size"] = "4K"
                logger.debug("è®¾ç½®å›¾åƒå°ºå¯¸: 4K")
            else:
                # é»˜è®¤ä½¿ç”¨1K
                image_config["image_size"] = "1K"
                logger.warning(f"ä¸æ”¯æŒçš„åˆ†è¾¨ç‡: {config.resolution}ï¼Œä½¿ç”¨é»˜è®¤å°ºå¯¸ 1K")

        # è®¾ç½®é•¿å®½æ¯”
        if config.aspect_ratio and ":" in config.aspect_ratio:
            # å°†é•¿å®½æ¯”è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            ratio_map = {
                "1:1": "1:1",
                "16:9": "16:9",
                "9:16": "9:16",
                "3:2": "3:2",
                "4:3": "4:3",
            }
            ratio = ratio_map.get(config.aspect_ratio, config.aspect_ratio)
            image_config["aspect_ratio"] = ratio
            logger.debug(f"è®¾ç½®é•¿å®½æ¯”: {ratio}")
        elif config.aspect_ratio:
            logger.warning(
                f"ä¸æ”¯æŒçš„é•¿å®½æ¯”æ ¼å¼: {config.aspect_ratio}ï¼Œå°†ä½¿ç”¨é»˜è®¤é•¿å®½æ¯”"
            )

        if image_config:
            generation_config["image_config"] = image_config

        # æ·»åŠ å®˜æ–¹æ–‡æ¡£æ¨èå‚æ•°
        if config.temperature is not None:
            generation_config["temperature"] = config.temperature
        if config.seed is not None:
            generation_config["seed"] = config.seed
        if config.safety_settings:
            generation_config["safetySettings"] = config.safety_settings

        tools = []
        if config.enable_grounding:
            tools.append({"google_search": {}})

        payload = {"contents": contents, "generationConfig": generation_config}

        if tools:
            payload["tools"] = tools

        # è°ƒè¯•ï¼šè®°å½• image_config
        if "image_config" in generation_config:
            logger.debug(
                f"å®é™…å‘é€çš„ image_config: {generation_config['image_config']}"
            )

        return payload

    @staticmethod
    async def _prepare_openai_payload(config: ApiRequestConfig) -> dict[str, Any]:
        """å‡†å¤‡ OpenAI API è¯·æ±‚è´Ÿè½½"""
        logger.debug(
            "[FLOW_DEBUG][openai] æ„å»º payload: model=%s refs=%s force_b64=%s aspect=%s res=%s",
            config.model,
            len(config.reference_images or []),
            True,
            config.aspect_ratio,
            config.resolution,
        )
        message_content = [
            {"type": "text", "text": f"Generate an image: {config.prompt}"}
        ]

        force_b64 = True

        def _ensure_valid_base64(data: str, context: str):
            try:
                cleaned = data.strip().replace("\n", "")
                if ";base64," in cleaned:
                    _, _, cleaned = cleaned.partition(";base64,")
                base64.b64decode(cleaned, validate=True)
            except Exception:
                raise APIError(
                    f"å‚è€ƒå›¾ base64 æ ¡éªŒå¤±è´¥ï¼ˆforce_base64ï¼‰ï¼Œæ¥æº: {context}",
                    None,
                    "invalid_reference_image",
                )

        added_refs = 0
        fail_reasons: list[str] = []
        if config.reference_images:
            # æœ¬åœ°ç¼“å­˜é¿å…é‡å¤å¤„ç†åŒä¸€å¼•ç”¨å›¾ï¼Œè®°å½•è€—æ—¶ä¾¿äºæ€§èƒ½è§‚å¯Ÿ
            processed_cache: dict[str, dict[str, Any]] = {}
            supported_exts = {
                "jpg",
                "jpeg",
                "png",
                "webp",
                "gif",
                "bmp",
                "tif",
                "tiff",
                "heic",
                "avif",
            }
            total_start = time.perf_counter()

            for idx, image_input in enumerate(config.reference_images[:6]):
                logger.debug(
                    "[REF_DEBUG][openai] å¤„ç†å‚è€ƒå›¾ idx=%s type=%s preview=%s",
                    idx,
                    type(image_input),
                    str(image_input)[:120],
                )
                per_start = time.perf_counter()
                image_str = str(image_input).strip()
                if not image_str:
                    logger.warning(f"è·³è¿‡ç©ºç™½å‚è€ƒå›¾åƒ: idx={idx}")
                    continue

                if "&amp;" in image_str:
                    image_str = image_str.replace("&amp;", "&")

                # å‘½ä¸­ç¼“å­˜ç›´æ¥å¤ç”¨ï¼Œé¿å…é‡å¤ base64 å¤„ç†
                if image_str in processed_cache:
                    logger.debug(f"å‚è€ƒå›¾åƒå‘½ä¸­ç¼“å­˜: idx={idx}")
                    message_content.append(processed_cache[image_str])
                    continue

                parsed = urllib.parse.urlparse(image_str)
                image_payload: dict[str, Any] | None = None

                try:
                    # http(s) URLï¼šä¼˜å…ˆç”¨ç¼“å­˜/æœ¬åœ°æ–‡ä»¶ï¼Œå†è§„èŒƒåŒ–ä¸º base64
                    if parsed.scheme in ("http", "https") and parsed.netloc:
                        data = None
                        mime_type = None
                        local_path: str | None = None
                        try:
                            local_path = await resolve_image_source_to_path(
                                image_input,
                                image_input_mode=config.image_input_mode,
                                api_client=None,
                                download_qq_image_fn=download_qq_image,
                            )
                            if local_path and Path(local_path).exists():
                                suffix = (
                                    Path(local_path).suffix.lower().lstrip(".") or "png"
                                )
                                mime_type = f"image/{suffix}"
                                data = encode_file_to_base64(local_path)
                        except Exception:
                            local_path = None
                            data = None
                            mime_type = None

                        if not data:
                            temp_cache = Path(
                                tempfile.mkdtemp(prefix="gemini_ref_tmp_", dir="/tmp")
                            )
                            (
                                mime_type,
                                data,
                            ) = await GeminiAPIClient._normalize_image_input(
                                image_input,
                                image_input_mode=config.image_input_mode,
                                image_cache_dir=temp_cache,
                            )
                        if not data and isinstance(image_input, str):
                            try:
                                qq_data = await download_qq_image(str(image_input))
                                if qq_data:
                                    if ";base64," in qq_data:
                                        mime_type = qq_data.split(";", 1)[0].replace(
                                            "data:", ""
                                        )
                                        _, _, raw_b64 = qq_data.partition(";base64,")
                                        data = raw_b64
                                    else:
                                        data = qq_data
                            except Exception:
                                pass
                        if not data:
                            # æœ€ç»ˆå…œåº•ï¼šç›´æ¥ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²äº¤ç»™ APIï¼Œé¿å…åœ¨æ’ä»¶ä¾§æ‹¦æˆª
                            data = str(image_input).strip()
                            mime_type = mime_type or "image/png"

                        if not mime_type or not mime_type.startswith("image/"):
                            mime_type = "image/png"

                        try:
                            cleaned = GeminiAPIClient._validate_and_normalize_b64(
                                data,
                                context=f"openai-url-idx-{idx}",
                                allow_relaxed_return=True,
                            )
                        except APIError as e:
                            # æ ¡éªŒå¤±è´¥æ—¶ç›´æ¥é€ä¼ åŸå§‹/å»å‰ç¼€çš„ base64ï¼Œé¿å…ä¸¢å¼ƒå‚è€ƒå›¾
                            raw = str(data).strip()
                            if ";base64," in raw:
                                _, _, raw = raw.partition(";base64,")
                            cleaned = raw
                            logger.debug(
                                "openai-url æ ¡éªŒå¤±è´¥ï¼Œç›´æ¥é€ä¼  base64ï¼šidx=%s | %s",
                                idx,
                                e.message,
                            )
                            fail_reasons.append(
                                f"idx={idx} openai-url æ ¡éªŒå¤±è´¥ï¼Œå·²é€ä¼  | {e.message}"
                            )

                        payload_url = (
                            cleaned
                            if force_b64
                            else f"data:{mime_type};base64,{cleaned}"
                        )

                        image_payload = {
                            "type": "image_url",
                            "image_url": {"url": payload_url},
                        }
                        logger.debug(
                            "OpenAIå…¼å®¹APIä½¿ç”¨æœ¬åœ°è½¬ç å‚è€ƒå›¾: idx=%s mime=%s",
                            idx,
                            mime_type,
                        )

                    # data URLï¼šèµ°ç»Ÿä¸€çš„è§„èŒƒåŒ–æµç¨‹ï¼Œç¡®ä¿æ ¼å¼å—æ”¯æŒ
                    elif (
                        image_str.startswith("data:image/") and ";base64," in image_str
                    ):
                        mime_type, data = await GeminiAPIClient._normalize_image_input(
                            image_str, image_input_mode=config.image_input_mode
                        )
                        if not data:
                            data = str(image_str).strip()

                        if not mime_type or not mime_type.startswith("image/"):
                            mime_type = "image/png"

                        ext = mime_type.split("/")[-1]
                        if ext and ext not in supported_exts:
                            logger.debug(
                                "data URL å›¾ç‰‡æ ¼å¼ä¸å¸¸è§: idx=%s mime=%s",
                                idx,
                                mime_type,
                            )

                        try:
                            normalized = GeminiAPIClient._validate_and_normalize_b64(
                                data,
                                context=f"openai-dataurl-{idx}",
                                allow_relaxed_return=True,
                            )
                        except APIError as e:
                            raw = str(data).strip()
                            if ";base64," in raw:
                                _, _, raw = raw.partition(";base64,")
                            normalized = raw
                            logger.debug(
                                "data URL æ ¡éªŒå¤±è´¥ï¼Œç›´æ¥é€ä¼  base64ï¼šidx=%s | %s",
                                idx,
                                e.message,
                            )
                            fail_reasons.append(
                                f"idx={idx} dataurl æ ¡éªŒå¤±è´¥ï¼Œå·²é€ä¼  | {e.message}"
                            )

                        if force_b64:
                            payload_url = normalized
                        else:
                            payload_url = f"data:{mime_type};base64,{normalized}"

                        image_payload = {
                            "type": "image_url",
                            "image_url": {"url": payload_url},
                        }
                        logger.debug(
                            "OpenAIå…¼å®¹APIä½¿ç”¨è§„èŒƒåŒ–data URLå‚è€ƒå›¾: idx=%s mime=%s",
                            idx,
                            mime_type,
                        )

                    # å…¶ä»–è¾“å…¥äº¤ç»™è§„èŒƒåŒ–é€»è¾‘ï¼Œè‡ªåŠ¨è½¬æ¢ä¸º data URL
                    else:
                        mime_type, data = await GeminiAPIClient._normalize_image_input(
                            image_input, image_input_mode=config.image_input_mode
                        )
                        if not data:
                            # ä¸ google åˆ†æ”¯ä¸€è‡´ï¼šå…œåº•ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²ï¼Œé¿å…ç›´æ¥ä¸­æ–­
                            data = str(image_input).strip()
                            mime_type = mime_type or "image/png"
                            fail_reasons.append(
                                f"idx={idx} normalize ä¸ºç©ºï¼Œå·²ç”¨åŸå§‹å­—ç¬¦ä¸²å…œåº•"
                            )

                        if not mime_type or not mime_type.startswith("image/"):
                            logger.debug(
                                "æœªæ£€æµ‹åˆ°æ˜ç¡®çš„å›¾ç‰‡ MIMEï¼Œé»˜è®¤ä½¿ç”¨ image/png: idx=%s",
                                idx,
                            )
                            mime_type = "image/png"

                        ext = mime_type.split("/")[-1]
                        if ext and ext not in supported_exts:
                            logger.debug(
                                "è§„èŒƒåŒ–åå›¾ç‰‡æ ¼å¼ä¸å¸¸è§: idx=%s mime=%s", idx, mime_type
                            )

                        try:
                            normalized = GeminiAPIClient._validate_and_normalize_b64(
                                data,
                                context=f"openai-other-{idx}",
                                allow_relaxed_return=True,
                            )
                        except APIError as e:
                            raw = str(data).strip()
                            if ";base64," in raw:
                                _, _, raw = raw.partition(";base64,")
                            normalized = raw
                            logger.debug(
                                "å‚è€ƒå›¾æ ¡éªŒå¤±è´¥ï¼Œç›´æ¥é€ä¼  base64ï¼šidx=%s type=%s | %s",
                                idx,
                                type(image_input),
                                e.message,
                            )
                            fail_reasons.append(
                                f"idx={idx} other æ ¡éªŒå¤±è´¥ï¼Œå·²é€ä¼  | {e.message}"
                            )
                        payload_url = (
                            normalized
                            if force_b64
                            else f"data:{mime_type};base64,{normalized}"
                        )

                        image_payload = {
                            "type": "image_url",
                            "image_url": {"url": payload_url},
                        }

                    if image_payload:
                        message_content.append(image_payload)
                        processed_cache[image_str] = image_payload
                        added_refs += 1
                        elapsed_ms = (time.perf_counter() - per_start) * 1000
                        logger.debug(
                            "å‚è€ƒå›¾åƒå¤„ç†å®Œæˆ: idx=%s è€—æ—¶=%.2fms æ¥æº=%s",
                            idx,
                            elapsed_ms,
                            parsed.scheme or "normalized",
                        )
                        logger.debug(
                            "[REF_DEBUG][openai] æˆåŠŸå¤„ç†å‚è€ƒå›¾ idx=%s mime=%s size=%s",
                            idx,
                            mime_type,
                            len(str(cleaned if "cleaned" in locals() else data))
                            if (locals().get("cleaned") or data)
                            else 0,
                        )
                except APIError as e:
                    logger.warning(
                        "å¤„ç†å‚è€ƒå›¾åƒæ—¶å‡ºç°å¼‚å¸¸: idx=%s err=%s", idx, e.message or e
                    )
                    fail_reasons.append(f"idx={idx} APIError: {e.message or str(e)}")
                    continue
                except Exception as e:
                    logger.warning("å¤„ç†å‚è€ƒå›¾åƒæ—¶å‡ºç°å¼‚å¸¸: idx=%s err=%s", idx, e)
                    fail_reasons.append(f"idx={idx} Exception: {e}")
                    continue

            total_elapsed_ms = (time.perf_counter() - total_start) * 1000
            if processed_cache:
                logger.debug(
                    "å‚è€ƒå›¾åƒå¤„ç†ç»Ÿè®¡: æ€»æ•°=%s æ€»è€—æ—¶=%.2fms å¹³å‡=%.2fms",
                    len(processed_cache),
                    total_elapsed_ms,
                    total_elapsed_ms / len(processed_cache),
                )
        if config.reference_images and added_refs == 0:
            raise APIError(
                "å‚è€ƒå›¾å…¨éƒ¨æ— æ•ˆæˆ–ä¸‹è½½å¤±è´¥ï¼Œè¯·é‡æ–°å‘é€å›¾ç‰‡åé‡è¯•ã€‚"
                + (f" è¯¦æƒ…: {'; '.join(fail_reasons[:3])}" if fail_reasons else ""),
                None,
                "invalid_reference_image",
            )

        # OpenAI å…¼å®¹æ¥å£ä¸‹ï¼š
        # - ä½¿ç”¨ chat/completions
        # - modalities: ["image", "text"]
        # - image_config: {aspect_ratio, image_size}
        # - tools: [{google_search:{}}]ï¼ˆå½“å¯ç”¨æœç´¢æ¥åœ°æ—¶ï¼‰
        payload: dict[str, Any] = {
            "model": config.model,
            "messages": [{"role": "user", "content": message_content}],
            "max_tokens": config.max_tokens,
            "temperature": 0.7,
            "modalities": ["image", "text"],
            # æ˜ç¡®å…³é—­æµå¼å“åº”ï¼Œé¿å…éƒ¨åˆ† OpenAI å…¼å®¹æœåŠ¡é»˜è®¤è¿”å› SSE
            "stream": False,
        }

        # image_config ä¸ Gemini 3 Pro Image æ¨¡å‹ç›¸å…³çš„é…ç½®
        image_config: dict[str, Any] = {}

        if config.aspect_ratio:
            image_config["aspect_ratio"] = config.aspect_ratio

        # ä»…åœ¨ Gemini 3 Pro Image ç³»åˆ—æ¨¡å‹ä¸‹ä¼ é€’ image_size
        model_name = (config.model or "").lower()
        is_gemini_image_model = (
            "gemini-3-pro-image" in model_name
            or "gemini-3-pro-preview" in model_name
            or config.force_resolution  # å¦‚æœå¼ºåˆ¶å¼€å¯ï¼Œåˆ™å¿½ç•¥æ¨¡å‹åç§°æ£€æŸ¥
        )

        if is_gemini_image_model and config.resolution:
            # å‰ç«¯ router ä¾§ç›´æ¥ä¼ é€’ "1K"/"2K"/"4K"ï¼Œè¿™é‡Œä¿æŒä¸€è‡´
            image_config["image_size"] = config.resolution

        if image_config:
            payload["image_config"] = image_config

        # ä¸å‰ç«¯ router ä¸€è‡´ï¼šå¯ç”¨æœç´¢æ¥åœ°æ—¶ï¼Œé€šè¿‡ tools.google_search æ§åˆ¶
        if is_gemini_image_model and config.enable_grounding:
            payload["tools"] = [{"google_search": {}}]

        return payload

    @staticmethod
    async def _normalize_image_input(
        image_input: Any,
        image_input_mode: str = "force_base64",
        image_cache_dir=None,
    ) -> tuple[str | None, str | None]:
        """ç»Ÿä¸€è°ƒç”¨ tl_utils çš„å‚è€ƒå›¾è§„èŒƒåŒ–é€»è¾‘"""
        return await normalize_image_input(
            image_input,
            image_cache_dir=image_cache_dir or IMAGE_CACHE_DIR,
            image_input_mode=image_input_mode,
        )

    async def _get_api_url(
        self, config: ApiRequestConfig
    ) -> tuple[str, dict[str, str], dict[str, Any]]:
        """
        æ ¹æ®é…ç½®è·å– API URLã€è¯·æ±‚å¤´å’Œè´Ÿè½½

        æ™ºèƒ½å¤„ç†APIè·¯å¾„å‰ç¼€ï¼Œæ— éœ€æ‰‹åŠ¨è¾“å…¥/v1æˆ–/v1beta
        """
        # ç¡®å®š API åŸºç¡€åœ°å€ï¼ˆæ”¯æŒåä»£ï¼‰
        if config.api_base:
            api_base = config.api_base.rstrip("/")
            logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰ API Base: {api_base}")
        else:
            if config.api_type == "google":
                api_base = self.GOOGLE_API_BASE
            else:  # openai å…¼å®¹æ ¼å¼
                api_base = self.OPENAI_API_BASE

            logger.debug(f"ä½¿ç”¨é»˜è®¤ API Base ({config.api_type}): {api_base}")

        # æ™ºèƒ½æ„å»ºå®Œæ•´URLï¼Œè‡ªåŠ¨æ·»åŠ æ­£ç¡®çš„è·¯å¾„å‰ç¼€ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        if config.api_type == "google":
            # Google API éœ€è¦ç‰ˆæœ¬å‰ç¼€
            if not config.api_base or api_base == self.GOOGLE_API_BASE:
                # ä½¿ç”¨é»˜è®¤å®˜æ–¹åœ°å€ï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´è·¯å¾„
                url = f"{api_base}/models/{config.model}:generateContent"
            elif not any(api_base.endswith(suffix) for suffix in ["/v1beta", "/v1"]):
                # è‡ªå®šä¹‰åœ°å€ä½†æ²¡æœ‰ç‰ˆæœ¬å‰ç¼€ï¼Œè‡ªåŠ¨æ·»åŠ 
                url = f"{api_base}/v1beta/models/{config.model}:generateContent"
                logger.debug("ä¸ºGoogle APIè‡ªåŠ¨æ·»åŠ v1betaå‰ç¼€")
            else:
                # å·²ç»åŒ…å«ç‰ˆæœ¬å‰ç¼€ï¼Œç›´æ¥ä½¿ç”¨
                url = f"{api_base}/models/{config.model}:generateContent"
                logger.debug("ä½¿ç”¨å·²åŒ…å«ç‰ˆæœ¬å‰ç¼€çš„Google APIåœ°å€")

            payload = await self._prepare_google_payload(config)
            headers = {
                "x-goog-api-key": config.api_key,
                "Content-Type": "application/json",
            }
        else:
            # OpenAI å…¼å®¹æ ¼å¼
            if not config.api_base or api_base == self.OPENAI_API_BASE:
                # ä½¿ç”¨é»˜è®¤åœ°å€ï¼Œéœ€è¦å®Œæ•´è·¯å¾„
                url = f"{api_base}/chat/completions"
            elif not any(api_base.endswith(suffix) for suffix in ["/v1", "/v1beta"]):
                # è‡ªå®šä¹‰åœ°å€ä½†æ²¡æœ‰ç‰ˆæœ¬å‰ç¼€ï¼Œè‡ªåŠ¨æ·»åŠ 
                url = f"{api_base}/v1/chat/completions"
                logger.debug("ä¸ºOpenAIå…¼å®¹APIè‡ªåŠ¨æ·»åŠ v1å‰ç¼€")
            else:
                # å·²ç»åŒ…å«ç‰ˆæœ¬å‰ç¼€ï¼Œç›´æ¥ä½¿ç”¨
                url = f"{api_base}/chat/completions"
                logger.debug("ä½¿ç”¨å·²åŒ…å«ç‰ˆæœ¬å‰ç¼€çš„OpenAIå…¼å®¹APIåœ°å€")

            payload = await self._prepare_openai_payload(config)
            headers = {
                "Authorization": f"Bearer {config.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/astrbot",
                "X-Title": "AstrBot Gemini Image Advanced",
            }

        logger.debug(f"æ™ºèƒ½æ„å»ºAPI URL: {url}")
        return url, headers, payload

    async def generate_image(
        self,
        config: ApiRequestConfig,
        max_retries: int = 3,
        total_timeout: int = 120,
        per_retry_timeout: int = None,
        max_total_time: int = None,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """
        ç”Ÿæˆå›¾åƒ

        Args:
            config: è¯·æ±‚é…ç½®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            total_timeout: æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            (image_urls, image_paths, text_content, thought_signature)ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨å’ŒNone
        """
        if not self.api_keys:
            raise ValueError("æœªé…ç½® API å¯†é’¥")

        if not config.api_key:
            config.api_key = await self.get_next_api_key()

        # è·å–è¯·æ±‚ä¿¡æ¯
        url, headers, payload = await self._get_api_url(config)

        logger.debug(f"ä½¿ç”¨ {config.model} (é€šè¿‡ {config.api_type}) ç”Ÿæˆå›¾åƒ")
        logger.debug(f"API ç«¯ç‚¹: {url[:80]}...")
        logger.debug(
            "[FLOW_DEBUG] è¯·æ±‚å‚æ•°æ¦‚è§ˆ: refs=%s prompt_len=%s aspect=%s res=%s",
            len(config.reference_images or []),
            len(config.prompt or ""),
            config.aspect_ratio,
            config.resolution,
        )

        if config.resolution or config.aspect_ratio:
            logger.debug(
                f"åˆ†è¾¨ç‡: {config.resolution or 'é»˜è®¤'}, é•¿å®½æ¯”: {config.aspect_ratio or 'é»˜è®¤'}"
            )

        if config.api_base:
            logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰ API Base: {config.api_base}")

        # åŒæ­¥è¯¦ç»†æ—¥å¿—å¼€å…³ï¼Œä¾¿äºåœ¨å†…éƒ¨ç½‘ç»œè¯·æ±‚ä¸­æ§åˆ¶è¾“å‡ºç²’åº¦
        self.verbose_logging = bool(getattr(config, "verbose_logging", False))

        return await self._make_request(
            url=url,
            payload=payload,
            headers=headers,
            api_type=config.api_type,
            model=config.model,
            max_retries=max_retries,
            total_timeout=total_timeout,
        )

    async def _make_request(
        self,
        url: str,
        payload: dict[str, Any],
        headers: dict[str, str],
        api_type: str,
        model: str,
        max_retries: int,
        total_timeout: int = 120,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """æ‰§è¡Œ API è¯·æ±‚å¹¶å¤„ç†å“åº”ï¼Œæ¯ä¸ªé‡è¯•æœ‰ç‹¬ç«‹çš„è¶…æ—¶æ§åˆ¶"""

        current_retry = 0
        last_error = None

        while current_retry < max_retries:
            try:
                # æ¯ä¸ªé‡è¯•ä½¿ç”¨ç‹¬ç«‹çš„è¶…æ—¶æ§åˆ¶ï¼Œä¸å…±äº«æ€»è¶…æ—¶æ—¶é—´
                timeout = aiohttp.ClientTimeout(
                    total=total_timeout, sock_read=total_timeout
                )
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    logger.debug(f"å‘é€è¯·æ±‚ï¼ˆé‡è¯• {current_retry}/{max_retries - 1}ï¼‰")
                    return await self._perform_request(
                        session, url, payload, headers, api_type, model
                    )

            except asyncio.CancelledError:
                # åªæœ‰æ¡†æ¶å–æ¶ˆæ‰ä¸é‡è¯•ï¼ˆè¿™æ˜¯æœ€é¡¶å±‚çš„è¶…æ—¶ï¼‰
                logger.debug("è¯·æ±‚è¢«æ¡†æ¶å–æ¶ˆï¼ˆå·¥å…·è°ƒç”¨æ€»è¶…æ—¶ï¼‰ï¼Œä¸å†é‡è¯•")
                timeout_msg = "å›¾åƒç”Ÿæˆæ—¶é—´è¿‡é•¿ï¼Œè¶…å‡ºäº†æ¡†æ¶é™åˆ¶ã€‚è¯·å°è¯•ç®€åŒ–å›¾åƒæè¿°æˆ–åœ¨æ¡†æ¶é…ç½®ä¸­å¢åŠ  tool_call_timeout åˆ° 90-120 ç§’ã€‚"
                raise APIError(timeout_msg, None, "cancelled") from None
            except Exception as e:
                error_msg = str(e)
                error_type = self._classify_error(e, error_msg)

                # åˆ¤æ–­æ˜¯å¦å¯é‡è¯•çš„é”™è¯¯
                if self._is_retryable_error(error_type, e):
                    last_error = APIError(error_msg, None, error_type)
                    logger.warning(
                        f"å¯é‡è¯•é”™è¯¯ (é‡è¯• {current_retry + 1}/{max_retries}): {error_msg}"
                    )

                    current_retry += 1
                    if current_retry < max_retries:
                        # æŒ‡æ•°é€€é¿å»¶è¿Ÿï¼š2ç§’ã€4ç§’ã€8ç§’â€¦â€¦æœ€å¤§10ç§’
                        delay = min(2 ** (current_retry + 1), 10)
                        logger.debug(f"ç­‰å¾… {delay} ç§’åé‡è¯•...")
                        await asyncio.sleep(delay)
                        continue  # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
                    else:
                        logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œç”Ÿæˆå¤±è´¥")
                else:
                    # ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç«‹å³æŠ›å‡º
                    logger.error(f"ä¸å¯é‡è¯•é”™è¯¯: {error_msg}")
                    raise APIError(error_msg, None, error_type) from None

        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›æœ€åä¸€æ¬¡é”™è¯¯
        if last_error:
            raise last_error

        return [], [], None, None

    def _classify_error(self, exception: Exception, error_msg: str) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        if isinstance(exception, asyncio.TimeoutError):
            return "timeout"
        elif "timeout" in error_msg.lower():
            return "timeout"
        elif "connection" in error_msg.lower():
            return "network"
        elif isinstance(exception, aiohttp.ClientError):
            return "network"
        else:
            return "unknown"

    def _is_retryable_error(self, error_type: str, exception: Exception) -> bool:
        """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•"""
        # ç‰¹æ®Šå¤„ç†ï¼šæœªç”Ÿæˆå›¾åƒçš„é‡è¯•
        if error_type == "no_image_retry":
            return True

        # å¯é‡è¯•çš„é”™è¯¯ï¼šè¶…æ—¶ã€ç½‘ç»œé”™è¯¯ã€æœåŠ¡å™¨é”™è¯¯
        if error_type in ["timeout", "network"]:
            return True

        # HTTP çŠ¶æ€ç åˆ¤æ–­
        if hasattr(exception, "status"):
            status = exception.status
            # å¯é‡è¯•ï¼š408, 500, 502, 503, 504
            # ä¸å¯é‡è¯•ï¼š401, 402, 403, 422, 429ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰
            if status in [408, 500, 502, 503, 504]:
                return True
            elif status in [401, 402, 403, 422, 429]:
                return False

        return True  # é»˜è®¤é‡è¯•æœªçŸ¥é”™è¯¯

    async def _perform_request(
        self,
        session: aiohttp.ClientSession,
        url: str,
        payload: dict[str, Any],
        headers: dict[str, str],
        api_type: str,
        model: str,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """æ‰§è¡Œå®é™…çš„HTTPè¯·æ±‚"""
        logger.debug(
            "[FLOW_DEBUG] å‘é€è¯·æ±‚: url=%s api_type=%s model=%s payload_keys=%s",
            url[:100],
            api_type,
            model,
            list(payload.keys()),
        )

        async with session.post(
            url, json=payload, headers=headers, proxy=self.proxy
        ) as response:
            logger.debug(f"å“åº”çŠ¶æ€: {response.status}")
            response_text = await response.text()
            content_type = response.headers.get("Content-Type", "") or ""

            # è§£æ JSON å“åº”ï¼Œæ·»åŠ é”™è¯¯å¤„ç†
            try:
                response_data = json.loads(response_text) if response_text else {}
            except json.JSONDecodeError as e:
                # SSE å“åº”ï¼ˆtext/event-streamï¼‰éœ€è¦é¢å¤–è§£æ
                if (
                    "text/event-stream" in content_type.lower()
                    or response_text.strip().startswith("data:")
                ):
                    try:
                        response_data = self._parse_sse_payload(response_text)
                        logger.debug("æ£€æµ‹åˆ° SSE å“åº”ï¼Œå·²å®Œæˆ JSON è½¬æ¢")
                    except Exception as sse_error:
                        logger.error(f"SSE è§£æå¤±è´¥: {sse_error}")
                        logger.error(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response_text[:500]}")
                        raise APIError(
                            f"API è¿”å›äº†æ— æ•ˆçš„ JSON/SSE å“åº”: {sse_error}",
                            response.status,
                        ) from None
                else:
                    logger.error(f"JSON è§£æå¤±è´¥: {e}")
                    logger.error(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response_text[:500]}")
                    raise APIError(
                        f"API è¿”å›äº†æ— æ•ˆçš„ JSON å“åº”: {e}", response.status
                    ) from None

            if response.status == 200:
                logger.debug("API è°ƒç”¨æˆåŠŸ")
                if api_type == "google":
                    return await self._parse_gresponse(response_data, session)
                else:  # openai å…¼å®¹æ ¼å¼
                    return await self._parse_openai_response(response_data, session)
            elif response.status in [429, 402, 403]:
                error_msg = response_data.get("error", {}).get(
                    "message", f"HTTP {response.status}"
                )
                logger.warning(f"API é…é¢/æƒé™é—®é¢˜: {error_msg}")
                raise APIError(error_msg, response.status, "quota")
            else:
                error_msg = response_data.get("error", {}).get(
                    "message", f"HTTP {response.status}"
                )
                logger.warning(f"API é”™è¯¯: {error_msg}")
                raise APIError(error_msg, response.status)

    def _parse_sse_payload(self, raw_text: str) -> dict[str, Any]:
        """è§£æ text/event-stream å“åº”ï¼Œæå–æœ€åä¸€ä¸ªåŒ…å«æœ‰æ•ˆ payload çš„ data åŒ…"""

        events: list[dict[str, Any]] = []
        data_lines: list[str] = []

        def flush_event():
            """å°†ç´¯è®¡çš„ data è¡Œæ‹¼æ¥å¹¶è§£æä¸ºä¸€ä¸ªäº‹ä»¶"""
            if not data_lines:
                return
            data_text = "\n".join(data_lines).strip()
            data_lines.clear()
            if not data_text or data_text == "[DONE]":
                return
            try:
                parsed = json.loads(data_text)
                if isinstance(parsed, dict):
                    events.append(parsed)
            except json.JSONDecodeError as e:
                logger.warning(
                    "SSE äº‹ä»¶è§£æå¤±è´¥: %s | ç‰‡æ®µ: %s",
                    e,
                    data_text[:160],
                )

        for raw_line in raw_text.splitlines():
            stripped = raw_line.strip()
            if not stripped:
                flush_event()
                continue
            if stripped.startswith(":"):
                # SSE æ³¨é‡Šè¡Œï¼Œç›´æ¥è·³è¿‡
                continue
            if stripped.startswith("data:"):
                data_lines.append(stripped.removeprefix("data:").lstrip())
                continue

            # å°‘æ•°å®ç°ä¼šçœç•¥å‰ç¼€ï¼Œè¿™é‡Œå°è¯•å…¼å®¹
            if stripped and stripped != "[DONE]":
                data_lines.append(stripped)

        flush_event()

        if not events:
            raise ValueError(
                f"SSE å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ data äº‹ä»¶ (æ”¶åˆ° {len(raw_text)} å­—ç¬¦, ç‰‡æ®µ: {raw_text[:160]!r})"
            )

        # ä¼˜å…ˆè¿”å›å« candidates/choices/data å­—æ®µçš„äº‹ä»¶ï¼Œé¿å… STOP åŒ…è¦†ç›–æœ‰æ•ˆè´Ÿè½½
        for event in reversed(events):
            if not isinstance(event, dict):
                continue
            if event.get("candidates") or event.get("choices") or event.get("data"):
                logger.debug(
                    "SSE å“åº”å…±è§£æ %s ä¸ªäº‹ä»¶ï¼Œè¿”å›å«æœ‰æ•ˆè´Ÿè½½çš„æœ«å°¾äº‹ä»¶",
                    len(events),
                )
                return event

        logger.debug("SSE å“åº”åªåŒ…å«é€šç”¨äº‹ä»¶ï¼Œè¿”å›æœ€åä¸€ä¸ª data åŒ…")
        return events[-1]

    async def _parse_gresponse(
        self, response_data: dict, session: aiohttp.ClientSession
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """è§£æ Google å®˜æ–¹ API å“åº”"""
        import asyncio

        parse_start = asyncio.get_event_loop().time()
        logger.debug("ğŸ” å¼€å§‹è§£æAPIå“åº”æ•°æ®...")

        image_urls: list[str] = []
        image_paths: list[str] = []
        text_chunks: list[str] = []
        thought_signature = None
        fallback_texts = self._collect_fallback_texts(response_data)

        if "candidates" not in response_data or not response_data["candidates"]:
            logger.warning("Google å“åº”ç¼ºå°‘ candidates å­—æ®µï¼Œå°è¯•ä» fallback æ–‡æœ¬æå–å›¾åƒ")
            appended = False
            if fallback_texts:
                appended = await self._append_images_from_texts(
                    fallback_texts, image_urls, image_paths
                )
            if appended and (image_urls or image_paths):
                text_content = (
                    " ".join(t.strip() for t in fallback_texts if t and t.strip())
                    or None
                )
                return image_urls, image_paths, text_content, thought_signature

            if "promptFeedback" in response_data:
                feedback = response_data["promptFeedback"]
                logger.warning(f"è¯·æ±‚è¢«é˜»æ­¢: {feedback}")
            else:
                logger.error(f"å“åº”ä¸­æ²¡æœ‰ candidates: {response_data}")
            return [], [], None, None

        candidates = response_data["candidates"]
        logger.debug(f"ğŸ“ æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç»“æœ")

        for idx, candidate in enumerate(candidates):
            finish_reason = candidate.get("finishReason")
            if finish_reason in ["SAFETY", "RECITATION"]:
                logger.warning(f"å€™é€‰ {idx} ç”Ÿæˆè¢«é˜»æ­¢: {finish_reason}")
                continue

            content = candidate.get("content", {})
            parts = content.get("parts") or []
            logger.debug(f"ğŸ“‹ å€™é€‰ {idx} åŒ…å« {len(parts)} ä¸ªéƒ¨åˆ†")

            for i, part in enumerate(parts):
                try:
                    logger.debug(f"æ£€æŸ¥å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpart: {list(part.keys())}")

                    if "thoughtSignature" in part and not thought_signature:
                        thought_signature = part["thoughtSignature"]
                        logger.debug(f"ğŸ§  æ‰¾åˆ°æ€ç»´ç­¾å: {thought_signature[:50]}...")

                    # ç´¯ç§¯æ–‡æœ¬ï¼Œä¾¿äºåç»­ä»æ–‡æœ¬ä¸­æå– data URI / http(s) é“¾æ¥
                    if "text" in part and isinstance(part.get("text"), str):
                        text_chunks.append(part.get("text", ""))

                    inline_data = part.get("inlineData") or part.get("inline_data")
                    if inline_data and not part.get("thought", False):
                        mime_type = (
                            inline_data.get("mimeType")
                            or inline_data.get("mime_type")
                            or "image/png"
                        )
                        base64_data = inline_data.get("data", "")

                        logger.debug(
                            f"ğŸ¯ æ‰¾åˆ°å›¾åƒæ•°æ® (å€™é€‰{idx} ç¬¬{i + 1}éƒ¨åˆ†): {mime_type}, å¤§å°: {len(base64_data)} å­—ç¬¦"
                        )

                        if base64_data:
                            image_format = (
                                mime_type.split("/")[1] if "/" in mime_type else "png"
                            )

                            logger.debug("ğŸ’¾ å¼€å§‹ä¿å­˜å›¾åƒæ–‡ä»¶...")
                            save_start = asyncio.get_event_loop().time()

                            saved_path = await save_base64_image(
                                base64_data, image_format
                            )

                            save_end = asyncio.get_event_loop().time()
                            logger.debug(
                                f"âœ… å›¾åƒä¿å­˜å®Œæˆï¼Œè€—æ—¶: {save_end - save_start:.2f}ç§’"
                            )

                            if saved_path:
                                image_paths.append(saved_path)
                                image_urls.append(saved_path)
                            else:
                                # ä¿å­˜å¤±è´¥æ—¶å°è¯•å®½æ¾è§£ç å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…è¯¯åˆ¤ä¸ºæ— å›¾
                                try:
                                    import tempfile

                                    tmp_path = Path(
                                        tempfile.mktemp(
                                            prefix="gem_inline_", suffix=".png"
                                        )
                                    )
                                    cleaned = base64_data.strip().replace("\n", "")
                                    if ";base64," in cleaned:
                                        _, _, cleaned = cleaned.partition(";base64,")
                                    raw = base64.b64decode(cleaned, validate=False)
                                    tmp_path.write_bytes(raw)
                                    image_paths.append(str(tmp_path))
                                    image_urls.append(str(tmp_path))
                                    logger.debug(
                                        "âš ï¸ save_base64_image å¤±è´¥ï¼Œå·²ä½¿ç”¨å®½æ¾è§£ç å†™å…¥ä¸´æ—¶æ–‡ä»¶: %s",
                                        tmp_path,
                                    )
                                except Exception as e:
                                    logger.warning(
                                        "å€™é€‰ %s ç¬¬ %s éƒ¨åˆ† inlineData è§£ç å¤±è´¥ï¼Œè·³è¿‡ï¼š%s",
                                        idx,
                                        i + 1,
                                        e,
                                    )
                        else:
                            logger.warning(
                                f"å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpartæœ‰inlineDataä½†dataä¸ºç©º"
                            )
                    elif "thought" in part and part.get("thought", False):
                        logger.debug(f"å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpartæ˜¯æ€è€ƒå†…å®¹")
                    else:
                        logger.debug(
                            f"å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpartä¸æ˜¯å›¾åƒä¹Ÿä¸æ˜¯æ€è€ƒ: {list(part.keys())}"
                        )
                except Exception as e:
                    logger.error(
                        f"å¤„ç†å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpartæ—¶å‡ºé”™: {e}", exc_info=True
                    )

        logger.debug(f"ğŸ–¼ï¸ å…±æ‰¾åˆ° {len(image_paths)} å¼ å›¾ç‰‡")

        # æ–‡æœ¬ä¸­å°è¯•è§£æå¯èƒ½çš„å›¾åƒURLæˆ–Base64ï¼ˆç”¨äºåªè¿”å›æ–‡æœ¬çš„æƒ…å†µï¼‰
        if text_chunks:
            extracted_urls: list[str] = []
            extracted_paths: list[str] = []
            for chunk in text_chunks:
                # http(s) å›¾ç‰‡é“¾æ¥
                extracted_urls.extend(self._find_image_urls_in_text(chunk))
                # data URI / base64
                urls2, paths2 = await self._extract_from_content(chunk)
                extracted_urls.extend(urls2)
                extracted_paths.extend(paths2)

            if extracted_urls or extracted_paths:
                image_urls.extend(extracted_urls)
                image_paths.extend(extracted_paths)

        text_content = (
            " ".join(chunk for chunk in text_chunks if chunk).strip()
            if text_chunks
            else None
        )
        if text_content:
            logger.debug(f"ğŸ¯ æ‰¾åˆ°æ–‡æœ¬å†…å®¹: {text_content[:100]}...")

        if not (image_paths or image_urls) and fallback_texts:
            appended = await self._append_images_from_texts(
                fallback_texts, image_urls, image_paths
            )
            if appended and not text_content:
                text_content = (
                    " ".join(t.strip() for t in fallback_texts if t and t.strip())
                    or text_content
                )

        if image_paths or image_urls:
            parse_end = asyncio.get_event_loop().time()
            logger.debug(f"ğŸ‰ APIå“åº”è§£æå®Œæˆï¼Œæ€»è€—æ—¶: {parse_end - parse_start:.2f}ç§’")
            return image_urls, image_paths, text_content, thought_signature

        if text_content:
            logger.warning("APIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæœªç”Ÿæˆå›¾åƒï¼Œå°†è§¦å‘é‡è¯•")
            raise APIError(
                "å›¾åƒç”Ÿæˆå¤±è´¥ï¼šAPIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæ­£åœ¨é‡è¯•...",
                500,
                "no_image_retry",
            )

        logger.error("æœªåœ¨å“åº”ä¸­æ‰¾åˆ°å›¾åƒæ•°æ®")
        raise APIError(
            "å›¾åƒç”Ÿæˆå¤±è´¥ï¼šå“åº”æ ¼å¼å¼‚å¸¸ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæ•°æ®", None, "invalid_response"
        )

    async def _parse_openai_response(
        self, response_data: dict, session: aiohttp.ClientSession
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """è§£æ OpenAI API å“åº”"""

        image_urls: list[str] = []
        image_paths: list[str] = []
        text_content = None
        thought_signature = None
        fail_reasons: list[str] = []
        fallback_texts = self._collect_fallback_texts(response_data)

        message: dict[str, Any] | None = None
        if "choices" in response_data and response_data["choices"]:
            choice = response_data["choices"][0]
            message = choice.get("message", {})
        else:
            message = self._coerce_basic_openai_message(response_data)

        if message:
            if "choices" not in response_data:
                logger.debug(
                    "[FLOW_DEBUG][openai] ä½¿ç”¨éæ ‡å‡†å­—æ®µæ„é€  messageï¼Œkeys=%s",
                    list(response_data.keys())[:5],
                )
            content = message.get("content", "")

            text_chunks: list[str] = []
            image_candidates: list[str] = []
            extracted_urls: list[str] = []

            logger.debug(
                "[FLOW_DEBUG][openai] è§£æå“åº” choicesï¼Œcontent_type=%s images_field=%s",
                type(content),
                bool(message.get("images")),
            )

            if isinstance(content, list):
                for part in content:
                    if not isinstance(part, dict):
                        continue

                    part_type = part.get("type")
                    if part_type == "text" and "text" in part:
                        text_val = str(part.get("text", ""))
                        text_chunks.append(text_val)
                        extracted_urls.extend(self._find_image_urls_in_text(text_val))
                    elif part_type == "image_url":
                        image_obj = part.get("image_url") or {}
                        if isinstance(image_obj, dict):
                            url_val = image_obj.get("url")
                            if url_val:
                                image_candidates.append(url_val)
            elif isinstance(content, str):
                text_chunks.append(content)
                extracted_urls.extend(self._find_image_urls_in_text(content))

            # æ ‡å‡† images å­—æ®µï¼ˆå…¼å®¹ Gemini/OpenAI æ··åˆæ ¼å¼ï¼‰
            if message.get("images"):
                for image_item in message["images"]:
                    if not isinstance(image_item, dict):
                        continue

                    # å…¸å‹æ ¼å¼ï¼š{"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
                    image_obj = image_item.get("image_url")
                    if isinstance(image_obj, dict):
                        url_val = image_obj.get("url")
                        if isinstance(url_val, str) and url_val:
                            image_candidates.append(url_val)
                    elif isinstance(image_obj, str) and image_obj:
                        image_candidates.append(image_obj)
                    # é€€åŒ–æ ¼å¼ï¼š{"url": "..."}
                    elif isinstance(image_item.get("url"), str):
                        image_candidates.append(image_item["url"])

            # åˆå¹¶åœ¨æ–‡æœ¬é‡Œè§£æåˆ°çš„å›¾åƒ URL
            if extracted_urls:
                image_candidates.extend(extracted_urls)

            # ç»„è£…æ–‡æœ¬å†…å®¹
            if text_chunks:
                text_content = " ".join([t for t in text_chunks if t]).strip() or None

            # æŒ‰é¡ºåºå¤„ç†å›¾åƒå€™é€‰
            for candidate_url in image_candidates:
                logger.debug(
                    "[REF_DEBUG][openai] å¤„ç†å€™é€‰URL: %s", str(candidate_url)[:120]
                )
                if isinstance(candidate_url, str) and candidate_url.startswith(
                    "data:image/"
                ):
                    image_url, image_path = await self._parse_data_uri(candidate_url)
                elif isinstance(candidate_url, str):
                    # å¯¹äºå¯è®¿é—®çš„ http(s) é“¾æ¥ï¼Œç›´æ¥è¿”å› URLï¼Œé¿å…é‡å¤ä¸‹è½½å ç”¨å¸¦å®½
                    if candidate_url.startswith("http://") or candidate_url.startswith(
                        "https://"
                    ):
                        image_urls.append(candidate_url)
                        logger.debug(
                            f"ğŸ–¼ï¸ OpenAI è¿”å›å¯ç›´æ¥è®¿é—®çš„å›¾åƒé“¾æ¥: {candidate_url}"
                        )
                        continue
                    image_url, image_path = await self._download_image(
                        candidate_url, session, use_cache=False
                    )
                else:
                    logger.warning(f"è·³è¿‡éå­—ç¬¦ä¸²ç±»å‹çš„å›¾åƒURL: {type(candidate_url)}")
                    continue

                if image_url or image_path:
                    if image_url:
                        image_urls.append(image_url)
                    if image_path:
                        image_paths.append(image_path)

            # content ä¸­æŸ¥æ‰¾å†…è” data URIï¼ˆæ–‡æœ¬é‡Œï¼‰
            extracted_urls: list[str] = []
            extracted_paths: list[str] = []

            if isinstance(content, str):
                extracted_urls, extracted_paths = await self._extract_from_content(
                    content
                )
            elif text_content:
                extracted_urls, extracted_paths = await self._extract_from_content(
                    text_content
                )

            if extracted_urls or extracted_paths:
                image_urls.extend(extracted_urls)
                image_paths.extend(extracted_paths)

            # é¢å¤–åœ¨æ±‡æ€»æ–‡æœ¬ä¸­æœç´¢ http(s) å›¾ç‰‡é“¾æ¥ï¼Œå…¼å®¹åªè¿”å›æ–‡æœ¬çš„æƒ…å†µ
            if text_content:
                http_urls = self._find_image_urls_in_text(text_content)
                for url in http_urls:
                    if url not in image_urls:
                        image_urls.append(url)

                # æ¾æ•£æå– data:image ç‰‡æ®µï¼Œé¿å…å›  Markdown/æ¢è¡Œå¯¼è‡´é—æ¼
                loose_matches = re.finditer(
                    r"data:image/([a-zA-Z0-9.+-]+);base64,([-A-Za-z0-9+/=_\\s]+)",
                    text_content,
                    flags=re.IGNORECASE,
                )
                for m in loose_matches:
                    fmt = m.group(1)
                    b64_raw = m.group(2)
                    b64_clean = re.sub(r"\\s+", "", b64_raw)
                    image_path = await save_base64_image(b64_clean, fmt.lower())
                    if image_path:
                        image_urls.append(image_path)
                        image_paths.append(image_path)
                        logger.debug(
                            "[FLOW_DEBUG][openai] æ¾æ•£æå– data URI æˆåŠŸ: fmt=%s len=%s",
                            fmt,
                            len(b64_clean),
                        )

        else:
            logger.debug(
                "[FLOW_DEBUG][openai] å“åº”ç¼ºå°‘å¯ç”¨çš„ message å­—æ®µï¼Œå°è¯• data/b64 è§£æ"
            )

        if not (image_urls or image_paths) and fallback_texts:
            fallback_added = await self._append_images_from_texts(
                fallback_texts, image_urls, image_paths
            )
            if fallback_added and not text_content:
                text_content = (
                    " ".join(t.strip() for t in fallback_texts if t and t.strip())
                    or text_content
                )

        # OpenAI æ ¼å¼
        if not image_urls and not image_paths and response_data.get("data"):
            for image_item in response_data["data"]:
                if "url" in image_item:
                    image_url, image_path = await self._download_image(
                        image_item["url"], session, use_cache=False
                    )
                    if image_url:
                        image_urls.append(image_url)
                    if image_path:
                        image_paths.append(image_path)
                elif "b64_json" in image_item:
                    image_path = await save_base64_image(image_item["b64_json"], "png")
                    if image_path:
                        # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                        image_urls.append(image_path)
                        image_paths.append(image_path)

        if image_urls or image_paths:
            logger.debug(
                f"ğŸ–¼ï¸ OpenAI æ”¶é›†åˆ° {len(image_paths) or len(image_urls)} å¼ å›¾ç‰‡"
            )
            return image_urls, image_paths, text_content, thought_signature

        # å¦‚æœåªæœ‰æ–‡æœ¬å†…å®¹ï¼Œä¹Ÿè¿”å›
        if text_content:
            # å¦‚æœé…ç½®äº†éœ€è¦æ–‡æœ¬å“åº”ï¼Œä¸”ç¡®å®æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¿™é‡Œåº”è¯¥æŠ¥é”™è§¦å‘é‡è¯•è€Œä¸æ˜¯ç›´æ¥è¿”å›æ–‡æœ¬
            # é™¤éè¿™æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬è¯·æ±‚ï¼ˆä½†åœ¨ç”Ÿå›¾æ’ä»¶é‡Œé€šå¸¸ä¸æ˜¯ï¼‰
            detail = (
                f" | å‚è€ƒå›¾å¤„ç†æç¤º: {'; '.join(fail_reasons[:3])}"
                if fail_reasons
                else ""
            )
            logger.debug(
                "[FLOW_DEBUG][openai] ä»…è¿”å›æ–‡æœ¬ï¼Œé•¿åº¦=%s é¢„è§ˆ=%s",
                len(text_content),
                text_content[:200],
            )
            logger.warning(f"OpenAIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæœªç”Ÿæˆå›¾åƒï¼Œå°†è§¦å‘é‡è¯•{detail}")
            raise APIError(
                "å›¾åƒç”Ÿæˆå¤±è´¥ï¼šAPIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæ­£åœ¨é‡è¯•...", 500, "no_image_retry"
            )

        logger.warning("OpenAI å“åº”æ ¼å¼ä¸æ”¯æŒæˆ–æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
        return image_urls, image_paths, text_content, thought_signature

    def _normalize_message_value(self, raw_value: Any) -> dict[str, Any] | None:
        """å½’ä¸€åŒ–ä»»æ„å¸¸è§å­—æ®µä¸ºæ ‡å‡† message ç»“æ„"""
        if raw_value is None:
            return None

        if isinstance(raw_value, dict):
            if raw_value.get("role") and "content" in raw_value:
                return raw_value

            if "message" in raw_value:
                nested = self._normalize_message_value(raw_value.get("message"))
                if nested:
                    return nested

            for key in ("content", "text", "output", "result", "response"):
                if key in raw_value:
                    nested = self._normalize_message_value(raw_value.get(key))
                    if nested:
                        return nested

            return None

        if isinstance(raw_value, list):
            if raw_value:
                return {"role": "assistant", "content": raw_value}
            return None

        if isinstance(raw_value, str):
            cleaned = raw_value.strip()
            if cleaned:
                return {"role": "assistant", "content": cleaned}
            return None

        return None

    def _coerce_basic_openai_message(
        self, response_data: dict[str, Any]
    ) -> dict[str, Any] | None:
        """ä»å¸¸è§å…¼å®¹æ ¼å¼æå– messageï¼Œå…¼å®¹ body/content/text ç­‰å­—æ®µ"""

        primary_keys = [
            "message",
            "content",
            "text",
            "output",
            "result",
            "response",
        ]
        nested_keys = [
            "body",
            "modelOutput",
            "model_output",
            "response_body",
        ]

        for key in primary_keys:
            normalized = self._normalize_message_value(response_data.get(key))
            if normalized:
                return normalized

        for key in nested_keys:
            value = response_data.get(key)
            if isinstance(value, (dict, list, str)):
                normalized = self._normalize_message_value(value)
                if normalized:
                    return normalized

        return None

    def _collect_fallback_texts(self, payload: dict[str, Any]) -> list[str]:
        """æ”¶é›†å¸¸è§å­—æ®µä¸­çš„æ–‡æœ¬å“åº”ï¼Œç”¨äºå…œåº•æå– Markdown é“¾æ¥"""
        if not isinstance(payload, dict):
            return []

        candidate_keys = (
            "content",
            "text",
            "output",
            "result",
            "response",
            "message",
        )
        container_keys = (
            "body",
            "response_body",
            "modelOutput",
            "model_output",
            "modelOutputs",
            "model_outputs",
        )

        texts: list[str] = []

        def push(value: Any):
            if value is None:
                return
            if isinstance(value, str):
                cleaned = value.strip()
                if cleaned:
                    texts.append(cleaned)
                return
            if isinstance(value, list):
                for item in value:
                    push(item)
                return
            if isinstance(value, dict):
                for key in candidate_keys:
                    if key in value:
                        push(value.get(key))

        for key in candidate_keys:
            push(payload.get(key))

        for key in container_keys:
            push(payload.get(key))

        # å»é‡ä½†ä¿æŒé¡ºåº
        seen: set[str] = set()
        ordered: list[str] = []
        for text in texts:
            if text not in seen:
                seen.add(text)
                ordered.append(text)
        return ordered

    async def _append_images_from_texts(
        self,
        texts: list[str],
        image_urls: list[str],
        image_paths: list[str],
    ) -> bool:
        """ä»é¢å¤–çš„æ–‡æœ¬å­—æ®µä¸­æå– http(s)/data URI å›¾åƒ"""

        appended = False
        for text in texts:
            if not text:
                continue

            http_urls = self._find_image_urls_in_text(text)
            for url in http_urls:
                if url not in image_urls:
                    image_urls.append(url)
                    appended = True

            extra_urls, extra_paths = await self._extract_from_content(text)
            for url in extra_urls:
                if url not in image_urls:
                    image_urls.append(url)
                    appended = True
            for path in extra_paths:
                if path not in image_paths:
                    image_paths.append(path)
                    appended = True

        return appended

    async def _parse_data_uri(self, data_uri: str) -> tuple[str | None, str | None]:
        """è§£æ data URI æ ¼å¼çš„å›¾åƒ"""
        try:
            if ";base64," not in data_uri:
                logger.error("æ— æ•ˆçš„ data URI æ ¼å¼")
                return None, None

            header, base64_data = data_uri.split(";base64,", 1)
            mime_type = header.replace("data:", "")
            format_type = mime_type.split("/")[1] if "/" in mime_type else "png"

            image_path = await save_base64_image(base64_data, format_type)
            if image_path:
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                image_url = image_path
                return image_url, image_path
        except Exception as e:
            logger.error(f"è§£æ data URI å¤±è´¥: {e}")

        return None, None

    async def _extract_from_content(self, content: str) -> tuple[list[str], list[str]]:
        """ä»æ–‡æœ¬å†…å®¹ä¸­æå–æ‰€æœ‰ data URI å›¾åƒï¼Œä¿æŒé¡ºåº"""
        # OpenAI å…¼å®¹æ¥å£æœ‰æ—¶ä¼šæŠŠå›¾ç‰‡ä»¥ Markdown data URI å½¢å¼å¡è¿›çº¯æ–‡æœ¬
        # ä¸ºäº†æ›´é²æ£’ï¼Œå…è®¸å¤§å°å†™æ··æ’ã€åŒ…å« -/_ï¼Œå¹¶è·¨å¤šè¡ŒåŒ¹é…
        pattern = re.compile(
            r"data\s*:\s*image/([a-zA-Z0-9.+-]+)\s*;\s*base64\s*,\s*([-A-Za-z0-9+/=_\s]+)",
            flags=re.IGNORECASE,
        )
        matches = pattern.findall(content)

        image_urls: list[str] = []
        image_paths: list[str] = []

        for image_format, base64_string in matches:
            # å…ˆç®€å•æ¸…æ´—éæ³•å­—ç¬¦ï¼Œé¿å…å› æ„å¤–æ’å…¥çš„ç¬¦å·å¯¼è‡´è§£ç å¤±è´¥
            cleaned_b64 = re.sub(r"[^A-Za-z0-9+/=_-]", "", base64_string)
            image_path = await save_base64_image(
                cleaned_b64 or base64_string, image_format.lower()
            )
            if image_path:
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                image_url = image_path
                image_urls.append(image_url)
                image_paths.append(image_path)

        return image_urls, image_paths

    def _find_image_urls_in_text(self, text: str) -> list[str]:
        """ä»æ–‡æœ¬/Markdownä¸­æå–å¯ç”¨çš„ http(s) å›¾ç‰‡é“¾æ¥"""
        if not text:
            return []

        # Markdown å›¾ç‰‡è¯­æ³•ä¸è£¸éœ²çš„å›¾ç‰‡é“¾æ¥
        markdown_pattern = r"!\[[^\]]*\]\((https?://[^)]+)\)"
        # Markdown å›¾ç‰‡è¯­æ³•ä¸­çš„ data URIï¼ˆå¦‚ ![image](data:image/png;base64,...)ï¼‰
        markdown_data_uri_pattern = r"!\[[^\]]*\]\((data:image/[^)]+)\)"
        raw_pattern = (
            r"(https?://[^\s)]+\.(?:png|jpe?g|gif|webp|bmp|tiff|avif))(?:\b|$)"
        )
        spaced_pattern = r"(https?\s*:\s*/\s*/[^\s)]+)"

        urls: list[str] = []
        seen: set[str] = set()

        def _push(candidate: str):
            cleaned = candidate.strip().replace("&amp;", "&").rstrip(").,;")
            if cleaned and cleaned not in seen:
                seen.add(cleaned)
                urls.append(cleaned)

        for pattern in (markdown_pattern, markdown_data_uri_pattern, raw_pattern):
            for match in re.findall(pattern, text, flags=re.IGNORECASE):
                _push(match)

        # é€‚é…å¸¦ç©ºæ ¼çš„ http:// ç‰‡æ®µï¼ˆå¦‚ "http: //1. 2. 3. 4/image.png"ï¼‰
        for match in re.findall(spaced_pattern, text, flags=re.IGNORECASE):
            compact = re.sub(r"\s+", "", match)
            if compact.lower().startswith(("http://", "https://")):
                _push(compact)

        return urls

    async def _download_image(
        self,
        image_url: str,
        session: aiohttp.ClientSession,
        use_cache: bool = False,
    ) -> tuple[str | None, str | None]:
        """ä¸‹è½½å¹¶ä¿å­˜å›¾åƒï¼Œå¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤å…³é—­ä»¥é¿å…è¿”å›æ—§å›¾ï¼‰"""
        cleaned_url = (
            image_url.replace("&amp;", "&") if isinstance(image_url, str) else image_url
        )
        parsed = urllib.parse.urlparse(cleaned_url)
        is_http = parsed.scheme in {"http", "https"}
        cache_key = None

        # é’ˆå¯¹ CQ ç å›¾æœåŠ¡å™¨å¢åŠ ä¸“ç”¨è¯·æ±‚å¤´
        headers: dict[str, str] = {}
        if is_http:
            headers.update(
                {
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36"
                    ),
                    "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
                    "Accept-Language": "zh-CN,zh;q=0.9",
                    "Connection": "keep-alive",
                }
            )
            if "gchat.qpic.cn" in (parsed.netloc or ""):
                headers["Referer"] = "https://qun.qq.com"
            elif parsed.scheme and parsed.netloc:
                headers["Referer"] = f"{parsed.scheme}://{parsed.netloc}"

        # ç¼“å­˜å‘½ä¸­ç›´æ¥è¿”å›ï¼Œå‡å°‘é‡å¤ä¸‹è½½ä¸å†…å­˜å ç”¨
        if cache_key:
            try:
                IMAGE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
                cached = next(IMAGE_CACHE_DIR.glob(f"{cache_key}.*"), None)
                if cached and cached.exists() and cached.stat().st_size > 0:
                    logger.debug(f"å›¾åƒä¸‹è½½å‘½ä¸­ç¼“å­˜: {cleaned_url}")
                    return str(cached), str(cached)
            except Exception as e:
                logger.debug(f"æ£€æŸ¥å›¾åƒç¼“å­˜å¤±è´¥: {e}")

        max_retries = 1
        retry_interval = 1.0

        for attempt in range(1, max_retries + 1):
            try:
                logger.debug(
                    f"æ­£åœ¨ä¸‹è½½å›¾åƒ: {cleaned_url[:100]}... å°è¯• {attempt}/{max_retries}"
                )

                async with session.get(
                    cleaned_url,
                    timeout=aiohttp.ClientTimeout(total=30),
                    proxy=self.proxy,
                    headers=headers or None,
                ) as response:
                    if response.status != 200:
                        try:
                            err_text = await response.text()
                        except Exception:
                            err_text = ""

                        response_reason = response.reason or ""
                        response_content_type = response.headers.get("Content-Type", "")
                        query_params = urllib.parse.parse_qs(parsed.query)
                        param_issues: list[str] = []

                        # ä»…åœ¨å‡ºç° 400 é”™è¯¯æ—¶è¿›è¡Œå‚æ•°åˆæ³•æ€§æ£€æŸ¥
                        if response.status == 400:
                            appid = (query_params.get("appid") or [None])[0]
                            if appid and not re.fullmatch(r"[A-Za-z0-9]+", appid):
                                param_issues.append("appid æ ¼å¼å¼‚å¸¸ï¼ˆä»…å…è®¸å­—æ¯æ•°å­—ï¼‰")

                            fileid = (query_params.get("fileid") or [None])[0]
                            if fileid and not re.fullmatch(r"[A-Za-z0-9._-]+", fileid):
                                param_issues.append(
                                    "fileid æ ¼å¼å¼‚å¸¸ï¼ˆä»…å…è®¸å­—æ¯æ•°å­—ã€.ã€_ã€-ï¼‰"
                                )

                            rkey = (query_params.get("rkey") or [None])[0]
                            if rkey and re.search(r"[^A-Za-z0-9._-]", rkey):
                                param_issues.append("rkey åŒ…å«ç‰¹æ®Šå­—ç¬¦")

                            spec = (query_params.get("spec") or [None])[0]
                            if spec and not str(spec).isdigit():
                                param_issues.append("spec å‚æ•°åº”ä¸ºæ•°å­—")

                        # æ ¹æ®å“åº”å†…å®¹ä¸æ ¡éªŒç»“æœç»™å‡ºå»ºè®®
                        suggestions: list[str] = []
                        if " " in cleaned_url or "%20" in cleaned_url:
                            suggestions.append("URLæ ¼å¼é”™è¯¯ â†’ æ£€æŸ¥URLç¼–ç ")
                        if param_issues:
                            suggestions.append("å‚æ•°é”™è¯¯ â†’ æ£€æŸ¥å‚æ•°æ ¼å¼")
                        err_lower = err_text.lower() if err_text else ""
                        if any(keyword in err_lower for keyword in ["auth", "key"]):
                            suggestions.append("è®¤è¯é”™è¯¯ â†’ æ£€æŸ¥APIå¯†é’¥")
                        if any(
                            keyword in err_lower
                            for keyword in ["limit", "é¢‘ç‡", "é™åˆ¶"]
                        ):
                            suggestions.append("æœåŠ¡å™¨é™åˆ¶ â†’ å»ºè®®ç¨åé‡è¯•")
                        if not suggestions:
                            suggestions.append("æœåŠ¡å™¨é™åˆ¶ â†’ å»ºè®®ç¨åé‡è¯•")

                        logger.error(
                            "ä¸‹è½½å›¾åƒå¤±è´¥: HTTP %s %s url=%s å“åº”æ‘˜è¦=%s å»ºè®®=%s",
                            response.status,
                            response_reason,
                            cleaned_url,
                            err_text[:200],
                            "ï¼›".join(dict.fromkeys(suggestions)),
                        )

                        if self.verbose_logging:
                            logger.debug(
                                "HTTP 400 å‚æ•°æ£€æŸ¥ç»“æœ: %s",
                                "; ".join(param_issues)
                                if param_issues
                                else "æœªå‘ç°æ˜æ˜¾å¼‚å¸¸",
                            )
                            logger.debug("å®Œæ•´è¯·æ±‚å¤´: %s", headers or {})
                            logger.debug(
                                "User-Agent: %s", (headers or {}).get("User-Agent", "")
                            )
                            logger.debug(
                                "Content-Type: %s, Accept: %s",
                                (headers or {}).get("Content-Type", "æœªè®¾ç½®"),
                                (headers or {}).get("Accept", "æœªè®¾ç½®"),
                            )
                            logger.debug(
                                "æœåŠ¡å™¨å“åº”è¯¦æƒ…: status=%s, reason=%s, phrase=%s, content-type=%s",
                                response.status,
                                response_reason,
                                getattr(response, "reason", ""),
                                response_content_type,
                            )
                            logger.debug(
                                "æœåŠ¡å™¨å“åº”ä½“é¢„è§ˆ: %s",
                                err_text[:1000] if err_text else "<empty>",
                            )

                        if response.status == 400 and attempt < max_retries:
                            await asyncio.sleep(retry_interval * attempt)
                            continue
                        return None, None

                    content_type = response.headers.get("Content-Type", "")

                    if "/" in content_type:
                        image_format = content_type.split("/")[1].split(";")[0] or "png"
                    else:
                        image_format = "png"

                    target_path = None
                    if cache_key:
                        target_path = IMAGE_CACHE_DIR / f"{cache_key}.{image_format}"

                    image_path = await save_image_stream(
                        response.content, image_format, target_path=target_path
                    )
                    if image_path:
                        # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                        image_url_local = image_path
                        return image_url_local, image_path
            except aiohttp.ClientError as e:
                logger.error(f"ä¸‹è½½å›¾åƒå‘ç”Ÿç½‘ç»œå¼‚å¸¸: {e}")
            except Exception as e:
                logger.error(f"ä¸‹è½½å›¾åƒå¤±è´¥: {e}")

            if attempt < max_retries:
                await asyncio.sleep(retry_interval * attempt)

        return None, None


# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºAPIClientåˆ«å
APIClient = GeminiAPIClient

# å…¨å±€ API å®¢æˆ·ç«¯å®ä¾‹
_api_client: GeminiAPIClient | None = None


def get_api_client(api_keys: list[str]) -> GeminiAPIClient:
    """è·å–æˆ–åˆ›å»º API å®¢æˆ·ç«¯å®ä¾‹"""
    global _api_client
    if _api_client is None:
        _api_client = GeminiAPIClient(api_keys)
    return _api_client


def clear_api_client():
    """æ¸…é™¤å…¨å±€ API å®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _api_client
    _api_client = None

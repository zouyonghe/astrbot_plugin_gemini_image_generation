"""
APIå®¢æˆ·ç«¯æ¨¡å—
æä¾›Google Geminiå’ŒOpenAIå…¼å®¹APIçš„å®¢æˆ·ç«¯å®ç°
"""

from __future__ import annotations

import asyncio
import base64
import binascii
import hashlib
import json
import os
import re
import time
import urllib.parse
import urllib.request
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import aiohttp

from astrbot.api import logger

try:
    from .tl_utils import (
        encode_file_to_base64,
        get_plugin_data_dir,
        save_base64_image,
        save_image_data,
        save_image_stream,
    )
except ImportError:
    from pathlib import Path

    async def save_base64_image(
        base64_data: str, image_format: str = "png"
    ) -> str | None:
        """å ä½ç¬¦å‡½æ•°"""
        return None

    async def save_image_data(
        image_data: bytes, image_format: str = "png"
    ) -> str | None:
        """å ä½ç¬¦å‡½æ•°"""
        return None

    async def save_image_stream(stream_reader, image_format: str = "png", target_path=None):
        return None

    def encode_file_to_base64(file_path, chunk_size: int = 65536) -> str:
        return ""

    def get_plugin_data_dir() -> Path:
        return Path(".")


IMAGE_CACHE_DIR = get_plugin_data_dir() / "images" / "download_cache"


@dataclass
class ApiRequestConfig:
    """API è¯·æ±‚é…ç½®ï¼ˆåŸºäº Gemini å®˜æ–¹æ–‡æ¡£ï¼‰"""

    model: str
    prompt: str
    api_type: str = "openai"
    api_base: str | None = None
    api_key: str | None = None
    resolution: str | None = None
    aspect_ratio: str | None = None
    enable_grounding: bool = False
    response_modalities: str = "TEXT_IMAGE"  # é»˜è®¤åŒæ—¶è¿”å›æ–‡æœ¬å’Œå›¾åƒ
    max_tokens: int = 1000
    reference_images: list[str] | None = None
    response_text: str | None = None  # å­˜å‚¨æ–‡æœ¬å“åº”
    enable_smart_retry: bool = True  # æ™ºèƒ½é‡è¯•å¼€å…³
    enable_text_response: bool = False  # æ–‡æœ¬å“åº”å¼€å…³
    force_resolution: bool = False  # å¼ºåˆ¶ä¼ é€’åˆ†è¾¨ç‡å‚æ•°
    verbose_logging: bool = False  # è¯¦ç»†æ—¥å¿—å¼€å…³

    # å®˜æ–¹æ–‡æ¡£æ¨èå‚æ•°
    temperature: float = 0.7  # æ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼Œ0.0-1.0
    seed: int | None = None  # å›ºå®šç§å­ä»¥ç¡®ä¿ä¸€è‡´æ€§
    safety_settings: dict | None = None  # å®‰å…¨è®¾ç½®


class APIError(Exception):
    """API é”™è¯¯åŸºç±»"""

    def __init__(self, message: str, status_code: int = None, error_type: str = None):
        super().__init__(message)
        self.message = message
        self.status_code = status_code
        self.error_type = error_type


class GeminiAPIClient:
    """éµå¾ªå®˜æ–¹ API è§„èŒƒçš„ Gemini API å®¢æˆ·ç«¯

    ç‰¹æ€§ï¼š
    - æ”¯æŒ Google å®˜æ–¹ API å’Œ OpenAI API
    - æ”¯æŒè‡ªå®šä¹‰ API Base URLï¼ˆåä»£ï¼‰
    - æ”¯æŒä»»æ„æ¨¡å‹åç§°
    - éµå¾ªå®˜æ–¹ Gemini API è§„èŒƒ
    """

    # Google å®˜æ–¹ API é»˜è®¤åœ°å€
    GOOGLE_API_BASE = "https://generativelanguage.googleapis.com/v1beta"

    # OpenAI API é»˜è®¤åœ°å€
    OPENAI_API_BASE = "https://api.openai.com/v1"

    def __init__(self, api_keys: list[str]):
        """
        åˆå§‹åŒ– API å®¢æˆ·ç«¯

        Args:
            api_keys: API å¯†é’¥åˆ—è¡¨
        """
        self.api_keys = api_keys or []
        self.current_key_index = 0
        self._lock = asyncio.Lock()
        self.proxy = (
            os.environ.get("HTTPS_PROXY")
            or os.environ.get("https_proxy")
            or os.environ.get("HTTP_PROXY")
            or os.environ.get("http_proxy")
        )
        if self.proxy:
            logger.debug(f"æ£€æµ‹åˆ°ä»£ç†é…ç½®ï¼Œä½¿ç”¨ä»£ç†: {self.proxy}")
        logger.debug(f"API å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œæ”¯æŒ {len(self.api_keys)} ä¸ª API å¯†é’¥")
        self.verbose_logging: bool = False

    async def get_next_api_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ª API å¯†é’¥"""
        async with self._lock:
            if not self.api_keys:
                raise ValueError("API å¯†é’¥åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            key = self.api_keys[self.current_key_index % len(self.api_keys)]
            return key

    async def rotate_api_key(self):
        """è½®æ¢åˆ°ä¸‹ä¸€ä¸ª API å¯†é’¥"""
        async with self._lock:
            if len(self.api_keys) > 1:
                self.current_key_index = (self.current_key_index + 1) % len(
                    self.api_keys
                )
                logger.debug(
                    f"å·²è½®æ¢åˆ°ä¸‹ä¸€ä¸ª API å¯†é’¥ï¼Œå½“å‰ç´¢å¼•: {self.current_key_index}"
                )

    @staticmethod
    async def _prepare_google_payload(config: ApiRequestConfig) -> dict[str, Any]:
        """å‡†å¤‡ Google å®˜æ–¹ API è¯·æ±‚è´Ÿè½½ï¼ˆéµå¾ªå®˜æ–¹è§„èŒƒï¼‰"""
        parts = [{"text": config.prompt}]

        if config.reference_images:
            for image_input in config.reference_images[:14]:
                # å¯¹Google APIï¼Œæ‰€æœ‰å›¾åƒéƒ½éœ€è¦è½¬æ¢ä¸ºbase64
                mime_type, data = await GeminiAPIClient._normalize_image_input(image_input)
                if not data:
                    logger.warning(f"è·³è¿‡æ— æ³•è¯†åˆ«/è¯»å–çš„å‚è€ƒå›¾åƒ: {type(image_input)}")
                    continue

                parts.append({"inlineData": {"mimeType": mime_type, "data": data}})

        contents = [{"role": "user", "parts": parts}]

        generation_config = {"responseModalities": ["TEXT", "IMAGE"]}

        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œå›¾åƒç”Ÿæˆå¿…é¡»åŒæ—¶åŒ…å« TEXT å’Œ IMAGE modalities
        # è¿™æ ·å¯ä»¥ç¡®ä¿è¿”å›å›¾åƒè€Œä¸æ˜¯çº¯æ–‡æœ¬
        modalities_map = {
            "TEXT": ["TEXT"],
            "IMAGE": ["IMAGE"],
            "TEXT_IMAGE": ["TEXT", "IMAGE"],
        }

        # è·å–é…ç½®çš„æ¨¡æ€
        modalities = modalities_map.get(config.response_modalities, ["TEXT", "IMAGE"])

        # ç¡®ä¿åŒ…å«å›¾åƒæ¨¡æ€
        if "IMAGE" not in modalities:
            logger.warning("é…ç½®ä¸­ç¼ºå°‘ IMAGE modalityï¼Œè‡ªåŠ¨æ·»åŠ ä»¥æ”¯æŒå›¾åƒç”Ÿæˆ")
            modalities.append("IMAGE")

        # ç¡®ä¿åŒ…å«æ–‡æœ¬æ¨¡æ€
        if "TEXT" not in modalities:
            logger.debug("æ·»åŠ  TEXT modality ä»¥æä¾›æ›´å¥½çš„å…¼å®¹æ€§")
            modalities.append("TEXT")

        generation_config["responseModalities"] = modalities
        logger.debug(f"å“åº”æ¨¡æ€: {modalities}")

        image_config = {}

        # æ ¹æ®å®˜æ–¹æ–‡æ¡£è®¾ç½®å›¾åƒå°ºå¯¸
        if config.resolution:
            resolution = config.resolution.upper()

            if resolution in ["1K", "1024x1024"]:
                image_config["image_size"] = "1K"
                logger.debug("è®¾ç½®å›¾åƒå°ºå¯¸: 1K")
            elif resolution in ["2K", "2048x2048"]:
                image_config["image_size"] = "2K"
                logger.debug("è®¾ç½®å›¾åƒå°ºå¯¸: 2K")
            elif resolution in ["4K", "4096x4096"]:
                image_config["image_size"] = "4K"
                logger.debug("è®¾ç½®å›¾åƒå°ºå¯¸: 4K")
            else:
                # é»˜è®¤ä½¿ç”¨1K
                image_config["image_size"] = "1K"
                logger.warning(f"ä¸æ”¯æŒçš„åˆ†è¾¨ç‡: {config.resolution}ï¼Œä½¿ç”¨é»˜è®¤å°ºå¯¸ 1K")

        # è®¾ç½®é•¿å®½æ¯”
        if config.aspect_ratio and ":" in config.aspect_ratio:
            # å°†é•¿å®½æ¯”è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            ratio_map = {
                "1:1": "1:1",
                "16:9": "16:9",
                "9:16": "9:16",
                "3:2": "3:2",
                "4:3": "4:3",
            }
            ratio = ratio_map.get(config.aspect_ratio, config.aspect_ratio)
            image_config["aspect_ratio"] = ratio
            logger.debug(f"è®¾ç½®é•¿å®½æ¯”: {ratio}")
        elif config.aspect_ratio:
            logger.warning(
                f"ä¸æ”¯æŒçš„é•¿å®½æ¯”æ ¼å¼: {config.aspect_ratio}ï¼Œå°†ä½¿ç”¨é»˜è®¤é•¿å®½æ¯”"
            )

        if image_config:
            generation_config["image_config"] = image_config

        # æ·»åŠ å®˜æ–¹æ–‡æ¡£æ¨èå‚æ•°
        if config.temperature is not None:
            generation_config["temperature"] = config.temperature
        if config.seed is not None:
            generation_config["seed"] = config.seed
        if config.safety_settings:
            generation_config["safetySettings"] = config.safety_settings

        tools = []
        if config.enable_grounding:
            tools.append({"google_search": {}})

        payload = {"contents": contents, "generationConfig": generation_config}

        if tools:
            payload["tools"] = tools

        # è°ƒè¯•ï¼šè®°å½• image_config
        if "image_config" in generation_config:
            logger.debug(
                f"å®é™…å‘é€çš„ image_config: {generation_config['image_config']}"
            )

        return payload

    @staticmethod
    async def _prepare_openai_payload(config: ApiRequestConfig) -> dict[str, Any]:
        """å‡†å¤‡ OpenAI API è¯·æ±‚è´Ÿè½½"""
        message_content = [
            {"type": "text", "text": f"Generate an image: {config.prompt}"}
        ]

        if config.reference_images:
            # æœ¬åœ°ç¼“å­˜é¿å…é‡å¤å¤„ç†åŒä¸€å¼•ç”¨å›¾ï¼Œè®°å½•è€—æ—¶ä¾¿äºæ€§èƒ½è§‚å¯Ÿ
            processed_cache: dict[str, dict[str, Any]] = {}
            supported_exts = {
                "jpg",
                "jpeg",
                "png",
                "webp",
                "gif",
                "bmp",
                "tif",
                "tiff",
                "heic",
                "avif",
            }
            total_start = time.perf_counter()

            for idx, image_input in enumerate(config.reference_images[:6]):
                per_start = time.perf_counter()
                image_str = str(image_input).strip()
                if not image_str:
                    logger.warning(f"è·³è¿‡ç©ºç™½å‚è€ƒå›¾åƒ: idx={idx}")
                    continue

                if "&amp;" in image_str:
                    image_str = image_str.replace("&amp;", "&")

                # å‘½ä¸­ç¼“å­˜ç›´æ¥å¤ç”¨ï¼Œé¿å…é‡å¤ base64 å¤„ç†
                if image_str in processed_cache:
                    logger.debug(f"å‚è€ƒå›¾åƒå‘½ä¸­ç¼“å­˜: idx={idx}")
                    message_content.append(processed_cache[image_str])
                    continue

                parsed = urllib.parse.urlparse(image_str)
                image_payload: dict[str, Any] | None = None

                try:
                    # ä¼˜å…ˆå¤„ç† http(s) URLï¼Œç¡®ä¿ scheme å’Œ netloc åˆæ³•
                    if parsed.scheme in ("http", "https") and parsed.netloc:
                        ext = Path(parsed.path).suffix.lower().lstrip(".")
                        if ext and ext not in supported_exts:
                            logger.debug(
                                "å‚è€ƒå›¾åƒURLæ‰©å±•åä¸åœ¨å¸¸è§åˆ—è¡¨: idx=%s ext=%s url=%s",
                                idx,
                                ext,
                                image_str[:80],
                            )

                        image_payload = {
                            "type": "image_url",
                            "image_url": {"url": image_str},
                        }
                        logger.debug(
                            "OpenAIå…¼å®¹APIä½¿ç”¨URLå‚è€ƒå›¾: idx=%s ext=%s url=%s",
                            idx,
                            ext or "unknown",
                            image_str[:120],
                        )

                    # data URLï¼šç›´æ¥æ ¡éªŒ base64ï¼Œæœ‰æ•ˆåˆ™ä¸å†é‡å¤è½¬ç 
                    elif image_str.startswith("data:image/") and ";base64," in image_str:
                        header, _, data_part = image_str.partition(";base64,")
                        mime_type = header.replace("data:", "").lower()
                        try:
                            base64.b64decode(data_part, validate=True)
                        except (binascii.Error, ValueError) as e:
                            logger.warning(
                                "è·³è¿‡æ— æ•ˆçš„ data URL å‚è€ƒå›¾: idx=%s é”™è¯¯=%s", idx, e
                            )
                            mime_type = None

                        if mime_type:
                            ext = mime_type.split("/")[-1]
                            if ext and ext not in supported_exts:
                                logger.debug(
                                    "data URL å›¾ç‰‡æ ¼å¼ä¸å¸¸è§: idx=%s mime=%s", idx, mime_type
                                )
                            image_payload = {
                                "type": "image_url",
                                "image_url": {"url": image_str},
                            }
                            logger.debug(
                                "OpenAIå…¼å®¹APIä½¿ç”¨data URLå‚è€ƒå›¾: idx=%s mime=%s",
                                idx,
                                mime_type,
                            )

                    # å…¶ä»–è¾“å…¥äº¤ç»™è§„èŒƒåŒ–é€»è¾‘ï¼Œè‡ªåŠ¨è½¬æ¢ä¸º data URL
                    else:
                        mime_type, data = await GeminiAPIClient._normalize_image_input(
                            image_input
                        )
                        if not data:
                            logger.warning(
                                "è·³è¿‡æ— æ³•è¯†åˆ«/è¯»å–çš„å‚è€ƒå›¾åƒ: idx=%s type=%s",
                                idx,
                                type(image_input),
                            )
                            continue

                        if not mime_type or not mime_type.startswith("image/"):
                            logger.debug(
                                "æœªæ£€æµ‹åˆ°æ˜ç¡®çš„å›¾ç‰‡ MIMEï¼Œé»˜è®¤ä½¿ç”¨ image/png: idx=%s",
                                idx,
                            )
                            mime_type = "image/png"

                        ext = mime_type.split("/")[-1]
                        if ext and ext not in supported_exts:
                            logger.debug(
                                "è§„èŒƒåŒ–åå›¾ç‰‡æ ¼å¼ä¸å¸¸è§: idx=%s mime=%s", idx, mime_type
                            )

                        image_payload = {
                            "type": "image_url",
                            "image_url": {"url": f"data:{mime_type};base64,{data}"},
                        }

                    if image_payload:
                        message_content.append(image_payload)
                        processed_cache[image_str] = image_payload
                        elapsed_ms = (time.perf_counter() - per_start) * 1000
                        logger.debug(
                            "å‚è€ƒå›¾åƒå¤„ç†å®Œæˆ: idx=%s è€—æ—¶=%.2fms æ¥æº=%s",
                            idx,
                            elapsed_ms,
                            parsed.scheme or "normalized",
                        )
                except Exception as e:
                    logger.warning("å¤„ç†å‚è€ƒå›¾åƒæ—¶å‡ºç°å¼‚å¸¸: idx=%s err=%s", idx, e)
                    continue

            total_elapsed_ms = (time.perf_counter() - total_start) * 1000
            if processed_cache:
                logger.debug(
                    "å‚è€ƒå›¾åƒå¤„ç†ç»Ÿè®¡: æ€»æ•°=%s æ€»è€—æ—¶=%.2fms å¹³å‡=%.2fms",
                    len(processed_cache),
                    total_elapsed_ms,
                    total_elapsed_ms / len(processed_cache),
                )

        # OpenAI å…¼å®¹æ¥å£ä¸‹ï¼š
        # - ä½¿ç”¨ chat/completions
        # - modalities: ["image", "text"]
        # - image_config: {aspect_ratio, image_size}
        # - tools: [{google_search:{}}]ï¼ˆå½“å¯ç”¨æœç´¢æ¥åœ°æ—¶ï¼‰
        payload: dict[str, Any] = {
            "model": config.model,
            "messages": [{"role": "user", "content": message_content}],
            "max_tokens": config.max_tokens,
            "temperature": 0.7,
            "modalities": ["image", "text"],
        }

        # image_config ä¸ Gemini 3 Pro Image æ¨¡å‹ç›¸å…³çš„é…ç½®
        image_config: dict[str, Any] = {}

        if config.aspect_ratio:
            image_config["aspect_ratio"] = config.aspect_ratio

        # ä»…åœ¨ Gemini 3 Pro Image ç³»åˆ—æ¨¡å‹ä¸‹ä¼ é€’ image_size
        model_name = (config.model or "").lower()
        is_gemini_image_model = (
            "gemini-3-pro-image" in model_name
            or "gemini-3-pro-preview" in model_name
            or config.force_resolution  # å¦‚æœå¼ºåˆ¶å¼€å¯ï¼Œåˆ™å¿½ç•¥æ¨¡å‹åç§°æ£€æŸ¥
        )

        if is_gemini_image_model and config.resolution:
            # å‰ç«¯ router ä¾§ç›´æ¥ä¼ é€’ "1K"/"2K"/"4K"ï¼Œè¿™é‡Œä¿æŒä¸€è‡´
            image_config["image_size"] = config.resolution

        if image_config:
            payload["image_config"] = image_config

        # ä¸å‰ç«¯ router ä¸€è‡´ï¼šå¯ç”¨æœç´¢æ¥åœ°æ—¶ï¼Œé€šè¿‡ tools.google_search æ§åˆ¶
        if is_gemini_image_model and config.enable_grounding:
            payload["tools"] = [{"google_search": {}}]

        return payload

    @staticmethod
    async def _normalize_image_input(image_input: Any) -> tuple[str | None, str | None]:
        """
        å°†å‚è€ƒå›¾åƒè¾“å…¥è§„èŒƒåŒ–ä¸º (mime_type, base64_data)ã€‚
        æ”¯æŒ data URIã€çº¯/å®½æ¾ base64 å­—ç¬¦ä¸²ã€æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€file://ã€http/https URLã€‚
        """
        try:
            if image_input is None:
                return None, None

            image_str = str(image_input).strip()
            if "&amp;" in image_str:
                image_str = image_str.replace("&amp;", "&")
            if not image_str:
                return None, None

            # data URI
            if image_str.startswith("data:image/") and ";base64," in image_str:
                header, data = image_str.split(";base64,", 1)
                mime_type = header.replace("data:", "")
                return mime_type, data

            # file:// è·¯å¾„
            if image_str.startswith("file://"):
                parsed = urllib.parse.urlparse(image_str)
                image_path = Path(parsed.path)
                if image_path.exists() and image_path.is_file():
                    suffix = image_path.suffix.lower().lstrip(".") or "png"
                    mime_type = f"image/{suffix}"
                    try:
                        data = encode_file_to_base64(image_path)
                        return mime_type, data
                    except Exception as e:
                        logger.warning(f"è¯»å– file:// è·¯å¾„å¤±è´¥: {e}")
                else:
                    logger.warning(f"file:// è·¯å¾„ä¸å­˜åœ¨: {image_str}")

            # http(s) URL -> ä¸‹è½½å¹¶è½¬base64ï¼ˆå¸¦é‡è¯•å’Œè¯¦ç»†æ—¥å¿—ï¼‰
            if image_str.startswith("http://") or image_str.startswith("https://"):
                cleaned_url = image_str.replace("&amp;", "&")
                parsed_url = urllib.parse.urlparse(cleaned_url)

                # ç¼“å­˜å‘½ä¸­ç›´æ¥è¯»å–ï¼Œé¿å…é‡å¤ä¸‹è½½å’Œå†…å­˜å ç”¨
                try:
                    cache_key = hashlib.sha256(cleaned_url.encode("utf-8")).hexdigest()
                    IMAGE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
                    cached = next(IMAGE_CACHE_DIR.glob(f"{cache_key}.*"), None)
                    if cached and cached.exists() and cached.stat().st_size > 0:
                        mime_guess = f"image/{cached.suffix.lstrip('.') or 'png'}"
                        data = encode_file_to_base64(cached)
                        logger.debug(f"å‚è€ƒå›¾å‘½ä¸­ç¼“å­˜: {cleaned_url}")
                        return mime_guess, data
                except Exception as e:
                    logger.debug(f"æ£€æŸ¥å‚è€ƒå›¾ç¼“å­˜å¤±è´¥: {e}")

                # ä¼˜åŒ–è¯·æ±‚å¤´ï¼Œå…¼å®¹ CQ ç å›¾æœåŠ¡å™¨
                headers: dict[str, str] = {
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36"
                    ),
                    "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Accept-Encoding": "gzip, deflate, br",
                }
                if parsed_url.scheme and parsed_url.netloc:
                    headers["Referer"] = f"{parsed_url.scheme}://{parsed_url.netloc}"
                if "gchat.qpic.cn" in (parsed_url.netloc or ""):
                    headers["Referer"] = "https://qun.qq.com"
                    headers["Origin"] = "https://qun.qq.com"
                    headers.setdefault("Accept", headers["Accept"] + ",image/png")

                timeout = aiohttp.ClientTimeout(total=12, connect=5)
                max_retries = 3
                retry_interval = 1.0

                async with aiohttp.ClientSession(timeout=timeout, trust_env=True) as session:
                    fallback_reason = None

                    for attempt in range(1, max_retries + 1):
                        try:
                            async with session.get(cleaned_url, headers=headers) as resp:
                                if resp.status == 200:
                                    content_type = resp.headers.get("Content-Type", "image/png")
                                    mime_type = content_type.split(";")[0] if content_type else "image/png"
                                    image_format = (
                                        mime_type.split("/")[1] if "/" in mime_type else "png"
                                    )

                                    cache_path = IMAGE_CACHE_DIR / f"{cache_key}.{image_format}"
                                    saved_path = await save_image_stream(
                                        resp.content, image_format, cache_path
                                    )
                                    if saved_path:
                                        data = encode_file_to_base64(Path(saved_path))
                                        return mime_type, data

                                    logger.warning(
                                        "ä¸‹è½½å‚è€ƒå›¾ä¸ºç©º: attempt=%s/%s url=%s",
                                        attempt,
                                        max_retries,
                                        cleaned_url,
                                    )
                                else:
                                    try:
                                        err_text = (await resp.text())[:200]
                                    except Exception:
                                        err_text = ""
                                    extra_hint = ""
                                    if resp.status == 400 and "gchat.qpic.cn" in (parsed_url.netloc or ""):
                                        extra_hint = "ï¼ˆQQ å›¾ç‰‡å¯èƒ½éœ€è¦æœ‰æ•ˆ Refererï¼Œè¯·å°è¯•é‡æ–°å‘é€å›¾ç‰‡æˆ–ç¨åå†è¯•ï¼‰"
                                    logger.warning(
                                        "ä¸‹è½½å›¾ç‰‡å¤±è´¥: HTTP %s %s attempt=%s/%s url=%s å“åº”æ‘˜è¦=%s %s",
                                        resp.status,
                                        resp.reason or "",
                                        attempt,
                                        max_retries,
                                        cleaned_url,
                                        err_text,
                                        extra_hint,
                                    )
                                    if resp.status == 400:
                                        fallback_reason = "http400"
                                        break
                        except (
                            aiohttp.ClientConnectionError,
                            aiohttp.ClientPayloadError,
                            aiohttp.ServerTimeoutError,
                            asyncio.TimeoutError,
                        ) as e:
                            logger.warning(
                                "ä¸‹è½½å›¾ç‰‡è¿æ¥å¼‚å¸¸: %s attempt=%s/%s url=%s",
                                e,
                                attempt,
                                max_retries,
                                cleaned_url,
                            )
                            if attempt == max_retries:
                                fallback_reason = "aiohttp_error"
                        except Exception as e:
                            logger.warning(
                                "ä¸‹è½½å‚è€ƒå›¾å¤±è´¥: %s attempt=%s/%s url=%s",
                                e,
                                attempt,
                                max_retries,
                                cleaned_url,
                            )
                            if attempt == max_retries:
                                fallback_reason = "aiohttp_error"

                        if attempt < max_retries:
                            await asyncio.sleep(retry_interval * attempt)

                    if not fallback_reason:
                        fallback_reason = "aiohttp_error"

                if fallback_reason:
                    logger.debug("aiohttp ä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨ urllib åå¤‡æ–¹æ¡ˆ: reason=%s url=%s", fallback_reason, cleaned_url)
                    fallback_headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                        "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
                    }

                    async def _download_with_urllib():
                        def _blocking_download():
                            try:
                                req = urllib.request.Request(cleaned_url, headers=fallback_headers)
                                with urllib.request.urlopen(req, timeout=12) as resp:
                                    status = getattr(resp, "status", None) or resp.getcode()
                                    if status != 200:
                                        logger.warning(
                                            "urllib åå¤‡ä¸‹è½½å¤±è´¥: HTTP %s url=%s",
                                            status,
                                            cleaned_url,
                                        )
                                        return None

                                    content_type = resp.headers.get("Content-Type", "image/png")
                                    mime_type = (
                                        content_type.split(";")[0] if content_type else "image/png"
                                    )
                                    image_format = (
                                        mime_type.split("/")[1] if "/" in mime_type else "png"
                                    )

                                    cache_path = IMAGE_CACHE_DIR / f"{cache_key}.{image_format}"
                                    try:
                                        cache_path.parent.mkdir(parents=True, exist_ok=True)
                                        data_bytes = resp.read()
                                        if not data_bytes:
                                            logger.warning("urllib åå¤‡ä¸‹è½½è¿”å›ç©ºæ•°æ®: url=%s", cleaned_url)
                                            return None

                                        with open(cache_path, "wb") as f:
                                            f.write(data_bytes)

                                        encoded = base64.b64encode(data_bytes).decode("utf-8")
                                        return mime_type, encoded
                                    except Exception as e:
                                        logger.warning(
                                            "urllib åå¤‡ä¸‹è½½å†™å…¥ç¼“å­˜å¤±è´¥: %s url=%s", e, cleaned_url
                                        )
                                        return None
                            except Exception as e:
                                logger.warning("urllib åå¤‡ä¸‹è½½å¼‚å¸¸: %s url=%s", e, cleaned_url)
                                return None

                        return await asyncio.to_thread(_blocking_download)

                    mime_and_data = await _download_with_urllib()
                    if mime_and_data:
                        return mime_and_data

            # å°è¯•è§£æä¸ºè£¸/å®½æ¾ base64 æ•°æ®ï¼ˆåœ¨æ–‡ä»¶è·¯å¾„ä¹‹å‰ï¼Œé¿å…é•¿å­—ç¬¦ä¸²å¯¼è‡´ "File name too long"ï¼‰
            if len(image_str) > 255 or not any(
                char in image_str for char in ["/", "\\", "."]
            ):
                try:
                    cleaned = image_str.replace("\n", "").replace(" ", "")
                    decoded = base64.b64decode(cleaned, validate=False)
                    if decoded and len(decoded) > 100:
                        normalized = base64.b64encode(decoded).decode("utf-8")
                        return "image/png", normalized
                except (binascii.Error, ValueError):
                    pass

            # æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆä»…å½“å­—ç¬¦ä¸²é•¿åº¦åˆç†æ—¶å°è¯•ï¼‰
            if len(image_str) <= 255:
                candidate_paths = [
                    Path(image_str),
                    get_plugin_data_dir() / image_str,
                    Path.cwd() / image_str,
                ]
                for image_path in candidate_paths:
                    try:
                        if image_path.exists() and image_path.is_file():
                            suffix = image_path.suffix.lower().lstrip(".") or "png"
                            mime_type = f"image/{suffix}"
                            data = encode_file_to_base64(image_path)
                            return mime_type, data
                    except OSError:
                        continue

            return None, None
        except Exception as e:
            logger.warning(f"å‚è€ƒå›¾åƒè§„èŒƒåŒ–å¤±è´¥: {e}")
            return None, None

    async def _get_api_url(
        self, config: ApiRequestConfig
    ) -> tuple[str, dict[str, str], dict[str, Any]]:
        """
        æ ¹æ®é…ç½®è·å– API URLã€è¯·æ±‚å¤´å’Œè´Ÿè½½

        æ™ºèƒ½å¤„ç†APIè·¯å¾„å‰ç¼€ï¼Œæ— éœ€æ‰‹åŠ¨è¾“å…¥/v1æˆ–/v1beta
        """
        # ç¡®å®š API åŸºç¡€åœ°å€ï¼ˆæ”¯æŒåä»£ï¼‰
        if config.api_base:
            api_base = config.api_base.rstrip("/")
            logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰ API Base: {api_base}")
        else:
            if config.api_type == "google":
                api_base = self.GOOGLE_API_BASE
            else:  # openai å…¼å®¹æ ¼å¼
                api_base = self.OPENAI_API_BASE

            logger.debug(f"ä½¿ç”¨é»˜è®¤ API Base ({config.api_type}): {api_base}")

        # æ™ºèƒ½æ„å»ºå®Œæ•´URLï¼Œè‡ªåŠ¨æ·»åŠ æ­£ç¡®çš„è·¯å¾„å‰ç¼€ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        if config.api_type == "google":
            # Google API éœ€è¦ç‰ˆæœ¬å‰ç¼€
            if not config.api_base or api_base == self.GOOGLE_API_BASE:
                # ä½¿ç”¨é»˜è®¤å®˜æ–¹åœ°å€ï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´è·¯å¾„
                url = f"{api_base}/models/{config.model}:generateContent"
            elif not any(api_base.endswith(suffix) for suffix in ["/v1beta", "/v1"]):
                # è‡ªå®šä¹‰åœ°å€ä½†æ²¡æœ‰ç‰ˆæœ¬å‰ç¼€ï¼Œè‡ªåŠ¨æ·»åŠ 
                url = f"{api_base}/v1beta/models/{config.model}:generateContent"
                logger.debug("ä¸ºGoogle APIè‡ªåŠ¨æ·»åŠ v1betaå‰ç¼€")
            else:
                # å·²ç»åŒ…å«ç‰ˆæœ¬å‰ç¼€ï¼Œç›´æ¥ä½¿ç”¨
                url = f"{api_base}/models/{config.model}:generateContent"
                logger.debug("ä½¿ç”¨å·²åŒ…å«ç‰ˆæœ¬å‰ç¼€çš„Google APIåœ°å€")

            payload = await self._prepare_google_payload(config)
            headers = {
                "x-goog-api-key": config.api_key,
                "Content-Type": "application/json",
            }
        else:
            # OpenAI å…¼å®¹æ ¼å¼
            if not config.api_base or api_base == self.OPENAI_API_BASE:
                # ä½¿ç”¨é»˜è®¤åœ°å€ï¼Œéœ€è¦å®Œæ•´è·¯å¾„
                url = f"{api_base}/chat/completions"
            elif not any(api_base.endswith(suffix) for suffix in ["/v1", "/v1beta"]):
                # è‡ªå®šä¹‰åœ°å€ä½†æ²¡æœ‰ç‰ˆæœ¬å‰ç¼€ï¼Œè‡ªåŠ¨æ·»åŠ 
                url = f"{api_base}/v1/chat/completions"
                logger.debug("ä¸ºOpenAIå…¼å®¹APIè‡ªåŠ¨æ·»åŠ v1å‰ç¼€")
            else:
                # å·²ç»åŒ…å«ç‰ˆæœ¬å‰ç¼€ï¼Œç›´æ¥ä½¿ç”¨
                url = f"{api_base}/chat/completions"
                logger.debug("ä½¿ç”¨å·²åŒ…å«ç‰ˆæœ¬å‰ç¼€çš„OpenAIå…¼å®¹APIåœ°å€")

            payload = await self._prepare_openai_payload(config)
            headers = {
                "Authorization": f"Bearer {config.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/astrbot",
                "X-Title": "AstrBot Gemini Image Advanced",
            }

        logger.debug(f"æ™ºèƒ½æ„å»ºAPI URL: {url}")
        return url, headers, payload

    async def generate_image(
        self,
        config: ApiRequestConfig,
        max_retries: int = 3,
        total_timeout: int = 120,
        per_retry_timeout: int = None,
        max_total_time: int = None,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """
        ç”Ÿæˆå›¾åƒ

        Args:
            config: è¯·æ±‚é…ç½®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            total_timeout: æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            (image_urls, image_paths, text_content, thought_signature)ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨å’ŒNone
        """
        if not self.api_keys:
            raise ValueError("æœªé…ç½® API å¯†é’¥")

        if not config.api_key:
            config.api_key = await self.get_next_api_key()

        # è·å–è¯·æ±‚ä¿¡æ¯
        url, headers, payload = await self._get_api_url(config)

        logger.debug(f"ä½¿ç”¨ {config.model} (é€šè¿‡ {config.api_type}) ç”Ÿæˆå›¾åƒ")
        logger.debug(f"API ç«¯ç‚¹: {url[:80]}...")

        if config.resolution or config.aspect_ratio:
            logger.debug(
                f"åˆ†è¾¨ç‡: {config.resolution or 'é»˜è®¤'}, é•¿å®½æ¯”: {config.aspect_ratio or 'é»˜è®¤'}"
            )

        if config.api_base:
            logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰ API Base: {config.api_base}")

        # åŒæ­¥è¯¦ç»†æ—¥å¿—å¼€å…³ï¼Œä¾¿äºåœ¨å†…éƒ¨ç½‘ç»œè¯·æ±‚ä¸­æ§åˆ¶è¾“å‡ºç²’åº¦
        self.verbose_logging = bool(getattr(config, "verbose_logging", False))

        return await self._make_request(
            url=url,
            payload=payload,
            headers=headers,
            api_type=config.api_type,
            model=config.model,
            max_retries=max_retries,
            total_timeout=total_timeout,
        )

    async def _make_request(
        self,
        url: str,
        payload: dict[str, Any],
        headers: dict[str, str],
        api_type: str,
        model: str,
        max_retries: int,
        total_timeout: int = 120,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """æ‰§è¡Œ API è¯·æ±‚å¹¶å¤„ç†å“åº”ï¼Œæ¯ä¸ªé‡è¯•æœ‰ç‹¬ç«‹çš„è¶…æ—¶æ§åˆ¶"""

        current_retry = 0
        last_error = None

        while current_retry < max_retries:
            try:
                # æ¯ä¸ªé‡è¯•ä½¿ç”¨ç‹¬ç«‹çš„è¶…æ—¶æ§åˆ¶ï¼Œä¸å…±äº«æ€»è¶…æ—¶æ—¶é—´
                timeout = aiohttp.ClientTimeout(
                    total=total_timeout, sock_read=total_timeout
                )
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    logger.debug(f"å‘é€è¯·æ±‚ï¼ˆé‡è¯• {current_retry}/{max_retries - 1}ï¼‰")
                    return await self._perform_request(
                        session, url, payload, headers, api_type, model
                    )

            except asyncio.CancelledError:
                # åªæœ‰æ¡†æ¶å–æ¶ˆæ‰ä¸é‡è¯•ï¼ˆè¿™æ˜¯æœ€é¡¶å±‚çš„è¶…æ—¶ï¼‰
                logger.debug("è¯·æ±‚è¢«æ¡†æ¶å–æ¶ˆï¼ˆå·¥å…·è°ƒç”¨æ€»è¶…æ—¶ï¼‰ï¼Œä¸å†é‡è¯•")
                timeout_msg = "å›¾åƒç”Ÿæˆæ—¶é—´è¿‡é•¿ï¼Œè¶…å‡ºäº†æ¡†æ¶é™åˆ¶ã€‚è¯·å°è¯•ç®€åŒ–å›¾åƒæè¿°æˆ–åœ¨æ¡†æ¶é…ç½®ä¸­å¢åŠ  tool_call_timeout åˆ° 90-120 ç§’ã€‚"
                raise APIError(timeout_msg, None, "cancelled") from None
            except Exception as e:
                error_msg = str(e)
                error_type = self._classify_error(e, error_msg)

                # åˆ¤æ–­æ˜¯å¦å¯é‡è¯•çš„é”™è¯¯
                if self._is_retryable_error(error_type, e):
                    last_error = APIError(error_msg, None, error_type)
                    logger.warning(
                        f"å¯é‡è¯•é”™è¯¯ (é‡è¯• {current_retry + 1}/{max_retries}): {error_msg}"
                    )

                    current_retry += 1
                    if current_retry < max_retries:
                        # æŒ‡æ•°é€€é¿å»¶è¿Ÿï¼š2ç§’ã€4ç§’ã€8ç§’â€¦â€¦æœ€å¤§10ç§’
                        delay = min(2 ** (current_retry + 1), 10)
                        logger.debug(f"ç­‰å¾… {delay} ç§’åé‡è¯•...")
                        await asyncio.sleep(delay)
                        continue  # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
                    else:
                        logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œç”Ÿæˆå¤±è´¥")
                else:
                    # ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç«‹å³æŠ›å‡º
                    logger.error(f"ä¸å¯é‡è¯•é”™è¯¯: {error_msg}")
                    raise APIError(error_msg, None, error_type) from None

        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›æœ€åä¸€æ¬¡é”™è¯¯
        if last_error:
            raise last_error

        return [], [], None, None

    def _classify_error(self, exception: Exception, error_msg: str) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        if isinstance(exception, asyncio.TimeoutError):
            return "timeout"
        elif "timeout" in error_msg.lower():
            return "timeout"
        elif "connection" in error_msg.lower():
            return "network"
        elif isinstance(exception, aiohttp.ClientError):
            return "network"
        else:
            return "unknown"

    def _is_retryable_error(self, error_type: str, exception: Exception) -> bool:
        """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•"""
        # ç‰¹æ®Šå¤„ç†ï¼šæœªç”Ÿæˆå›¾åƒçš„é‡è¯•
        if error_type == "no_image_retry":
            return True

        # å¯é‡è¯•çš„é”™è¯¯ï¼šè¶…æ—¶ã€ç½‘ç»œé”™è¯¯ã€æœåŠ¡å™¨é”™è¯¯
        if error_type in ["timeout", "network"]:
            return True

        # HTTP çŠ¶æ€ç åˆ¤æ–­
        if hasattr(exception, "status"):
            status = exception.status
            # å¯é‡è¯•ï¼š408, 500, 502, 503, 504
            # ä¸å¯é‡è¯•ï¼š401, 402, 403, 422, 429ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰
            if status in [408, 500, 502, 503, 504]:
                return True
            elif status in [401, 402, 403, 422, 429]:
                return False

        return True  # é»˜è®¤é‡è¯•æœªçŸ¥é”™è¯¯

    async def _perform_request(
        self,
        session: aiohttp.ClientSession,
        url: str,
        payload: dict[str, Any],
        headers: dict[str, str],
        api_type: str,
        model: str,
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """æ‰§è¡Œå®é™…çš„HTTPè¯·æ±‚"""
        logger.debug(f"å‘é€è¯·æ±‚åˆ°: {url[:100]}...")

        async with session.post(
            url, json=payload, headers=headers, proxy=self.proxy
        ) as response:
            logger.debug(f"å“åº”çŠ¶æ€: {response.status}")
            response_text = await response.text()

            # è§£æ JSON å“åº”ï¼Œæ·»åŠ é”™è¯¯å¤„ç†
            try:
                response_data = json.loads(response_text) if response_text else {}
            except json.JSONDecodeError as e:
                logger.error(f"JSON è§£æå¤±è´¥: {e}")
                logger.error(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response_text[:500]}")
                raise APIError(
                    f"API è¿”å›äº†æ— æ•ˆçš„ JSON å“åº”: {e}", response.status
                ) from None

            if response.status == 200:
                logger.debug("API è°ƒç”¨æˆåŠŸ")
                if api_type == "google":
                    return await self._parse_gresponse(response_data, session)
                else:  # openai å…¼å®¹æ ¼å¼
                    return await self._parse_openai_response(response_data, session)
            elif response.status in [429, 402, 403]:
                error_msg = response_data.get("error", {}).get(
                    "message", f"HTTP {response.status}"
                )
                logger.warning(f"API é…é¢/æƒé™é—®é¢˜: {error_msg}")
                raise APIError(error_msg, response.status, "quota")
            else:
                error_msg = response_data.get("error", {}).get(
                    "message", f"HTTP {response.status}"
                )
                logger.warning(f"API é”™è¯¯: {error_msg}")
                raise APIError(error_msg, response.status)

    async def _parse_gresponse(
        self, response_data: dict, session: aiohttp.ClientSession
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """è§£æ Google å®˜æ–¹ API å“åº”"""
        import asyncio

        parse_start = asyncio.get_event_loop().time()
        logger.debug("ğŸ” å¼€å§‹è§£æAPIå“åº”æ•°æ®...")

        if "candidates" not in response_data or not response_data["candidates"]:
            if "promptFeedback" in response_data:
                feedback = response_data["promptFeedback"]
                logger.warning(f"è¯·æ±‚è¢«é˜»æ­¢: {feedback}")
            else:
                logger.error(f"å“åº”ä¸­æ²¡æœ‰ candidates: {response_data}")
            return [], [], None, None

        candidates = response_data["candidates"]
        logger.debug(f"ğŸ“ æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç»“æœ")

        image_urls: list[str] = []
        image_paths: list[str] = []
        text_chunks: list[str] = []
        thought_signature = None

        for idx, candidate in enumerate(candidates):
            finish_reason = candidate.get("finishReason")
            if finish_reason in ["SAFETY", "RECITATION"]:
                logger.warning(f"å€™é€‰ {idx} ç”Ÿæˆè¢«é˜»æ­¢: {finish_reason}")
                continue

            content = candidate.get("content", {})
            parts = content.get("parts") or []
            logger.debug(f"ğŸ“‹ å€™é€‰ {idx} åŒ…å« {len(parts)} ä¸ªéƒ¨åˆ†")

            for i, part in enumerate(parts):
                try:
                    logger.debug(f"æ£€æŸ¥å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpart: {list(part.keys())}")

                    if "thoughtSignature" in part and not thought_signature:
                        thought_signature = part["thoughtSignature"]
                        logger.debug(f"ğŸ§  æ‰¾åˆ°æ€ç»´ç­¾å: {thought_signature[:50]}...")

                    inline_data = part.get("inlineData") or part.get("inline_data")
                    if inline_data and not part.get("thought", False):
                        mime_type = (
                            inline_data.get("mimeType")
                            or inline_data.get("mime_type")
                            or "image/png"
                        )
                        base64_data = inline_data.get("data", "")

                        logger.debug(
                            f"ğŸ¯ æ‰¾åˆ°å›¾åƒæ•°æ® (å€™é€‰{idx} ç¬¬{i + 1}éƒ¨åˆ†): {mime_type}, å¤§å°: {len(base64_data)} å­—ç¬¦"
                        )

                        if base64_data:
                            image_format = (
                                mime_type.split("/")[1] if "/" in mime_type else "png"
                            )

                            logger.debug("ğŸ’¾ å¼€å§‹ä¿å­˜å›¾åƒæ–‡ä»¶...")
                            save_start = asyncio.get_event_loop().time()

                            saved_path = await save_base64_image(
                                base64_data, image_format
                            )

                            save_end = asyncio.get_event_loop().time()
                            logger.debug(
                                f"âœ… å›¾åƒä¿å­˜å®Œæˆï¼Œè€—æ—¶: {save_end - save_start:.2f}ç§’"
                            )

                            if saved_path:
                                image_paths.append(saved_path)
                                image_urls.append(saved_path)
                        else:
                            logger.warning(f"å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpartæœ‰inlineDataä½†dataä¸ºç©º")
                    elif "thought" in part and part.get("thought", False):
                        logger.debug(f"å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpartæ˜¯æ€è€ƒå†…å®¹")
                    elif "text" in part and not part.get("thought", False):
                        text_chunks.append(part.get("text", ""))
                    else:
                        logger.debug(
                            f"å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpartä¸æ˜¯å›¾åƒä¹Ÿä¸æ˜¯æ€è€ƒ: {list(part.keys())}"
                        )
                except Exception as e:
                    logger.error(
                        f"å¤„ç†å€™é€‰ {idx} çš„ç¬¬ {i} ä¸ªpartæ—¶å‡ºé”™: {e}", exc_info=True
                    )

        logger.debug(f"ğŸ–¼ï¸ å…±æ‰¾åˆ° {len(image_paths)} å¼ å›¾ç‰‡")

        text_content = (
            " ".join(chunk for chunk in text_chunks if chunk).strip()
            if text_chunks
            else None
        )
        if text_content:
            logger.debug(f"ğŸ¯ æ‰¾åˆ°æ–‡æœ¬å†…å®¹: {text_content[:100]}...")

        if image_paths:
            parse_end = asyncio.get_event_loop().time()
            logger.debug(f"ğŸ‰ APIå“åº”è§£æå®Œæˆï¼Œæ€»è€—æ—¶: {parse_end - parse_start:.2f}ç§’")
            return image_urls, image_paths, text_content, thought_signature

        if text_content:
            logger.warning("APIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæœªç”Ÿæˆå›¾åƒï¼Œå°†è§¦å‘é‡è¯•")
            raise APIError(
                "å›¾åƒç”Ÿæˆå¤±è´¥ï¼šAPIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæ­£åœ¨é‡è¯•...",
                500,
                "no_image_retry",
            )

        logger.error("æœªåœ¨å“åº”ä¸­æ‰¾åˆ°å›¾åƒæ•°æ®")
        raise APIError(
            "å›¾åƒç”Ÿæˆå¤±è´¥ï¼šå“åº”æ ¼å¼å¼‚å¸¸ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæ•°æ®", None, "invalid_response"
        )

    async def _parse_openai_response(
        self, response_data: dict, session: aiohttp.ClientSession
    ) -> tuple[list[str], list[str], str | None, str | None]:
        """è§£æ OpenAI API å“åº”"""

        image_urls: list[str] = []
        image_paths: list[str] = []
        text_content = None
        thought_signature = None

        if "choices" in response_data:
            choice = response_data["choices"][0]
            message = choice.get("message", {})
            content = message.get("content", "")

            text_chunks: list[str] = []
            image_candidates: list[str] = []
            extracted_urls: list[str] = []

            if isinstance(content, list):
                for part in content:
                    if not isinstance(part, dict):
                        continue

                    part_type = part.get("type")
                    if part_type == "text" and "text" in part:
                        text_val = str(part.get("text", ""))
                        text_chunks.append(text_val)
                        extracted_urls.extend(self._find_image_urls_in_text(text_val))
                    elif part_type == "image_url":
                        image_obj = part.get("image_url") or {}
                        if isinstance(image_obj, dict):
                            url_val = image_obj.get("url")
                            if url_val:
                                image_candidates.append(url_val)
            elif isinstance(content, str):
                text_chunks.append(content)
                extracted_urls.extend(self._find_image_urls_in_text(content))

            # æ ‡å‡† images å­—æ®µï¼ˆå…¼å®¹ Gemini/OpenAI æ··åˆæ ¼å¼ï¼‰
            if "images" in message and message["images"]:
                for image_item in message["images"]:
                    if not isinstance(image_item, dict):
                        continue

                    # å…¸å‹æ ¼å¼ï¼š{"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
                    image_obj = image_item.get("image_url")
                    if isinstance(image_obj, dict):
                        url_val = image_obj.get("url")
                        if isinstance(url_val, str) and url_val:
                            image_candidates.append(url_val)
                    elif isinstance(image_obj, str) and image_obj:
                        image_candidates.append(image_obj)
                    # é€€åŒ–æ ¼å¼ï¼š{"url": "..."}
                    elif isinstance(image_item.get("url"), str):
                        image_candidates.append(image_item["url"])

            # åˆå¹¶åœ¨æ–‡æœ¬é‡Œè§£æåˆ°çš„å›¾åƒ URL
            if extracted_urls:
                image_candidates.extend(extracted_urls)

            # ç»„è£…æ–‡æœ¬å†…å®¹
            if text_chunks:
                text_content = " ".join([t for t in text_chunks if t]).strip() or None

            # æŒ‰é¡ºåºå¤„ç†å›¾åƒå€™é€‰
            for candidate_url in image_candidates:
                if isinstance(candidate_url, str) and candidate_url.startswith(
                    "data:image/"
                ):
                    image_url, image_path = await self._parse_data_uri(candidate_url)
                elif isinstance(candidate_url, str):
                    # å¯¹äºå¯è®¿é—®çš„ http(s) é“¾æ¥ï¼Œç›´æ¥è¿”å› URLï¼Œé¿å…é‡å¤ä¸‹è½½å ç”¨å¸¦å®½
                    if candidate_url.startswith("http://") or candidate_url.startswith(
                        "https://"
                    ):
                        image_urls.append(candidate_url)
                        logger.debug(
                            f"ğŸ–¼ï¸ OpenAI è¿”å›å¯ç›´æ¥è®¿é—®çš„å›¾åƒé“¾æ¥: {candidate_url}"
                        )
                        continue
                    image_url, image_path = await self._download_image(
                        candidate_url, session, use_cache=False
                    )
                else:
                    logger.warning(f"è·³è¿‡éå­—ç¬¦ä¸²ç±»å‹çš„å›¾åƒURL: {type(candidate_url)}")
                    continue

                if image_url or image_path:
                    if image_url:
                        image_urls.append(image_url)
                    if image_path:
                        image_paths.append(image_path)

            # content ä¸­æŸ¥æ‰¾å†…è” data URIï¼ˆæ–‡æœ¬é‡Œï¼‰
            extracted_urls: list[str] = []
            extracted_paths: list[str] = []

            if isinstance(content, str):
                extracted_urls, extracted_paths = await self._extract_from_content(
                    content
                )
            elif text_content:
                extracted_urls, extracted_paths = await self._extract_from_content(
                    text_content
                )

            if extracted_urls or extracted_paths:
                image_urls.extend(extracted_urls)
                image_paths.extend(extracted_paths)

        # OpenAI æ ¼å¼
        elif "data" in response_data and response_data["data"]:
            for image_item in response_data["data"]:
                if "url" in image_item:
                    image_url, image_path = await self._download_image(
                        image_item["url"], session, use_cache=False
                    )
                    if image_url:
                        image_urls.append(image_url)
                    if image_path:
                        image_paths.append(image_path)
                elif "b64_json" in image_item:
                    image_path = await save_base64_image(image_item["b64_json"], "png")
                    if image_path:
                        # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                        image_urls.append(image_path)
                        image_paths.append(image_path)

        if image_urls or image_paths:
            logger.debug(f"ğŸ–¼ï¸ OpenAI æ”¶é›†åˆ° {len(image_paths) or len(image_urls)} å¼ å›¾ç‰‡")
            return image_urls, image_paths, text_content, thought_signature

        # å¦‚æœåªæœ‰æ–‡æœ¬å†…å®¹ï¼Œä¹Ÿè¿”å›
        if text_content:
            # å¦‚æœé…ç½®äº†éœ€è¦æ–‡æœ¬å“åº”ï¼Œä¸”ç¡®å®æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¿™é‡Œåº”è¯¥æŠ¥é”™è§¦å‘é‡è¯•è€Œä¸æ˜¯ç›´æ¥è¿”å›æ–‡æœ¬
            # é™¤éè¿™æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬è¯·æ±‚ï¼ˆä½†åœ¨ç”Ÿå›¾æ’ä»¶é‡Œé€šå¸¸ä¸æ˜¯ï¼‰
            logger.warning("OpenAIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæœªç”Ÿæˆå›¾åƒï¼Œå°†è§¦å‘é‡è¯•")
            raise APIError(
                "å›¾åƒç”Ÿæˆå¤±è´¥ï¼šAPIåªè¿”å›äº†æ–‡æœ¬å“åº”ï¼Œæ­£åœ¨é‡è¯•...", 500, "no_image_retry"
            )

        logger.warning("OpenAI å“åº”æ ¼å¼ä¸æ”¯æŒæˆ–æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
        return image_urls, image_paths, text_content, thought_signature

    async def _parse_data_uri(self, data_uri: str) -> tuple[str | None, str | None]:
        """è§£æ data URI æ ¼å¼çš„å›¾åƒ"""
        try:
            if ";base64," not in data_uri:
                logger.error("æ— æ•ˆçš„ data URI æ ¼å¼")
                return None, None

            header, base64_data = data_uri.split(";base64,", 1)
            mime_type = header.replace("data:", "")
            format_type = mime_type.split("/")[1] if "/" in mime_type else "png"

            image_path = await save_base64_image(base64_data, format_type)
            if image_path:
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                image_url = image_path
                return image_url, image_path
        except Exception as e:
            logger.error(f"è§£æ data URI å¤±è´¥: {e}")

        return None, None

    async def _extract_from_content(
        self, content: str
    ) -> tuple[list[str], list[str]]:
        """ä»æ–‡æœ¬å†…å®¹ä¸­æå–æ‰€æœ‰ data URI å›¾åƒï¼Œä¿æŒé¡ºåº"""
        pattern = r"data:image/([^;]+);base64,([A-Za-z0-9+/=\s]+)"
        matches = re.findall(pattern, content)

        image_urls: list[str] = []
        image_paths: list[str] = []

        for image_format, base64_string in matches:
            image_path = await save_base64_image(base64_string, image_format)
            if image_path:
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                image_url = image_path
                image_urls.append(image_url)
                image_paths.append(image_path)

        return image_urls, image_paths

    def _find_image_urls_in_text(self, text: str) -> list[str]:
        """ä»æ–‡æœ¬/Markdownä¸­æå–å¯ç”¨çš„ http(s) å›¾ç‰‡é“¾æ¥"""
        if not text:
            return []

        # Markdown å›¾ç‰‡è¯­æ³•ä¸è£¸éœ²çš„å›¾ç‰‡é“¾æ¥
        markdown_pattern = r"!\[[^\]]*\]\((https?://[^)]+)\)"
        raw_pattern = r"(https?://[^\s)]+\.(?:png|jpe?g|gif|webp|bmp|tiff|avif))(?:\b|$)"

        urls: list[str] = []
        seen: set[str] = set()
        for pattern in (markdown_pattern, raw_pattern):
            for match in re.findall(pattern, text, flags=re.IGNORECASE):
                cleaned = match.strip().replace("&amp;", "&")
                if cleaned not in seen:
                    seen.add(cleaned)
                    urls.append(cleaned)

        return urls

    async def _download_image(
        self,
        image_url: str,
        session: aiohttp.ClientSession,
        use_cache: bool = False,
    ) -> tuple[str | None, str | None]:
        """ä¸‹è½½å¹¶ä¿å­˜å›¾åƒï¼Œå¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤å…³é—­ä»¥é¿å…è¿”å›æ—§å›¾ï¼‰"""
        cleaned_url = image_url.replace("&amp;", "&") if isinstance(image_url, str) else image_url
        parsed = urllib.parse.urlparse(cleaned_url)
        is_http = parsed.scheme in {"http", "https"}
        cache_key = (
            hashlib.sha256(cleaned_url.encode("utf-8")).hexdigest()
            if (use_cache and isinstance(cleaned_url, str))
            else None
        )

        # é’ˆå¯¹ CQ ç å›¾æœåŠ¡å™¨å¢åŠ ä¸“ç”¨è¯·æ±‚å¤´
        headers: dict[str, str] = {}
        if is_http:
            headers.update(
                {
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36"
                    ),
                    "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
                    "Accept-Language": "zh-CN,zh;q=0.9",
                    "Connection": "keep-alive",
                }
            )
            if "gchat.qpic.cn" in (parsed.netloc or ""):
                headers["Referer"] = "https://qun.qq.com"
            elif parsed.scheme and parsed.netloc:
                headers["Referer"] = f"{parsed.scheme}://{parsed.netloc}"

        # ç¼“å­˜å‘½ä¸­ç›´æ¥è¿”å›ï¼Œå‡å°‘é‡å¤ä¸‹è½½ä¸å†…å­˜å ç”¨
        if cache_key:
            try:
                IMAGE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
                cached = next(IMAGE_CACHE_DIR.glob(f"{cache_key}.*"), None)
                if cached and cached.exists() and cached.stat().st_size > 0:
                    logger.debug(f"å›¾åƒä¸‹è½½å‘½ä¸­ç¼“å­˜: {cleaned_url}")
                    return str(cached), str(cached)
            except Exception as e:
                logger.debug(f"æ£€æŸ¥å›¾åƒç¼“å­˜å¤±è´¥: {e}")

        max_retries = 3
        retry_interval = 1.0

        for attempt in range(1, max_retries + 1):
            try:
                logger.debug(
                    f"æ­£åœ¨ä¸‹è½½å›¾åƒ: {cleaned_url[:100]}... å°è¯• {attempt}/{max_retries}"
                )

                async with session.get(
                    cleaned_url,
                    timeout=aiohttp.ClientTimeout(total=30),
                    proxy=self.proxy,
                    headers=headers or None,
                ) as response:
                    if response.status != 200:
                        try:
                            err_text = await response.text()
                        except Exception:
                            err_text = ""

                        response_reason = response.reason or ""
                        response_content_type = response.headers.get(
                            "Content-Type", ""
                        )
                        query_params = urllib.parse.parse_qs(parsed.query)
                        param_issues: list[str] = []

                        # ä»…åœ¨å‡ºç° 400 é”™è¯¯æ—¶è¿›è¡Œå‚æ•°åˆæ³•æ€§æ£€æŸ¥
                        if response.status == 400:
                            appid = (query_params.get("appid") or [None])[0]
                            if appid and not re.fullmatch(r"[A-Za-z0-9]+", appid):
                                param_issues.append("appid æ ¼å¼å¼‚å¸¸ï¼ˆä»…å…è®¸å­—æ¯æ•°å­—ï¼‰")

                            fileid = (query_params.get("fileid") or [None])[0]
                            if fileid and not re.fullmatch(r"[A-Za-z0-9._-]+", fileid):
                                param_issues.append(
                                    "fileid æ ¼å¼å¼‚å¸¸ï¼ˆä»…å…è®¸å­—æ¯æ•°å­—ã€.ã€_ã€-ï¼‰"
                                )

                            rkey = (query_params.get("rkey") or [None])[0]
                            if rkey and re.search(r"[^A-Za-z0-9._-]", rkey):
                                param_issues.append("rkey åŒ…å«ç‰¹æ®Šå­—ç¬¦")

                            spec = (query_params.get("spec") or [None])[0]
                            if spec and not str(spec).isdigit():
                                param_issues.append("spec å‚æ•°åº”ä¸ºæ•°å­—")

                        # æ ¹æ®å“åº”å†…å®¹ä¸æ ¡éªŒç»“æœç»™å‡ºå»ºè®®
                        suggestions: list[str] = []
                        if " " in cleaned_url or "%20" in cleaned_url:
                            suggestions.append("URLæ ¼å¼é”™è¯¯ â†’ æ£€æŸ¥URLç¼–ç ")
                        if param_issues:
                            suggestions.append("å‚æ•°é”™è¯¯ â†’ æ£€æŸ¥å‚æ•°æ ¼å¼")
                        err_lower = err_text.lower() if err_text else ""
                        if any(keyword in err_lower for keyword in ["auth", "key"]):
                            suggestions.append("è®¤è¯é”™è¯¯ â†’ æ£€æŸ¥APIå¯†é’¥")
                        if any(keyword in err_lower for keyword in ["limit", "é¢‘ç‡", "é™åˆ¶"]):
                            suggestions.append("æœåŠ¡å™¨é™åˆ¶ â†’ å»ºè®®ç¨åé‡è¯•")
                        if not suggestions:
                            suggestions.append("æœåŠ¡å™¨é™åˆ¶ â†’ å»ºè®®ç¨åé‡è¯•")

                        logger.error(
                            "ä¸‹è½½å›¾åƒå¤±è´¥: HTTP %s %s url=%s å“åº”æ‘˜è¦=%s å»ºè®®=%s",
                            response.status,
                            response_reason,
                            cleaned_url,
                            err_text[:200],
                            "ï¼›".join(dict.fromkeys(suggestions)),
                        )

                        if self.verbose_logging:
                            logger.debug(
                                "HTTP 400 å‚æ•°æ£€æŸ¥ç»“æœ: %s",
                                "; ".join(param_issues) if param_issues else "æœªå‘ç°æ˜æ˜¾å¼‚å¸¸",
                            )
                            logger.debug("å®Œæ•´è¯·æ±‚å¤´: %s", headers or {})
                            logger.debug("User-Agent: %s", (headers or {}).get("User-Agent", ""))
                            logger.debug(
                                "Content-Type: %s, Accept: %s",
                                (headers or {}).get("Content-Type", "æœªè®¾ç½®"),
                                (headers or {}).get("Accept", "æœªè®¾ç½®"),
                            )
                            logger.debug(
                                "æœåŠ¡å™¨å“åº”è¯¦æƒ…: status=%s, reason=%s, phrase=%s, content-type=%s",
                                response.status,
                                response_reason,
                                getattr(response, "reason", ""),
                                response_content_type,
                            )
                            logger.debug(
                                "æœåŠ¡å™¨å“åº”ä½“é¢„è§ˆ: %s",
                                err_text[:1000] if err_text else "<empty>",
                            )

                        if response.status == 400 and attempt < max_retries:
                            await asyncio.sleep(retry_interval * attempt)
                            continue
                        return None, None

                    content_type = response.headers.get("Content-Type", "")

                    if "/" in content_type:
                        image_format = content_type.split("/")[1].split(";")[0] or "png"
                    else:
                        image_format = "png"

                    target_path = None
                    if cache_key:
                        target_path = IMAGE_CACHE_DIR / f"{cache_key}.{image_format}"

                    image_path = await save_image_stream(
                        response.content, image_format, target_path=target_path
                    )
                    if image_path:
                        # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œä¸ä½¿ç”¨ file:// URIï¼ˆæ ¹æ® AstrBot æ–‡æ¡£è¦æ±‚ï¼‰
                        image_url_local = image_path
                        return image_url_local, image_path
            except aiohttp.ClientError as e:
                logger.error(f"ä¸‹è½½å›¾åƒå‘ç”Ÿç½‘ç»œå¼‚å¸¸: {e}")
            except Exception as e:
                logger.error(f"ä¸‹è½½å›¾åƒå¤±è´¥: {e}")

            if attempt < max_retries:
                await asyncio.sleep(retry_interval * attempt)

        return None, None


# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºAPIClientåˆ«å
APIClient = GeminiAPIClient

# å…¨å±€ API å®¢æˆ·ç«¯å®ä¾‹
_api_client: GeminiAPIClient | None = None


def get_api_client(api_keys: list[str]) -> GeminiAPIClient:
    """è·å–æˆ–åˆ›å»º API å®¢æˆ·ç«¯å®ä¾‹"""
    global _api_client
    if _api_client is None:
        _api_client = GeminiAPIClient(api_keys)
    return _api_client


def clear_api_client():
    """æ¸…é™¤å…¨å±€ API å®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _api_client
    _api_client = None

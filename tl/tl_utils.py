"""
å·¥å…·å‡½æ•°æ¨¡å—
æä¾›å¤´åƒç®¡ç†ã€æ–‡ä»¶ä¼ è¾“å’Œå›¾åƒå¤„ç†åŠŸèƒ½
"""

import asyncio
import base64
import binascii
import hashlib
import io
import os
import re
import struct
import tempfile
import time
import urllib.parse
from collections import OrderedDict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any
from uuid import uuid4

import aiohttp
import cv2
from PIL import Image as PILImage

from astrbot.api import logger

SUPPORTED_IMAGE_MIME_TYPES = frozenset(
    {"image/png", "image/jpeg", "image/webp", "image/heic", "image/heif"}
)

# QQ åŸŸåé›†åˆï¼Œç”¨äºè¯†åˆ« QQ å›¾ç‰‡æœåŠ¡å™¨
QQ_IMAGE_HOSTS = frozenset({"qpic.cn", "qq.com", "nt.qq.com", "gchat.qpic.cn"})


def _is_qq_host(host: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸º QQ å›¾ç‰‡æœåŠ¡å™¨åŸŸå"""
    if not host:
        return False
    host_lower = host.lower()
    return any(qq_host in host_lower for qq_host in QQ_IMAGE_HOSTS)


def _build_http_headers(host: str = "", for_qq: bool = False) -> dict[str, str]:
    """
    æ„å»º HTTP è¯·æ±‚å¤´

    Args:
        host: ç›®æ ‡ä¸»æœºå
        for_qq: æ˜¯å¦ä¸º QQ å›¾ç‰‡æœåŠ¡å™¨ä¼˜åŒ–
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0"
        ),
        "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "Accept-Encoding": "gzip, deflate, br",
    }
    if host:
        scheme = "https" if not for_qq else "http"
        headers["Referer"] = f"{scheme}://{host}"

    if for_qq or _is_qq_host(host):
        headers["Referer"] = "https://qun.qq.com"
        headers["Origin"] = "https://qun.qq.com"
        if ",image/png" not in headers["Accept"]:
            headers["Accept"] += ",image/png"

    return headers


def _check_image_cache(url: str, cache_dir: Path) -> Path | None:
    """
    æ£€æŸ¥ URL å¯¹åº”çš„æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨

    Args:
        url: å›¾ç‰‡ URL
        cache_dir: ç¼“å­˜ç›®å½•

    Returns:
        ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œä¸å­˜åœ¨è¿”å› None
    """
    try:
        cache_key = hashlib.sha256(url.encode("utf-8")).hexdigest()
        cache_dir.mkdir(parents=True, exist_ok=True)
        cached = next(
            (
                p
                for p in cache_dir.glob(f"{cache_key}.*")
                if p.exists() and p.stat().st_size > 0
            ),
            None,
        )
        return cached
    except Exception as e:
        logger.debug(f"æ£€æŸ¥ç¼“å­˜å¤±è´¥: {e}")
        return None


def _save_to_cache(
    url: str, data: bytes, mime_type: str, cache_dir: Path
) -> Path | None:
    """
    å°†å›¾ç‰‡æ•°æ®ä¿å­˜åˆ°ç¼“å­˜

    Args:
        url: å›¾ç‰‡ URL
        data: å›¾ç‰‡æ•°æ®
        mime_type: MIME ç±»å‹
        cache_dir: ç¼“å­˜ç›®å½•

    Returns:
        ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        cache_key = hashlib.sha256(url.encode("utf-8")).hexdigest()
        suffix = mime_type.split("/")[-1] if "/" in mime_type else "png"
        cache_file = cache_dir / f"{cache_key}.{suffix}"
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        cache_file.write_bytes(data)
        return cache_file
    except Exception as e:
        logger.debug(f"å†™å…¥ç¼“å­˜å¤±è´¥: {e}")
        return None


def _decode_base64_to_temp_file(
    b64_data: str, *, verify_image: bool = True, logger_obj=logger
) -> str | None:
    """
    å°† base64 æ•°æ®è§£ç å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶

    æ³¨æ„ï¼šè°ƒç”¨æ–¹è´Ÿè´£æ¸…ç†è¿”å›çš„ä¸´æ—¶æ–‡ä»¶ã€‚å»ºè®®åœ¨ä½¿ç”¨å®Œæ¯•åè°ƒç”¨:
        Path(result).unlink(missing_ok=True)
    æˆ–ä½¿ç”¨ try...finally å—ç¡®ä¿æ¸…ç†ã€‚

    Args:
        b64_data: base64 æ•°æ®ï¼ˆå¯ä»¥æ˜¯ data URI æˆ–çº¯ base64ï¼‰
        verify_image: æ˜¯å¦éªŒè¯å›¾ç‰‡å¯è¯»æ€§
        logger_obj: æ—¥å¿—å¯¹è±¡

    Returns:
        ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› Noneã€‚è°ƒç”¨æ–¹è´Ÿè´£æ¸…ç†æ­¤æ–‡ä»¶ã€‚
    """
    try:
        data = b64_data
        if ";base64," in data:
            _, _, data = data.partition(";base64,")
        raw_bytes = base64.b64decode(data)
        tmp_file = tempfile.NamedTemporaryFile(
            prefix=f"cut_{int(time.time() * 1000)}_",
            suffix=".png",
            delete=False,
        )
        tmp_path = Path(tmp_file.name)
        tmp_file.close()
        tmp_path.write_bytes(raw_bytes)

        if verify_image and cv2.imread(str(tmp_path)) is None:
            logger_obj.debug("base64 è§£ç åå›¾ç‰‡ä¸å¯è¯»")
            tmp_path.unlink(missing_ok=True)
            return None

        return str(tmp_path)
    except Exception as e:
        logger_obj.debug(f"base64 è§£ç åˆ°ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        return None


def get_plugin_data_dir() -> Path:
    """è·å–æ’ä»¶æ•°æ®ç›®å½•"""
    # ä½¿ç”¨AstrBotçš„StarToolsè·å–æ ‡å‡†æ•°æ®ç›®å½•
    from astrbot.api.star import StarTools

    return StarTools.get_data_dir("astrbot_plugin_gemini_image_generation")


# ä¸‹è½½ç¼“å­˜ç›®å½•
IMAGE_CACHE_DIR = get_plugin_data_dir() / "images" / "download_cache"


def _build_image_path(
    image_format: str = "png", prefix: str = "gemini_advanced_image"
) -> Path:
    """ç”Ÿæˆè§„èŒƒçš„å›¾ç‰‡è·¯å¾„ï¼Œé¿å…é‡å¤é€»è¾‘"""
    images_dir = get_plugin_data_dir() / "images"
    images_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
    unique_suffix = uuid4().hex[:6]
    filename = f"{prefix}_{timestamp}_{unique_suffix}.{image_format}"
    return images_dir / filename


def _pick_avatar_url(data: dict | None) -> str | None:
    """å°è¯•ä»ä¸åŒå­—æ®µä¸­æå–å¤´åƒURL"""
    if not isinstance(data, dict):
        return None

    candidate_keys = [
        "avatar",
        "avatar_url",
        "user_avatar",
        "head_image",
        "tiny_avatar",
        "thumb_avatar",
        "url",
    ]
    for key in candidate_keys:
        url = data.get(key)
        if isinstance(url, str) and url.startswith(("http://", "https://")):
            return url

    inner = data.get("data")
    if isinstance(inner, dict):
        return _pick_avatar_url(inner)

    return None


def _encode_file_to_base64(file_path: Path, chunk_size: int = 65536) -> str:
    """æµå¼ç¼–ç æ–‡ä»¶ä¸ºbase64
    æ³¨æ„: chunk_size å¿…é¡»æ˜¯ 3 çš„å€æ•°ï¼Œå¦åˆ™ base64 ç¼–ç ä¼šå‡ºé”™
    """
    # ç¡®ä¿ chunk_size æ˜¯ 3 çš„å€æ•°
    chunk_size = (chunk_size // 3) * 3
    if chunk_size == 0:
        chunk_size = 3

    encoded_parts: list[str] = []
    with open(file_path, "rb") as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            encoded_parts.append(base64.b64encode(chunk).decode("utf-8"))
    return "".join(encoded_parts)


def encode_file_to_base64(file_path: str | Path, chunk_size: int = 65536) -> str:
    """å¯¹å¤–æš´éœ²çš„ç¼–ç æ–¹æ³•ï¼Œå…¼å®¹å­—ç¬¦ä¸²è·¯å¾„"""
    return _encode_file_to_base64(Path(file_path), chunk_size)


async def save_image_stream(
    stream_reader,
    image_format: str = "png",
    target_path: Path | None = None,
) -> str | None:
    """
    å°†å¼‚æ­¥æµå¼è¯»å–åˆ°çš„å›¾ç‰‡ä¿å­˜åˆ°æ–‡ä»¶ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜

    Args:
        stream_reader: aiohttp.StreamReader æˆ–ä»»æ„å¼‚æ­¥å¯è¿­ä»£çš„å­—èŠ‚æµ
        image_format: å›¾ç‰‡æ ¼å¼
        target_path: æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼Œä¾¿äºç¼“å­˜å¤ç”¨
    """
    file_path = target_path or _build_image_path(image_format)
    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        with open(file_path, "wb") as f:
            if hasattr(stream_reader, "iter_chunked"):
                async for chunk in stream_reader.iter_chunked(8192):
                    if chunk:
                        f.write(chunk)
            else:
                async for chunk in stream_reader:
                    if chunk:
                        f.write(chunk)

        logger.debug(f"å›¾åƒå·²æµå¼ä¿å­˜: {file_path}")
        return str(file_path)
    except Exception as e:
        logger.error(f"æµå¼ä¿å­˜å›¾åƒå¤±è´¥: {e}")
        return None


# base64 å›¾ç‰‡å»é‡ç¼“å­˜ï¼ˆbase64_hash -> file_pathï¼‰ï¼Œä½¿ç”¨ LRU ç­–ç•¥é™åˆ¶å¤§å°
_BASE64_CACHE_MAX_SIZE = 128  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°


class _LRUCache(OrderedDict):
    """ç®€å•çš„ LRU ç¼“å­˜å®ç°ï¼Œç»§æ‰¿ OrderedDict"""

    def __init__(self, maxsize: int = 128):
        super().__init__()
        if maxsize < 0:
            raise ValueError(f"maxsize å¿…é¡»ä¸ºéè´Ÿæ•´æ•°ï¼Œå½“å‰å€¼ä¸º: {maxsize}")
        self.maxsize = maxsize

    def get(self, key: str, default: str | None = None) -> str | None:
        if key in self:
            self.move_to_end(key)
            return self[key]
        return default

    def set(self, key: str, value: str) -> None:
        if key in self:
            self.move_to_end(key)
        self[key] = value
        while len(self) > self.maxsize:
            self.popitem(last=False)

    def __contains__(self, key: object) -> bool:
        return super().__contains__(key)


_base64_image_cache: _LRUCache = _LRUCache(maxsize=_BASE64_CACHE_MAX_SIZE)


async def save_base64_image(base64_data: str, image_format: str = "png") -> str | None:
    """
    ä¿å­˜base64å›¾åƒæ•°æ®åˆ°æ–‡ä»¶

    Args:
        base64_data: base64ç¼–ç çš„å›¾åƒæ•°æ®
        image_format: å›¾åƒæ ¼å¼ (png, jpg, jpegç­‰)

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›Noneï¼›è‹¥å·²ä¿å­˜è¿‡ç›¸åŒæ•°æ®åˆ™è¿”å›ç°æœ‰è·¯å¾„
    """

    # å»æ‰ç©ºç™½åè®¡ç®—å“ˆå¸Œï¼Œç”¨äºå»é‡
    cleaned_data = "".join(base64_data.split())
    data_hash = hashlib.md5(cleaned_data.encode()).hexdigest()

    # æ£€æŸ¥æ˜¯å¦å·²ä¿å­˜è¿‡ç›¸åŒçš„æ•°æ®
    existing_path = _base64_image_cache.get(data_hash)
    if existing_path:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿˜å­˜åœ¨
        if Path(existing_path).exists():
            logger.debug(f"å¤ç”¨å·²ä¿å­˜çš„å›¾ç‰‡: {existing_path}")
            return existing_path
        else:
            # æ–‡ä»¶å·²è¢«åˆ é™¤ï¼Œä»ç¼“å­˜ä¸­ç§»é™¤
            del _base64_image_cache[data_hash]

    try:
        file_path = _build_image_path(image_format)

        # å»æ‰ç©ºç™½å¹¶æŒ‰å—è§£ç ï¼Œé¿å…ä¸€æ¬¡æ€§å ç”¨è¿‡å¤§å†…å­˜
        # ä¸€æ¬¡æ€§è§£ç å®Œæ•´æ•°æ®ï¼Œè‹¥å¤±è´¥åˆ™å®½æ¾æ¸…æ´—åå†è¯•
        try:
            raw = base64.b64decode(cleaned_data, validate=False)
        except Exception:
            import re

            relaxed = re.sub(r"[^A-Za-z0-9+/=_-]", "", cleaned_data)
            pad_len = (-len(relaxed)) % 4
            if pad_len:
                relaxed += "=" * pad_len
            raw = base64.b64decode(relaxed, validate=False)

        with open(file_path, "wb") as f:
            f.write(raw)

        # åŠ å…¥ç¼“å­˜ï¼Œé¿å…é‡å¤ä¿å­˜ç›¸åŒæ•°æ®ï¼ˆä½¿ç”¨ LRU ç­–ç•¥ï¼‰
        _base64_image_cache.set(data_hash, str(file_path))

        logger.debug(f"å›¾åƒå·²ä¿å­˜: {file_path}")
        return str(file_path)

    except Exception as e:
        logger.error(f"ä¿å­˜å›¾åƒå¤±è´¥: {e}")
        return None


async def save_image_data(image_data: bytes, image_format: str = "png") -> str | None:
    """
    ä¿å­˜å›¾åƒå­—èŠ‚æ•°æ®åˆ°æ–‡ä»¶

    Args:
        image_data: å›¾åƒå­—èŠ‚æ•°æ®
        image_format: å›¾åƒæ ¼å¼

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        file_path = _build_image_path(image_format)
        with open(file_path, "wb") as f:
            f.write(image_data)

        logger.debug(f"å›¾åƒå·²ä¿å­˜: {file_path}")
        return str(file_path)

    except Exception as e:
        logger.error(f"ä¿å­˜å›¾åƒå¤±è´¥: {e}")
        return None


async def cleanup_old_images(
    images_dir: Path | None = None, ttl_minutes: int = 5, max_files: int = 100
):
    """
    æ¸…ç†è¶…è¿‡æŒ‡å®šæ—¶é—´çš„å›¾åƒæ–‡ä»¶å’Œç¼“å­˜ï¼Œå¹¶é™åˆ¶æ–‡ä»¶æ•°é‡

    Args:
        images_dir (Path): images ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        ttl_minutes (int): ç¼“å­˜ä¿ç•™æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤ 5 åˆ†é’Ÿï¼Œè®¾ä¸º 0 è¡¨ç¤ºä¸æŒ‰æ—¶é—´æ¸…ç†
        max_files (int): å„ç›®å½•æ–‡ä»¶æ•°é‡ä¸Šé™ï¼Œé»˜è®¤ 100ï¼Œè®¾ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶æ•°é‡
    """
    # å¦‚æœ TTL å’Œæ•°é‡é™åˆ¶éƒ½ä¸º 0ï¼Œä¸æ‰§è¡Œæ¸…ç†
    if ttl_minutes <= 0 and max_files <= 0:
        return

    try:
        plugin_data_dir = get_plugin_data_dir()
        if images_dir is None:
            images_dir = plugin_data_dir / "images"

        current_time = datetime.now()
        cutoff_time = current_time - timedelta(minutes=ttl_minutes)
        cleaned_count = 0

        # æ¸…ç† images ç›®å½•
        if images_dir.exists():
            image_patterns = [
                "gemini_image_*.*",
                "gemini_advanced_image_*.*",
                "help_*.png",
            ]
            all_files: list[Path] = []
            for pattern in image_patterns:
                all_files.extend(images_dir.glob(pattern))

            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ—§çš„åœ¨å‰é¢ï¼‰
            all_files.sort(key=lambda f: f.stat().st_mtime)

            for idx, file_path in enumerate(all_files):
                try:
                    should_delete = False
                    # æŒ‰æ—¶é—´æ¸…ç†
                    if ttl_minutes > 0:
                        if (
                            datetime.fromtimestamp(file_path.stat().st_mtime)
                            < cutoff_time
                        ):
                            should_delete = True
                    # æŒ‰æ•°é‡æ¸…ç†ï¼ˆä¿ç•™æœ€æ–°çš„ max_files ä¸ªï¼‰
                    if max_files > 0 and len(all_files) - idx > max_files:
                        should_delete = True

                    if should_delete:
                        file_path.unlink()
                        cleaned_count += 1
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

        # æ¸…ç† download_cache ç›®å½•
        cache_dir = (
            images_dir / "download_cache"
            if images_dir
            else plugin_data_dir / "images" / "download_cache"
        )
        if cache_dir.exists():
            cache_files = [f for f in cache_dir.glob("*") if f.is_file()]
            cache_files.sort(key=lambda f: f.stat().st_mtime)

            for idx, file_path in enumerate(cache_files):
                try:
                    should_delete = False
                    if ttl_minutes > 0:
                        if (
                            datetime.fromtimestamp(file_path.stat().st_mtime)
                            < cutoff_time
                        ):
                            should_delete = True
                    if max_files > 0 and len(cache_files) - idx > max_files:
                        should_delete = True

                    if should_delete:
                        file_path.unlink()
                        cleaned_count += 1
                except Exception as e:
                    logger.warning(f"æ¸…ç†ç¼“å­˜ {file_path} æ—¶å‡ºé”™: {e}")

        # æ¸…ç† split_output ç›®å½•
        split_dir = plugin_data_dir / "split_output"
        if split_dir.exists():
            split_files = [f for f in split_dir.glob("*") if f.is_file()]
            split_files.sort(key=lambda f: f.stat().st_mtime)

            for idx, file_path in enumerate(split_files):
                try:
                    should_delete = False
                    if ttl_minutes > 0:
                        if (
                            datetime.fromtimestamp(file_path.stat().st_mtime)
                            < cutoff_time
                        ):
                            should_delete = True
                    if max_files > 0 and len(split_files) - idx > max_files:
                        should_delete = True

                    if should_delete:
                        file_path.unlink()
                        cleaned_count += 1
                except Exception as e:
                    logger.warning(f"æ¸…ç†åˆ‡å›¾ {file_path} æ—¶å‡ºé”™: {e}")

        if cleaned_count > 0:
            logger.debug(f"å…±æ¸…ç† {cleaned_count} ä¸ªè¿‡æœŸæ–‡ä»¶")

    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")


async def download_qq_avatar(
    user_id: str,
    cache_name: str,
    images_dir: Path | None = None,
    event=None,
) -> str | None:
    """
    ä¸‹è½½QQå¤´åƒå¹¶è½¬æ¢ä¸ºbase64æ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨NapCatäº‹ä»¶ç³»ç»Ÿè·å–å¤´åƒURL

    Args:
        user_id (str): QQç”¨æˆ·ID
        cache_name (str): ç¼“å­˜æ–‡ä»¶åå‰ç¼€
        images_dir (Path): imagesç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        event: AstrMessageEventï¼Œä¾¿äºé€šè¿‡äº‹ä»¶æºå¸¦çš„ bot/åŸå§‹æ¶ˆæ¯æå–å¤´åƒURL

    Returns:
        str: base64æ ¼å¼çš„å¤´åƒæ•°æ®ï¼Œå¤±è´¥è¿”å›None
    """

    async def _resolve_avatar_url() -> str | None:
        """é€šè¿‡äº‹ä»¶ä¸Šä¸‹æ–‡æˆ–NapCatæ¥å£è§£æå¤´åƒURL"""
        # 1. åŸå§‹æ¶ˆæ¯
        try:
            raw_msg = getattr(getattr(event, "message_obj", None), "raw_message", None)
            sender_raw = getattr(raw_msg, "sender", None) or (
                raw_msg.get("sender") if isinstance(raw_msg, dict) else None
            )
            url = _pick_avatar_url(sender_raw)
            if url:
                logger.debug(f"ä»åŸå§‹æ¶ˆæ¯è·å–åˆ°å¤´åƒURL: {url}")
                return url
        except Exception as e:
            logger.debug(f"ä»åŸå§‹æ¶ˆæ¯æå–å¤´åƒå¤±è´¥: {e}")

        # 2. sender å¯¹è±¡
        try:
            sender_obj = getattr(getattr(event, "message_obj", None), "sender", None)
            url = _pick_avatar_url(sender_obj.__dict__ if sender_obj else None)
            if url:
                logger.debug("ä» sender å¯¹è±¡æå–åˆ°å¤´åƒURL")
                return url
        except Exception:
            pass

        # 3. NapCat / OneBot API
        bot = getattr(event, "bot", None) or getattr(event, "_bot", None)
        if bot:
            group_id = None
            try:
                raw_group_id = (
                    getattr(event, "group_id", None)
                    or getattr(event, "get_group_id", lambda: None)()
                )
                group_id = int(raw_group_id) if raw_group_id else None
            except Exception:
                group_id = None

            actions: list[tuple[str, dict]] = []
            actions.append(("get_avatar", {"user_id": int(user_id), "img_type": "640"}))
            actions.append(
                ("get_user_avatar", {"user_id": int(user_id), "img_type": "640"})
            )
            if group_id:
                actions.append(
                    (
                        "get_group_member_info",
                        {
                            "group_id": group_id,
                            "user_id": int(user_id),
                            "no_cache": False,
                        },
                    )
                )
            actions.append(
                ("get_stranger_info", {"user_id": int(user_id), "no_cache": False})
            )

            napcat_unsupported = False

            for action, payload in actions:
                if napcat_unsupported:
                    break
                try:
                    resp = await bot.call_action(action, **payload)
                    url = None
                    if isinstance(resp, str) and resp.startswith(
                        ("http://", "https://")
                    ):
                        url = resp
                    elif isinstance(resp, dict):
                        url = _pick_avatar_url(resp)
                    if url:
                        logger.debug(f"é€šè¿‡ {action} è·å–å¤´åƒURLæˆåŠŸ: {url}")
                        return url
                except Exception as e:
                    err_text = str(e)
                    logger.debug(f"è°ƒç”¨ {action} è·å–å¤´åƒå¤±è´¥: {err_text}")
                    # å¯¹äºä¸æ”¯æŒçš„æ¥å£ï¼Œé¿å…ç»§ç»­å°è¯•å…¶ä»– NapCat APIï¼Œç›´æ¥èµ°ç›´é“¾å›é€€
                    if getattr(e, "retcode", None) == 1404 or "ä¸æ”¯æŒçš„Api" in err_text:
                        napcat_unsupported = True

        logger.warning(f"æ— æ³•é€šè¿‡äº‹ä»¶ç³»ç»Ÿè·å–ç”¨æˆ· {user_id} çš„å¤´åƒURL")
        return None

    try:
        if images_dir is None:
            images_dir = get_plugin_data_dir() / "images"

        avatar_url = await _resolve_avatar_url()
        if not avatar_url:
            # å›é€€ä½¿ç”¨ qlogo ç›´é“¾ï¼ˆä½¿ç”¨ HTTP åè®®ï¼Œæ›´ç¨³å®šï¼‰
            avatar_url = f"http://q4.qlogo.cn/headimg_dl?dst_uin={user_id}&spec=640"
            logger.debug(f"æœªä»äº‹ä»¶è·å–å¤´åƒURLï¼Œå›é€€ qlogo: {avatar_url}")
        else:
            # å°†è·å–åˆ°çš„ URL è½¬ä¸º HTTP åè®®
            avatar_url = avatar_url.replace("https://", "http://")

        timeout = aiohttp.ClientTimeout(total=12, connect=5)
        max_retries = 3
        retry_interval = 1.0
        async with aiohttp.ClientSession() as session:
            for attempt in range(1, max_retries + 1):
                try:
                    async with session.get(
                        avatar_url,
                        timeout=timeout,
                    ) as response:
                        if response.status != 200:
                            logger.warning(
                                f"ä¸‹è½½å¤´åƒå¤±è´¥: HTTP {response.status} "
                                f"{response.reason} (å°è¯• {attempt}/{max_retries})"
                            )
                            if attempt < max_retries:
                                await asyncio.sleep(retry_interval * attempt)
                                continue
                            return None

                        data = await response.read()
                        if not data or len(data) <= 1000:
                            logger.warning(
                                f"ç”¨æˆ· {user_id} å¤´åƒå¯èƒ½ä¸ºç©ºæˆ–é»˜è®¤å¤´åƒï¼Œæ–‡ä»¶è¿‡å°ï¼Œæ”¾å¼ƒ"
                            )
                            return None

                        # å°è¯•ä»å“åº”å¤´çŒœæµ‹ mime
                        mime_type = (
                            (response.headers.get("Content-Type") or "")
                            .split(";")[0]
                            .lower()
                        )
                        if not mime_type or "/" not in mime_type:
                            mime_type = "image/jpeg"

                        encoded = base64.b64encode(data).decode()
                        base64_data = f"data:{mime_type};base64,{encoded}"
                        logger.debug(f"å¤´åƒä¸‹è½½æˆåŠŸ(ä»…å†…å­˜ä½¿ç”¨): {cache_name}")
                        return base64_data
                except Exception as e:
                    logger.warning(f"ä¸‹è½½å¤´åƒå¼‚å¸¸: {e} (å°è¯• {attempt}/{max_retries})")
                    if attempt < max_retries:
                        await asyncio.sleep(retry_interval * attempt)
                        continue
                    return None

    except Exception as e:
        logger.error(f"ä¸‹è½½å¤´åƒ {cache_name} å¤±è´¥: {e}")
        return None


async def send_file(filename: str, host: str, port: int):
    """
    å‘é€æ–‡ä»¶åˆ°è¿œç¨‹æœåŠ¡å™¨

    Args:
        filename: è¦å‘é€çš„æ–‡ä»¶è·¯å¾„
        host: è¿œç¨‹ä¸»æœºåœ°å€
        port: è¿œç¨‹ä¸»æœºç«¯å£

    Returns:
        str: è¿œç¨‹æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    reader = None
    writer = None

    async def recv_all(reader, size):
        """æ¥æ”¶æŒ‡å®šå¤§å°çš„æ•°æ®"""
        data = b""
        while len(data) < size:
            chunk = await reader.read(size - len(data))
            if not chunk:
                break
            data += chunk
        return data

    try:
        # æ·»åŠ è¿æ¥è¶…æ—¶æ§åˆ¶
        reader, writer = await asyncio.wait_for(
            asyncio.open_connection(host, port),
            timeout=5.0,  # 5ç§’è¿æ¥è¶…æ—¶
        )

        file_name = os.path.basename(filename)
        file_name_bytes = file_name.encode("utf-8")

        # å‘é€æ–‡ä»¶åé•¿åº¦å’Œæ–‡ä»¶å
        writer.write(struct.pack(">I", len(file_name_bytes)))
        writer.write(file_name_bytes)

        # å‘é€æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(filename)
        writer.write(struct.pack(">Q", file_size))

        # å‘é€æ–‡ä»¶å†…å®¹ï¼Œæ·»åŠ æ€»ä½“è¶…æ—¶æ§åˆ¶
        await writer.drain()
        with open(filename, "rb") as f:
            while True:
                data = f.read(4096)
                if not data:
                    break
                writer.write(data)
                await writer.drain()

        logger.debug(f"æ–‡ä»¶ {file_name} å‘é€æˆåŠŸ")

        # æ¥æ”¶æ¥æ”¶ç«¯å‘é€çš„æ–‡ä»¶ç»å¯¹è·¯å¾„
        try:
            file_abs_path_len_data = await recv_all(reader, 4)
            if not file_abs_path_len_data:
                logger.error("æ— æ³•æ¥æ”¶æ–‡ä»¶ç»å¯¹è·¯å¾„é•¿åº¦")
                return None
            file_abs_path_len = struct.unpack(">I", file_abs_path_len_data)[0]

            file_abs_path_data = await recv_all(reader, file_abs_path_len)
            if not file_abs_path_data:
                logger.error("æ— æ³•æ¥æ”¶æ–‡ä»¶ç»å¯¹è·¯å¾„")
                return None

            file_abs_path = file_abs_path_data.decode("utf-8")
            logger.debug(f"æ–‡ä»¶åœ¨è¿œç¨‹æœåŠ¡å™¨ä¿å­˜ä¸º: {file_abs_path}")
            return file_abs_path

        except Exception as e:
            logger.error(f"æ¥æ”¶è¿œç¨‹æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            return None

    except asyncio.TimeoutError:
        logger.error(f"è¿æ¥ {host}:{port} è¶…æ—¶")
        return None
    except Exception as e:
        logger.error(f"å‘é€æ–‡ä»¶å¤±è´¥: {e}")
        return None
    finally:
        if writer:
            try:
                writer.close()
                await writer.wait_closed()
            except Exception:
                pass
        if reader:
            try:
                reader.close()
            except Exception:
                pass


def is_valid_base64_image_str(value: str) -> bool:
    """ç²—ç•¥åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ base64 å›¾åƒæ•°æ®æˆ– data URL"""
    if not value:
        return False

    def _looks_like_image(raw: bytes) -> bool:
        """é€šè¿‡é­”æ•°å¿«é€Ÿåˆ¤æ–­æ˜¯å¦ä¸ºå¸¸è§å›¾ç‰‡æ ¼å¼"""
        if not raw or len(raw) < 4:
            return False
        if raw.startswith(b"\x89PNG\r\n\x1a\n"):
            return True
        if raw.startswith(b"\xff\xd8"):  # JPEG
            return True
        if raw.startswith(b"RIFF") and len(raw) >= 12 and raw[8:12] == b"WEBP":
            return True
        if raw.startswith((b"GIF87a", b"GIF89a")):  # GIF
            return True
        if len(raw) >= 12 and raw[4:12] in {
            b"ftypheic",
            b"ftypheif",
            b"ftypmif1",
            b"ftypmsf1",
            b"ftyphevc",
        }:
            return True
        return False

    cleaned = value.strip()
    if not cleaned:
        return False
    # æ˜ç¡®åŒ…å« URL ç‰¹å¾æ—¶ç›´æ¥æ’é™¤ï¼Œé¿å…è¯¯åˆ¤
    if "://" in cleaned:
        return False

    if cleaned.startswith("data:image/"):
        _, _, cleaned = cleaned.partition(";base64,")
    try:
        raw = base64.b64decode(cleaned, validate=True)
    except Exception:
        return False

    return _looks_like_image(raw)


async def collect_image_sources(event, log_debug=logger.debug) -> list[str]:
    """
    ä»æ¶ˆæ¯/å¼•ç”¨/åˆå¹¶è½¬å‘/ç¾¤æ–‡ä»¶ä¸­æ”¶é›†å›¾ç‰‡æº

    Args:
        event: AstrMessageEvent
        log_debug: æ—¥å¿—å‡½æ•°
    """
    sources: list[str] = []
    seen: set[str] = set()

    def add_source(val: str, origin: str):
        if not val:
            return
        if val in seen:
            return
        seen.add(val)
        sources.append(val)
        log_debug(f"âœ“ æ”¶é›†å›¾ç‰‡æº({origin}): {str(val)[:80]}")

    def extract_from_components(components, origin: str):
        if not components:
            return
        for comp in components:
            try:
                # å›¾ç‰‡ç»„ä»¶
                if comp.__class__.__name__ == "Image":
                    if getattr(comp, "url", None):
                        add_source(comp.url, origin)
                    elif getattr(comp, "file", None):
                        add_source(comp.file, origin)
                    continue

                # æ–‡ä»¶ç»„ä»¶ï¼ˆå°è¯•æŒ‰å›¾ç‰‡å¤„ç†ï¼‰
                if comp.__class__.__name__ == "File":
                    file_val = getattr(comp, "file", None) or getattr(comp, "url", None)
                    add_source(file_val, origin)
                    continue

                # å¼•ç”¨æ¶ˆæ¯
                if comp.__class__.__name__ == "Reply" and getattr(comp, "chain", None):
                    extract_from_components(comp.chain, "å¼•ç”¨æ¶ˆæ¯")
                    continue

                # åˆå¹¶è½¬å‘èŠ‚ç‚¹
                if comp.__class__.__name__ == "Node":
                    node_content = getattr(comp, "content", None)
                    extract_from_components(node_content, "åˆå¹¶è½¬å‘")
                    continue

                # Nodesï¼ˆå¤šä¸ªèŠ‚ç‚¹ï¼‰
                if comp.__class__.__name__ == "Nodes":
                    nodes = getattr(comp, "nodes", None) or getattr(comp, "list", None)
                    extract_from_components(nodes, "åˆå¹¶è½¬å‘")
                    continue
            except Exception as e:
                log_debug(f"æå–å›¾ç‰‡æºå¼‚å¸¸: {e}")

    try:
        message_chain = event.get_messages()
    except Exception:
        message_chain = getattr(event.message_obj, "message", []) or []

    extract_from_components(message_chain, "å½“å‰æ¶ˆæ¯")

    return sources


def coerce_supported_image_bytes(
    mime_type: str | None, raw_bytes: bytes
) -> tuple[str | None, str | None]:
    """
    å°†è¾“å…¥å›¾ç‰‡è½¬æ¢ä¸º Gemini æ”¯æŒçš„ MIMEã€‚
    - æ”¯æŒ: PNG/JPEG/WEBP/HEIC/HEIF
    - ä¸æ”¯æŒçš„æ ¼å¼å°è¯•ç”¨ Pillow è½¬ä¸º PNG
    """
    normalized_mime = (mime_type or "").lower()
    target_mime = (
        normalized_mime
        if normalized_mime in SUPPORTED_IMAGE_MIME_TYPES
        else "image/png"
    )
    try:
        with PILImage.open(io.BytesIO(raw_bytes)) as img:
            if target_mime == "image/png":
                save_format = "PNG"
                if img.mode not in ("RGB", "RGBA"):
                    img = img.convert("RGBA")
            elif target_mime == "image/jpeg":
                save_format = "JPEG"
                if img.mode not in ("RGB", "L"):
                    img = img.convert("RGB")
            elif target_mime == "image/webp":
                save_format = "WEBP"
                if img.mode not in ("RGB", "RGBA"):
                    img = img.convert("RGBA")
            else:
                save_format = "PNG"
                target_mime = "image/png"
                if img.mode not in ("RGB", "RGBA"):
                    img = img.convert("RGBA")

            buffer = io.BytesIO()
            img.save(buffer, format=save_format)
            buffer.seek(0)
            encoded = base64.b64encode(buffer.read()).decode("utf-8")
            return target_mime, encoded
    except Exception as e:
        logger.warning(f"å‚è€ƒå›¾æ ¼å¼ä¸å—æ”¯æŒä¸”è½¬æ¢å¤±è´¥: mime={mime_type}, err={e}")
        return None, None


def coerce_supported_image(
    mime_type: str | None, base64_data: str
) -> tuple[str | None, str | None]:
    """å…¼å®¹æ—§è°ƒç”¨ï¼šå…ˆå°è¯•è§£ç  base64ï¼Œå†è°ƒç”¨å­—èŠ‚çº§è½¬æ¢"""
    try:
        raw = base64.b64decode(base64_data, validate=False)
    except Exception as e:
        logger.warning(f"base64 è§£ç å¤±è´¥ï¼Œæ— æ³•è½¬æ¢ä¸ºå—æ”¯æŒæ ¼å¼: {e}")
        return None, None
    return coerce_supported_image_bytes(mime_type, raw)


async def normalize_image_input(
    image_input: Any,
    *,
    image_cache_dir: Path | None = None,
    image_input_mode: str = "force_base64",
) -> tuple[str | None, str | None]:
    """
    å°†å‚è€ƒå›¾åƒè¾“å…¥è§„èŒƒåŒ–ä¸º (mime_type, base64_data)ã€‚
    æ”¯æŒ data URIã€çº¯/å®½æ¾ base64 å­—ç¬¦ä¸²ã€æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€file://ã€http/https URLã€‚
    """
    try:
        if image_input is None:
            return None, None

        image_str = str(image_input).strip()
        # è‹¥æ˜¯ Markdown å›¾ç‰‡è¯­æ³•ï¼Œå…ˆæå–æ‹¬å·é‡Œçš„ URL/data URI
        md_match = re.search(r"!\[[^\]]*\]\(\s*([^)]+)\s*\)", image_str)
        if md_match:
            image_str = md_match.group(1).strip()

        if "&amp;" in image_str:
            image_str = image_str.replace("&amp;", "&")
        if not image_str:
            return None, None

        cache_dir = image_cache_dir or IMAGE_CACHE_DIR
        logger.debug(
            f"è§„èŒƒåŒ–å‚è€ƒå›¾è¾“å…¥: len={len(image_str)} "
            f"type={type(image_input)} mode={image_input_mode}"
        )

        # data URI
        if image_str.startswith("data:image/") and ";base64," in image_str:
            header, data = image_str.split(";base64,", 1)
            mime_type = header.replace("data:", "")
            logger.debug(f"æ£€æµ‹åˆ° data URIï¼Œmime={mime_type}")
            try:
                raw = base64.b64decode(data, validate=False)
            except Exception:
                logger.warning("data URL base64 è§£ç å¤±è´¥")
                return None, None
            return coerce_supported_image_bytes(mime_type, raw)

        # file:// è·¯å¾„
        if image_str.startswith("file://"):
            parsed = urllib.parse.urlparse(image_str)
            image_path = Path(parsed.path)
            if image_path.exists() and image_path.is_file():
                suffix = image_path.suffix.lower().lstrip(".") or "png"
                mime_type = f"image/{suffix}"
                logger.debug(f"ä½¿ç”¨ file:// è·¯å¾„: {image_path}")
                try:
                    data_bytes = image_path.read_bytes()
                    return coerce_supported_image_bytes(mime_type, data_bytes)
                except Exception as e:
                    logger.warning(f"è¯»å– file:// è·¯å¾„å¤±è´¥: {e}")
            else:
                logger.warning(f"file:// è·¯å¾„ä¸å­˜åœ¨: {image_str}")

        # http(s) URL -> ä¸‹è½½å¹¶è½¬base64ï¼ˆå¸¦é‡è¯•å’Œè¯¦ç»†æ—¥å¿—ï¼‰
        if image_str.startswith("http://") or image_str.startswith("https://"):
            cleaned_url = image_str.replace("&amp;", "&")
            parsed_url = urllib.parse.urlparse(cleaned_url)
            parsed_host = parsed_url.netloc or ""
            logger.debug(f"ä¸‹è½½ http(s) å‚è€ƒå›¾: {cleaned_url}")

            # ç¼“å­˜å‘½ä¸­ç›´æ¥è¯»å–
            cached = _check_image_cache(cleaned_url, cache_dir)
            if cached:
                mime_guess = f"image/{cached.suffix.lstrip('.') or 'png'}"
                data = encode_file_to_base64(cached)
                logger.debug(f"å‚è€ƒå›¾å‘½ä¸­ç¼“å­˜: {cleaned_url}")
                return mime_guess, data

            # ä½¿ç”¨ç»Ÿä¸€çš„è¯·æ±‚å¤´æ„å»º
            is_qq = _is_qq_host(parsed_host)
            headers = _build_http_headers(parsed_host, for_qq=is_qq)

            timeout = aiohttp.ClientTimeout(total=20, connect=10)
            trust_env = not is_qq

            async with aiohttp.ClientSession(
                timeout=timeout, trust_env=trust_env
            ) as session:
                fallback_reason = None

                try:
                    async with session.get(cleaned_url, headers=headers) as resp:
                        if resp.status == 200:
                            content_type = resp.headers.get("Content-Type", "image/png")
                            mime_type = (
                                content_type.split(";")[0]
                                if content_type
                                else "image/png"
                            )
                            try:
                                data_bytes = await resp.read()
                            except Exception as e:
                                fallback_reason = f"è¯»å–å“åº”ä½“å¤±è´¥: {e}"
                            else:
                                # ç¼“å­˜åˆ°æœ¬åœ°æ–‡ä»¶
                                _save_to_cache(
                                    cleaned_url, data_bytes, mime_type, cache_dir
                                )
                                return coerce_supported_image_bytes(
                                    mime_type, data_bytes
                                )
                        else:
                            fallback_reason = f"HTTP {resp.status} {resp.reason}"
                except Exception as e:
                    fallback_reason = str(e)
                    logger.warning(
                        f"ä¸‹è½½å‚è€ƒå›¾å¤±è´¥: {cleaned_url}ï¼ŒåŸå› : {fallback_reason}"
                    )

        # çº¯ base64ï¼ˆå®½æ¾æ ¡éªŒï¼‰ï¼Œå°è¯•è‡ªåŠ¨è¡¥é½/è¿‡æ»¤éæ³•å­—ç¬¦
        logger.debug("å°è¯•å°†è¾“å…¥è§†ä¸ºçº¯ base64")
        try:
            base64.b64decode(image_str, validate=False)
            return coerce_supported_image(None, image_str)
        except binascii.Error:
            cleaned = re.sub(r"[^A-Za-z0-9+/=_-]", "", image_str)
            pad_len = (-len(cleaned)) % 4
            if pad_len:
                cleaned += "=" * pad_len
            try:
                base64.b64decode(cleaned, validate=False)
                return coerce_supported_image(None, cleaned)
            except Exception:
                return None, None

    except Exception as e:
        logger.error(f"è§„èŒƒåŒ–å‚è€ƒå›¾è¾“å…¥å¤±è´¥: {e}")
        return None, None


async def resolve_image_source_to_path(
    source: str,
    *,
    image_input_mode: str = "force_base64",
    api_client=None,
    download_qq_image_fn=None,
    event=None,
    is_valid_checker=is_valid_base64_image_str,
    logger_obj=logger,
) -> str | None:
    """
    å°†å›¾ç‰‡æºè½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„ä»¥ä¾¿åˆ‡å‰²

    æ³¨æ„ï¼šå½“è¿”å›çš„è·¯å¾„æ˜¯ä¸´æ—¶æ–‡ä»¶ï¼ˆé€šè¿‡ base64/URL è½¬æ¢ç”Ÿæˆï¼‰æ—¶ï¼Œ
    è°ƒç”¨æ–¹è´Ÿè´£åœ¨ä½¿ç”¨å®Œæ¯•åæ¸…ç†è¯¥æ–‡ä»¶ã€‚å»ºè®®ä½¿ç”¨ try...finally å—ï¼š

        tmp_path = await resolve_image_source_to_path(source)
        try:
            # ä½¿ç”¨ tmp_path
            ...
        finally:
            if tmp_path and Path(tmp_path).exists():
                Path(tmp_path).unlink(missing_ok=True)

    Args:
        source: å›¾ç‰‡æºï¼ˆURL/æ–‡ä»¶/base64/data URLï¼‰
        image_input_mode: å‚è€ƒå›¾å¤„ç†æ¨¡å¼ï¼ˆç»Ÿä¸€ base64ï¼‰
        api_client: ç”¨äº normalize çš„ API å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        download_qq_image_fn: å¤„ç† qpic é“¾æ¥çš„ä¸‹è½½å‡½æ•°ï¼ˆå¯é€‰ï¼Œéœ€ä¸º asyncï¼‰
        is_valid_checker: base64 æ ¡éªŒå‡½æ•°
        logger_obj: æ—¥å¿—å¯¹è±¡

    Returns:
        æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› Noneã€‚è‹¥ä¸ºä¸´æ—¶æ–‡ä»¶ï¼Œè°ƒç”¨æ–¹éœ€è´Ÿè´£æ¸…ç†ã€‚
    """
    if not source:
        return None

    src = str(source).strip()
    # ä¿®æ­£ HTML è½¬ä¹‰çš„å‚æ•°
    if "&amp;" in src:
        src = src.replace("&amp;", "&")
    if not src:
        return None

    # æœ¬åœ°æ–‡ä»¶æˆ– file://
    if src.startswith("file:///"):
        fs_path = src[8:]
        if os.path.exists(fs_path):
            return fs_path
    if os.path.exists(src):
        return src

    # base64/data URL - ä½¿ç”¨ç»Ÿä¸€çš„è§£ç å‡½æ•°
    if is_valid_checker(src):
        result = _decode_base64_to_temp_file(
            src, verify_image=True, logger_obj=logger_obj
        )
        if result:
            return result

    # http(s) ä¸‹è½½
    if src.startswith(("http://", "https://")):
        parsed_host = ""
        try:
            parsed_host = urllib.parse.urlparse(src).netloc or ""
        except Exception:
            parsed_host = ""

        try:
            # å‘½ä¸­ä¸‹è½½ç¼“å­˜ç›´æ¥è¿”å›æ–‡ä»¶
            cached = _check_image_cache(src, IMAGE_CACHE_DIR)
            if cached:
                return str(cached)

            data_url = None
            is_qq = _is_qq_host(parsed_host)

            if download_qq_image_fn and is_qq:
                try:
                    data_url = await download_qq_image_fn(src, event=event)
                except Exception:
                    data_url = await download_qq_image_fn(src)

            if not data_url and api_client:
                mime_type, b64 = await api_client._normalize_image_input(
                    src, image_input_mode=image_input_mode
                )
                if b64:
                    data_url = (
                        b64
                        if image_input_mode == "force_base64"
                        else f"data:{mime_type};base64,{b64}"
                    )

            if not data_url:
                timeout = aiohttp.ClientTimeout(total=12, connect=5)
                headers = _build_http_headers(parsed_host, for_qq=is_qq)
                trust_env_flag = not is_qq
                async with aiohttp.ClientSession(
                    headers=headers, trust_env=trust_env_flag
                ) as session:
                    async with session.get(src, timeout=timeout) as resp:
                        if resp.status == 200:
                            mime = resp.headers.get("Content-Type", "image/png")
                            content = await resp.read()
                            b64 = base64.b64encode(content).decode()
                            data_url = f"data:{mime};base64,{b64}"

            # ä½¿ç”¨ç»Ÿä¸€çš„è§£ç å‡½æ•°
            if data_url and is_valid_checker(data_url):
                result = _decode_base64_to_temp_file(
                    data_url, verify_image=True, logger_obj=logger_obj
                )
                if result:
                    return result
        except Exception as e:
            logger_obj.warning(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e} | {src[:80]}")

    # å…¶ä»–å­—ç¬¦ä¸²å°è¯•å½“ä½œ base64
    result = _decode_base64_to_temp_file(src, verify_image=True, logger_obj=logger_obj)
    return result


class AvatarManager:
    """å¤´åƒç®¡ç†å™¨"""

    def __init__(self, images_dir: Path | None = None):
        self.images_dir = images_dir

    async def get_avatar(self, user_id: str, cache_name: str, event=None) -> str | None:
        """
        è·å–ç”¨æˆ·å¤´åƒ

        Args:
            user_id: ç”¨æˆ·ID
            cache_name: ç¼“å­˜åç§°
            event: AstrMessageEventï¼Œç”¨äºè°ƒç”¨NapCat API

        Returns:
            base64æ ¼å¼çš„å¤´åƒæ•°æ®
        """
        return await download_qq_avatar(user_id, cache_name, self.images_dir, event)

    async def cleanup_cache(self):
        """æ¸…ç†å¤´åƒç¼“å­˜"""
        if self.images_dir is None:
            self.images_dir = get_plugin_data_dir() / "images"

        cache_dir = self.images_dir / "avatar_cache"
        if cache_dir.exists():
            # ä¸å†ä½¿ç”¨å¤´åƒç¼“å­˜ï¼Œç›´æ¥æ¸…ç©ºç›®å½•
            try:
                for avatar_file in cache_dir.glob("*"):
                    avatar_file.unlink(missing_ok=True)
                cache_dir.rmdir()
                logger.debug("å·²æ¸…ç©ºå¤´åƒç¼“å­˜ç›®å½•")
            except Exception as e:
                logger.warning(f"æ¸…ç†å¤´åƒç¼“å­˜ç›®å½•å¤±è´¥: {e}")

    async def cleanup_used_avatars(self):
        """æ¸…ç†å·²ä½¿ç”¨çš„å¤´åƒç¼“å­˜ï¼ˆåˆ«åæ–¹æ³•ï¼‰"""
        await self.cleanup_cache()


# ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›ä¸€äº›æ—§åç§°çš„åˆ«å
def download_qq_avatar_legacy(user_id: str, cache_name: str, event=None) -> str | None:
    """
    ä¸‹è½½QQå¤´åƒçš„å…¼å®¹å‡½æ•°

    Args:
        user_id: QQç”¨æˆ·ID
        cache_name: ç¼“å­˜æ–‡ä»¶å

    Returns:
        base64æ ¼å¼çš„å¤´åƒæ•°æ®ï¼Œå¤±è´¥è¿”å›None
    """
    # ä½¿ç”¨ asyncio.run() ç®€åŒ–å¼‚æ­¥è°ƒç”¨
    return asyncio.run(download_qq_avatar(user_id, cache_name, event=event))


def format_error_message(error: Exception | str) -> str:
    """
    æ ¹æ®é”™è¯¯ç±»å‹ç”Ÿæˆç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯

    Args:
        error: å¼‚å¸¸å¯¹è±¡æˆ–é”™è¯¯å­—ç¬¦ä¸²

    Returns:
        æ ¼å¼åŒ–çš„é”™è¯¯æç¤ºæ¶ˆæ¯
    """
    error_str = str(error).lower()
    original_error = str(error)

    # image_config oneof å†²çªé”™è¯¯ï¼ˆå‚æ•°åé…ç½®é—®é¢˜ï¼‰
    if "image_config" in error_str and "oneof" in error_str:
        if "_image_size" in error_str or "imagesize" in error_str.replace("_", ""):
            return (
                f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
                "ğŸ§ åŸå› ï¼šåˆ†è¾¨ç‡å‚æ•°åé…ç½®ä¸æ­£ç¡®ã€‚\n"
                "âœ… å»ºè®®ï¼šè¯·è”ç³»ç®¡ç†å‘˜å°† resolution_param_name ä¿®æ”¹ä¸º imageSizeï¼ˆé©¼å³°å¼ï¼‰ã€‚"
            )
        if "_aspect_ratio" in error_str or "aspectratio" in error_str.replace("_", ""):
            return (
                f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
                "ğŸ§ åŸå› ï¼šå®½é«˜æ¯”å‚æ•°åé…ç½®ä¸æ­£ç¡®ã€‚\n"
                "âœ… å»ºè®®ï¼šè¯·è”ç³»ç®¡ç†å‘˜å°† aspect_ratio_param_name ä¿®æ”¹ä¸º aspectRatioï¼ˆé©¼å³°å¼ï¼‰ã€‚"
            )
        return (
            f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
            "ğŸ§ åŸå› ï¼šimageConfig å‚æ•°é…ç½®å†²çªã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·è”ç³»ç®¡ç†å‘˜æ£€æŸ¥ resolution_param_name å’Œ aspect_ratio_param_name é…ç½®ï¼Œ\n"
            "   Google API åº”ä½¿ç”¨é©¼å³°å¼å‘½åï¼ˆimageSize, aspectRatioï¼‰ã€‚"
        )

    # API å¯†é’¥é”™è¯¯
    if "api key" in error_str or "api_key" in error_str or "invalid key" in error_str:
        return (
            f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
            "ğŸ§ åŸå› ï¼šAPI å¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·è”ç³»ç®¡ç†å‘˜æ£€æŸ¥å¹¶æ›´æ–° API å¯†é’¥é…ç½®ã€‚"
        )

    # æ¨¡å‹ä¸å­˜åœ¨
    if "model not found" in error_str or "does not exist" in error_str:
        return (
            f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
            "ğŸ§ åŸå› ï¼šæŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·è”ç³»ç®¡ç†å‘˜æ£€æŸ¥æ¨¡å‹åç§°é…ç½®æ˜¯å¦æ­£ç¡®ã€‚"
        )

    # é…é¢/é™æµé”™è¯¯
    if "quota" in error_str or "rate limit" in error_str or "429" in error_str:
        return (
            f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
            "ğŸ§ åŸå› ï¼šAPI è¯·æ±‚é…é¢å·²ç”¨å°½æˆ–è¯·æ±‚è¿‡äºé¢‘ç¹ã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·ç¨åé‡è¯•ï¼Œæˆ–è”ç³»ç®¡ç†å‘˜æ£€æŸ¥ API é…é¢ã€‚"
        )

    # å†…å®¹å®‰å…¨è¿‡æ»¤
    if "safety" in error_str or "blocked" in error_str or "content_filter" in error_str:
        return (
            f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
            "ğŸ§ åŸå› ï¼šæç¤ºè¯æˆ–å›¾ç‰‡å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆªã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·ä¿®æ”¹æç¤ºè¯å†…å®¹åé‡è¯•ã€‚"
        )

    # è¶…æ—¶é”™è¯¯
    if "timeout" in error_str or "timed out" in error_str:
        return (
            f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
            "ğŸ§ åŸå› ï¼šè¯·æ±‚è¶…æ—¶ï¼ŒæœåŠ¡å™¨å“åº”è¿‡æ…¢ã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·ç¨åé‡è¯•ï¼Œå¦‚æŒç»­å‡ºç°è¯·è”ç³»ç®¡ç†å‘˜è°ƒæ•´è¶…æ—¶é…ç½®ã€‚"
        )

    # ç½‘ç»œè¿æ¥é”™è¯¯
    if "connection" in error_str or "network" in error_str or "connect" in error_str:
        return (
            f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
            "ğŸ§ åŸå› ï¼šç½‘ç»œè¿æ¥å¤±è´¥ã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•ï¼Œå¦‚éœ€ä»£ç†è¯·è”ç³»ç®¡ç†å‘˜é…ç½®ã€‚"
        )

    # å‚è€ƒå›¾ç‰‡é—®é¢˜
    if "reference" in error_str and "image" in error_str:
        return (
            f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
            "ğŸ§ åŸå› ï¼šå‚è€ƒå›¾ç‰‡å¤„ç†å¤±è´¥ã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·å°è¯•ä½¿ç”¨å…¶ä»–å›¾ç‰‡æˆ–ä¸ä½¿ç”¨å‚è€ƒå›¾é‡è¯•ã€‚"
        )

    # åªè¿”å›æ–‡æœ¬ï¼Œæœªç”Ÿæˆå›¾åƒï¼ˆno_image_retryï¼‰
    if "no_image_retry" in error_str or "åªè¿”å›äº†æ–‡æœ¬" in error_str:
        return (
            "âš ï¸ å›¾åƒç”ŸæˆæœªæˆåŠŸï¼šæ¨¡å‹åªè¿”å›äº†æ–‡å­—æè¿°ï¼Œæœªç”Ÿæˆå›¾ç‰‡ã€‚\n"
            "ğŸ§ åŸå› ï¼šæ¨¡å‹å¯èƒ½éœ€è¦æ›´æ˜ç¡®çš„ç»˜å›¾æŒ‡ä»¤ï¼Œæˆ–å½“å‰è¯·æ±‚ä¸é€‚åˆç”Ÿæˆå›¾åƒã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·å°è¯•æ›´æ˜ç¡®åœ°æè¿°æ‚¨æƒ³è¦çš„å›¾åƒå†…å®¹ã€‚"
        )

    # å“åº”æ ¼å¼å¼‚å¸¸ï¼Œæœªæ‰¾åˆ°å›¾åƒï¼ˆinvalid_responseï¼‰
    if "invalid_response" in error_str or "æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒ" in error_str:
        return (
            "âš ï¸ å›¾åƒç”Ÿæˆå¤±è´¥ï¼šAPI å“åº”ä¸­æœªåŒ…å«å›¾åƒæ•°æ®ã€‚\n"
            "ğŸ§ åŸå› ï¼šæœåŠ¡ç«¯è¿”å›äº†ç©ºå“åº”æˆ–æ ¼å¼å¼‚å¸¸ã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·ç¨åé‡è¯•ï¼Œå¦‚æŒç»­å‡ºç°è¯·è”ç³»ç®¡ç†å‘˜æ£€æŸ¥ API é…ç½®ã€‚"
        )

    # ç©ºå“åº”ï¼ˆæ—  candidatesï¼‰
    if "ç¼ºå°‘ candidates" in error_str or "no candidates" in error_str:
        return (
            "âš ï¸ å›¾åƒç”Ÿæˆå¤±è´¥ï¼šAPI è¿”å›äº†ç©ºå“åº”ã€‚\n"
            "ğŸ§ åŸå› ï¼šè¯·æ±‚å¯èƒ½è¢«è¿‡æ»¤æˆ–æœåŠ¡ç«¯æ— æ³•å¤„ç†ã€‚\n"
            "âœ… å»ºè®®ï¼šè¯·æ£€æŸ¥æç¤ºè¯æ˜¯å¦åˆé€‚ï¼Œå¦‚æŒç»­å‡ºç°è¯·è”ç³»ç®¡ç†å‘˜ã€‚"
        )

    # é»˜è®¤é€šç”¨é”™è¯¯
    return (
        f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥ï¼š{original_error}\n"
        "ğŸ§ å¯èƒ½åŸå› ï¼šç½‘ç»œæ³¢åŠ¨ã€é…ç½®ç¼ºå¤±æˆ–ä¾èµ–åŠ è½½å¤±è´¥ã€‚\n"
        "âœ… å»ºè®®ï¼šè¯·ç¨åé‡è¯•ï¼Œå¦‚æŒç»­å‡ºç°è¯·è”ç³»ç®¡ç†å‘˜æŸ¥çœ‹æ—¥å¿—ã€‚"
    )
